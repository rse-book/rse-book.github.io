[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Research Software Engineering",
    "section": "",
    "text": "Preface\nNote: This book is published by Chapman & Hall/CRC. The online version of this book is free to read here (thanks to Chapman & Hall/CRC), and licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. If you have any feedback, please feel free to file an issue on GitHub. Thank you!\n\nOne of the things that Baidu did well early on was to create an internal platform for deep learning. What that did was enable engineers all across the company, including people who were not AI researchers, to leverage deep learning in all sorts of creative ways - applications that an AI researcher like me never would have thought of. – Andrew Ng\n\nThe vast majority of data has been created within the last decade. In turn, many fields of research are confronted with an unprecedented wealth of data. The sheer amount of information and the complexity of modern datasets continue to point a kind of researcher to programming approaches that had not considered programming to process data so far. Research Software Engineering aims at two things: First, to give a big picture overview and starting point to reach what the open source software community calls a “software carpentry” level. Second, to give an understanding of the opportunities of automation and reproducibility, as well as the effort to maintain the required environment. This book argues a solid programming skill level and self-operation is totally in reach for most researchers. And most importantly, investing is worth the effort: being able to code leverages field-specific expertise and fosters interdisciplinary collaboration as source code continues to become an important communication channel.\n\n\n\n\n“Hackers wear hoodies, you know,” mumbles Dr. Egghead as he pulls up his coat’s hood and starts to figure out how his assistant got a week’s work done in hours. (Source: own illustration.)\n\n\n\n\n\nAcknowledgments\nI am thankful for the inspiration, help and perspectives of everyone who contributed to this book at its different stages. There are several people and organizations that I would like to thank in particular.\nFirst, thank you to David Grubbs at CRC Press for getting me started. From our first meeting at useR! in Toulouse, France, David helped streamline writing a book, and he kept adding value throughout the process with his remarks and contacts. I would like to thank Achim Zeileis who also played an important role at the early stage of my book project. Achim inspired me to become a co-host for useR! which led to many experiences that became important to this book. Our discussions about teaching amplified my motivation to have good material to accompany my own course – which eventually turned out to be one of the most important drivers.\nIn that regard, I would like to thank all participants of my Hacking for … courses. Your insights, questions, feedback and semester projects have been invaluable to this project. Your field-specific expertise is inspirational not only to me, but also to readers of the book as it shows the broad relevance of the approach. I would like to thank ETH Zurich, in particularly the Department of Management, Technology and Economics (D-MTEC) and the KOF Swiss Economic Institute for hosting my ideas over the last 13 years. Thank you to educational developers Karin Brown and Erik Jentges; having teaching professionals with open ears and minds around helped to channel motivation and ideas into a course concept that continues to be popular among participants across departments and disciplines. Torbjørn Netland deserves credit for enabling this widespread interest. His early advice turned what was initially thought of as Hacking for Economists into Hacking for Social Sciences, which eventually became Hacking for Science.\nThough this book is not exactly about the R programming language, I would like to thank the R community in particular because of its instrumental role in growing my own engagement and horizon of the open source ecosystem. My thanks go to statisticians Siegfried Heiler and Toni Stocker who pointed me to the R language almost 20 years ago. One of my most important learnings about how to leverage open source work may have come only 20 years later: In 2021, the community overcame the hurdles of the global COVID-19 pandemic that was unprecedented, at least in my lifetime, and held a virtual useR! conference that enabled a much larger and much more diverse group of participants to join (Joo et al. 2022).\nWorking with Rocío Joo, Dorothea Hug Peter, Heather Turner and Yanina Bellini Saibene has shown me the inclusion extra mile is not only worth the effort, but an inevitable mindset to bring our work as developers and scientists to the next level. Thank you, ladies! Thanks also to the local R user group in Zurich, Switzerland, who continues to show the practical relevance of open source through events. I want to mention the great people at my new employer, cynkra, too. Your influence on the community and therefore this book cannot be forgotten. Thank you for your support and open ears!\nLast but not least, I would like to thank Emily Riederer for her time and patience discussing my thoughts and reviewing my drafts. Particulary, her ideas to streamline and balance my ideas at a stage where they were rather messy added great value. Admittedly, your constructive feedback has caused some extra work, but was instrumental to making this book useful and accessible to a wider audience – Thank you.\n\n\n\n\nJoo, Rocío et al. 2022. “Ten Simple Rules to Host an Inclusive Conference.” PLOS Computational Biology 18 (7): 1–13. https://doi.org/10.1371/journal.pcbi.1010164.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Why Work Like a Software Engineer?\nWhy should a researcher or business analyst work Like a Software Engineer? First, because everybody and their grandmothers seem to work like software engineers. Statistical computing continues to be on the rise in many branches of research. Figure 1.1 shows the obvious trend in the sum of total R package downloads per month since 2015.\nFigure 1.1: Monthly R Package Downloads. (Source: RStudio CRAN mirror.)\nCode\nlibrary(cranlogs)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(tsbox)\ntop &lt;- cranlogs::cran_top_downloads()\n\npacks &lt;- cranlogs::cran_downloads(\n                               from = \"2015-01-01\",\n                               to = \"2022-09-30\")\n\npacks |&gt; \n  group_by(floor_date(date, \"month\")) |&gt; \n  summarize(m = sum(count)) |&gt; \n  ts_plot()\nBandwagonism aside, source code can be a tremendously sharp, unambiguous and international communication channel. Your web scraper does not work? Instead of reaching out in a clumsy but wordy cry for help, posting what you tried so far described by source code will often get you good answers within hours. Platforms like stackoverflow1 or Crossvalidated2 do not only store millions of questions and answers, they also gather a huge and active community to discuss issues. Plus, recent developments show that source code is not only useful to communicate with human experts. Have you tried to turn an idea into code or improve a piece of code by playing ping pong with chatGPT or Bing? Or think of feature requests: After a little code ping pong with the package author, your wish eventually becomes clearer. Let alone chats with colleagues and co-authors. Sharing code just works. Academic journals have found that out, too. Many outlets require you to make the data and source code behind your work available. Social Science Data Editors (Vilhuber et al. 2022) is a bleeding-edge project at the time of writing this, but it is already referred to by top-notch journals like American Economic Review (AER).\nIn addition to the above reproducibility and ability to share, code scales and automates. automation is very convenient; like when you want to download data, process and create the same visualization and put it on your website any given Sunday. automation is inevitable; like when you have to gather daily updates from different outlets or work through thousands of .pdfs.\nLast but not least, programming enables you to do many things you couldn’t do without being an absolute guru (if at all) if it wasn’t for programming. Take visualization, for example. Go check the D3 Examples at https://d3js.org/. Now, try to do that in Excel. If you did these things in Excel, it would make you an absolute spreadsheet visualization Jedi, probably missing out on other time-consuming skills to master. Yet, with decent, carpentry-level programming skills you can already do so many spectacular things while not really specializing and staying very flexible.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#why-work-like-a-software-engineer",
    "href": "intro.html#why-work-like-a-software-engineer",
    "title": "1  Introduction",
    "section": "",
    "text": "Source code is an unambiguous, interactive and global communication channel. (Source: own illustration.)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#why-work-like-an-operations-engineer",
    "href": "intro.html#why-work-like-an-operations-engineer",
    "title": "1  Introduction",
    "section": "1.2 Why Work Like an Operations Engineer?",
    "text": "1.2 Why Work Like an Operations Engineer?\nWhile software development has become closer to many researchers, operations, the second part of the term DevOps, is much further away from the average researcher or business analyst. Why even think about operations? Because we can afford to do so. Operations have become so much more accessible in recent years, that many applications can be dealt with single-handedly. Though one’s production applications may still be administered by operations professionals, the ability to create and run a proof of concept from scratch is an invaluable skill. A running example says more than a 15-page specification that fails to translate business talk into tech talk. The other way around, something to look at an early stage helps to acquire funds and convince managers and other collaborators.\nBut people without a computer engineering background are by no means limited to proof of concepts these days. Trends like cloud computing and software-as-a-service products help developers focus on their expertise and limit the amount of knowledge needed to host a service in secure and highly available fashion.\nAlso, automation is key to the DevOps approach and an important reason why DevOps thinking is also very well suited for academic researchers and business analysts. So-called continuous integration can help to enforce a battery of quality checks such as unit tests or installation checks. Let’s say a push to a certain branch of a git repository leads to the checks and tests. In a typical workflow, successful completion of quality checks triggers continuous deployment to a blog, rendering into a paper or interactive data visualization.\nBy embracing both parts of the DevOps approach, researchers do not only gain extra efficiency, but more importantly they improve reproducibility and therefore accountability and quality of their work. The effect of a DevOps approach on quality control is not limited to reproducible research in a publication sense only, but also enforces rules during collaboration: no matter who contributes, the contribution gets gut-checked and only deployed if checks passed. Similar to the well established term software carpentry that advocates a solid, application minded understanding of programming with data, I suggest a carpentry-level understanding of development and operations is desirable for the programming data analyst.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#how-to-read-this-book",
    "href": "intro.html#how-to-read-this-book",
    "title": "1  Introduction",
    "section": "1.3 How To Read This Book?",
    "text": "1.3 How To Read This Book?\nThe focal goal of this book is to map out the open source ecosystem, identify neuralgic components and give you an idea of how to improve not only in programming but also in navigating the wonderful but vast open source world. Chapter 2 is the road map for this book: it describes and classifies the different parts of the open source stack and explains how these pieces relate to each other. Subsequent chapters highlight core elements of the open source toolbox one at a time and walk through applied examples, mostly written in the R language.\nThis book is the companion I wish I had when I started an empirical, data intensive PhD in economics. Yet, the book is written years after my PhD was completed and with the hindsight of more than 10 years in academia. Research Software Engineering is written based on the experience of helping students and seasoned researchers of different fields with their data management, processing and communication of results.\nIf you are confident in your ambition to amp up your programming to at least solid software carpentry level3 within the next few months, I suggest getting an idea of your starting point relative to this book. The upcoming backlog section is essentially a list of suggested to-dos on your way to solid software carpentry. Obviously, you may have cleared a few tasks of this backlog before reading this book which is fine. On the other hand, you might not know about some of the things that are listed in the requirement section, which is fine, too. The backlog and requirements section just mean to give you some orientation.\nIf you do not feel committed, revisit the previous section, discuss the need for programming with peers from your domain and possibly talk to a seasoned R or Python programmer. Motivation and commitment are key to the endurance needed to develop programming into a skill that truly leverages your domain-specific expertise.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#backlog",
    "href": "intro.html#backlog",
    "title": "1  Introduction",
    "section": "1.4 Backlog",
    "text": "1.4 Backlog\nIf you can confidently say you can check all or most of the below, you have reached carpentry level in developing and running your applications. Even if this level is your starting point, you might find a few useful tips or simply may come to cherry-pick. In case you are rather unfamiliar with most of the below, the contextualization, the classification and overview of tools is likely the most valuable help this book provides.\nA solid, applied understanding of git version control and the collaboration workflow associated with git does not only help you stay ahead of your own code; git proficiency makes you a team player. If you are not familiar with commits, pulls, pushes, branches, forks and pull requests, Research Software Engineering will open up a new world for you. An introduction to industry standard collaboration makes you fit into a plethora of (software) teams around the globe – in academia and beyond.\nYour backlog en route to a researcher who is comfortable in Research Software Engineering obviously contains a strategy to improve your programming itself. To work with data in the long run, an idea of the challenges of data management from persistent storage to access restrictions should complement your programming. Plus, modern data-driven science often has to handle datasets so large, which is when infrastructure other than local desktops come into play. Yet, high performance computing (HPC) is by far not the only reason it is handy for a researcher to have a basic understanding of infrastructure. Communication of results, including data dissemination or interactive online reports require the content to be served from a server with permanent online access. Basic workflow automation of regular procedures, e.g., for a repeated extract-transform-load (ETL) process to update data, is a low-hanging (and very useful) fruit for a programming researcher.\nThe case studies at the end of the book are not exactly a backlog item like the above but still a recommended read. The case studies in this book are hands-on programming examples – mostly written in R – to showcase tasks from application programming interface (API) usage to geospatial visualization in reproducible fashion. Reading and actually running other developers’ code not only improve one’s own code, but helps to see what makes code inclusive and what hampers comprehensibility.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#requirements",
    "href": "intro.html#requirements",
    "title": "1  Introduction",
    "section": "1.5 Requirements",
    "text": "1.5 Requirements\nEven though Research Software Engineering aims to be inclusive and open to a broad audience with different starting points, several prerequisites exist to get the most out of this book. I recommend you have made your first experience with an interpreted programming language like R or Python. Be aware though that experience from a stats or math course does not automatically make you a programmer, as these courses are rightfully focused on their own domain. Books like R for Data Science (Wickham and Grolemund 2017) or websites like the The Carpentries4 help to solidify your data science minded, applied programming. Advanced R (Wickham 2019), despite its daunting title, is an excellent read to get past the superficial kind of understanding of R that we might acquire from a first encounter in a stats course.\nA certain familiarity with console/terminal basics will help the reader sail smoothly. At the end of the day, there are no real must-have requirements to benefit from this book. The ability to self-reflect on one’s starting point remains the most important requirement to leverage the book.\nResearch Software Engineering willingly accepts to be overwhelming at times. Given the variety of topics touched on in an effort to show the big picture, I encourage the reader to remain relaxed about a few blanks even when it comes to fundamentals. The open source community offers plenty of great resources to selectively upgrade skills. This book intends to show how to evaluate the need for help and how to find the right sources. If none of the above means something to you, though, I recommend making yourself comfortable with the basics of some of the above fields before you start to read this book.\n\n\n\n\nVilhuber, Lars, Marie Connolly, Miklós Koren, Joan Llull, and Peter Morrow. 2022. A template README for social science replication packages. https://doi.org/10.5281/zenodo.7293838.\n\n\nWickham, Hadley. 2019. Advanced r. 2nd ed. Chapman & Hall/CRC the r Series. Taylor & Francis.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 1st ed. O’Reilly Media, Inc.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#footnotes",
    "href": "intro.html#footnotes",
    "title": "1  Introduction",
    "section": "",
    "text": "https://stackoverflow.com↩︎\nhttps://crossvalidated.com↩︎\nhttps://software-carpentry.org/↩︎\nhttps://carpentries.org/↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "stack-developer-toolkit.html",
    "href": "stack-developer-toolkit.html",
    "title": "2  Stack: A Developer’s Toolkit",
    "section": "",
    "text": "2.1 Programming Language\nIn Statistical Computing, the interface between the researcher and the computation node is almost always an interpreted programming language as opposed to a compiled one. Compiled languages like C++ require the developer to write source code and compile, i.e., translate source code into what a machine can work with before runtime. The result of the compilation process is a binary which is specific to an operating system. Hence, you will need one version for Windows, one for OSX and one for Linux if you intend to reach a truly broad audience with your program. The main advantage of a compiled language is speed in terms of computing performance, because the translation into machine language does not happen during runtime. A reduction of development speed and increase in required developer skills are the downside of using compiled languages.\nThe above quote became famous in the hacking data community, not only because of the provocative, fun part of it, but also because of the implicit advice behind it. Given the enormous gain in computing power in recent decades, and also methodological advances, interpreted languages are often fast enough for many social science problems. And even if it turns out, your data grow out of your setup, a well-written proof of concept written in an interpreted language can be a formidable blueprint. Source code is turning into an important scientific communication channel. Put your money on it, your interdisciplinary collaborator from the High Performance Computing (HPC) group will prefer some Python code as a briefing for their C++ or Fortran program over a wordy description out of your field’s ivory tower.\nInterpreted languages are a bit like pocket calculators, you can look at intermediate results, line-by-line. R and Python are the most popular open source software (OSS) choices for hacking with data, Julia (Bezanson et al. 2017) is an up-and-coming, performance-focused language with a much slimmer ecosystem. A bit of JavaScript can’t hurt for advanced customization of graphics and online communication of your results.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Stack\\index{stack}: A Developer's Toolkit</span>"
    ]
  },
  {
    "objectID": "stack-developer-toolkit.html#programming-language",
    "href": "stack-developer-toolkit.html#programming-language",
    "title": "2  Stack: A Developer’s Toolkit",
    "section": "",
    "text": "Big data is like teenage sex: everyone talks about it, nobody really knows how to do it, everyone thinks everyone else is doing it, so everyone claims they are doing it. – Dan Ariely, Professor of Psychology and Behavioral Economics on X1",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Stack\\index{stack}: A Developer's Toolkit</span>"
    ]
  },
  {
    "objectID": "stack-developer-toolkit.html#interaction-environment",
    "href": "stack-developer-toolkit.html#interaction-environment",
    "title": "2  Stack: A Developer’s Toolkit",
    "section": "2.2 Interaction Environment",
    "text": "2.2 Interaction Environment\nWhile the fact that a software developer needs to choose a programming language to work with is rather obvious, the need to compose and customize an environment to interact with the computer is much less obvious to many. Understandably so because, outside of programming, software such as word processors, video or image editors presents itself to the user as a single program. That single program takes the user’s input, processes the input (in memory) and stores a result on disk for persistence – often in a proprietary, program specific format.\nYet, despite all the understanding for nonchalantly saying we keep documents in Word or data in R Studio, it’s beyond nerdy nitpicking when I insist that data are kept in files (or databases) – not in a program. And that R is not R Studio: This sloppy terminology contributes to making us implicitly accept that our office documents live in one single program. (And that there is only one way to find and edit these files: through said program).\nIt is important to understand that source code of essentially any programming languages is a just a plain text file and therefore can be edited in any editor. Editors come in all shades of gray: from lean, minimal code highlighting support to full-fledged integrated development environments (IDEs) with everything from version control integration to sophisticated debugging.\nWhich way of interaction the developer likes better is partly a matter of personal taste, but it also depends on the programming language, the team’s form of collaboration and size of the project. In addition to a customized editor, most developers use some form of a console to communicate with their operating system. The ability to send commands instead of clicking is not only reproducible and shareable, but it also outpaces mouse movement by a lot when commands come in batches. Admittedly, configuration, practice and getting up to speed takes time, but once you have properly customized the way you interact with your computer when writing code, you will never look back.\nIn any case, make sure to customize your development environment: choose the themes you like, make sure the cockpit you spent your day in is configured properly and feels comfy.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Stack\\index{stack}: A Developer's Toolkit</span>"
    ]
  },
  {
    "objectID": "stack-developer-toolkit.html#version-control",
    "href": "stack-developer-toolkit.html#version-control",
    "title": "2  Stack: A Developer’s Toolkit",
    "section": "2.3 Version Control",
    "text": "2.3 Version Control\nTo buy into the importance of managing one’s code professionally may be the single most important take-away from this book. Being able to work with will help you fit into numerous different teams that have contact points with data science and programming, let alone if you become part of a programming or data science team.\nWhile has a long history dating back to CVS2 and SVN3, the good news for the learner is, that there is a single dominant approach when it comes to in the data analysis world. Despite the fact that its predecessors and alternatives such as Mercurial are still around, git4 is the one you have to learn. To learn more about the history of and approaches other than git, Eric Sink’s Version Control by Example (Sink 2011) is for you.\nSo what does git do for us as researchers? How is it different from Dropbox?\ngit does not work like dropbox. git is not like dropbox.\ngit does not work like dropbox. git is not like dropbox. \ngit does not work like dropbox. git is not like dropbox.\ngit does not work like dropbox. git is not like dropbox. \ngit does not work like dropbox. git is not like dropbox.\ngit does not work like dropbox. git is not like dropbox. \n\nThe idea of thinking of a sync is what interferes with comprehension of the benefit of (which is why I hate that git GUIs call it “sync” anyway to avoid irritation of user’s initial beliefs.). Git is a decentralized system that keeps track of a history of semantic commits. Those commits may consist of changes to multiple files. A commit message summarizes the gist of a contribution. Diffs allow comparing different versions.\n\n\n\nThe diff output shows an edit during the writing of this book. The line preceded by ‘-’ was replaced with the line preceded by ‘+’.\n\n\nGit is well suited for any kind of text file, whether it is source code from Python or C++, or some text written in Markdown or LaTeX. Binaries like .pdfs or Word documents are possible, too, but certainly not the type of file for which git is really useful. This book contains a detailed, applied introduction tailored to researchers in a dedicated chapter, so let’s dwell with the above contextualization for a bit.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Stack\\index{stack}: A Developer's Toolkit</span>"
    ]
  },
  {
    "objectID": "stack-developer-toolkit.html#data-management",
    "href": "stack-developer-toolkit.html#data-management",
    "title": "2  Stack: A Developer’s Toolkit",
    "section": "2.4 Data Management",
    "text": "2.4 Data Management\nOne implication of bigger datasets and/or bigger teams is the need to manage data. Particularly when projects establish themselves or update their data regularly, well-defined processes, consistent storage and possibly access management become relevant. But even if you worked alone, it can be very helpful to have an idea about data storage in files systems, memory consumption and archiving data for good.\nData come in various forms and dimensions, potentially deeply nested. Yet, researchers and analysts are mostly trained to work with one-row-one-observation formats, in which columns typically represent different variables. In other words, two-dimensional representation of data remains the most common and intuitive form of data to many. Hence, office software offers different, text-based and proprietary spreadsheet file formats. On disk, comma separated files (.csv)5 are probably the purest representation of two-dimensional data that can be processed comfortably by any programming language and that can be read by many programs. Other formats such as .xml or .json allow storing even deeply nested data.\nIn-memory, that is when working interactively with programming languages such as R or Python, data.frames are the most common representation of two-dimensional data. Data.frames, their progressive relatives like tibbles or data.tables and the ability to manipulate them in reproducible, shareable and discussible fashion is the first superpower upgrade over pushing spreadsheets. Plus, other representation such as arrays, dictionaries or lists represent nested data in memory. Though in-memory data manipulation is very appealing, memory is limited and needs to be managed. Making the most of the memory available is one of the driving forces behind extensions of the original data.frame concept.\nThe other obvious limitation of data in memory is the lack of persistent storage. Therefore, in-memory data processing needs to be combined with file based storage are a database. The good news is that languages like R and Python are well-equipped to interface with a plethora of file-based approaches as well as databases. So well, that I often recommend these languages to researchers who work with other less-equipped tools, solely as an intermediate layer.\nTo evaluate which relational (essentially SQL) or non-relational database to pick up just seems like the next daunting task of stack choice. Luckily, in research, first encounters with a database are usually passive, in the sense that you want to query data from a source. In other words, the choice has been made for you. So unless you want to start your own data collection from scratch, simply sit back, relax and let the internet battle out another conceptual war. For now, let’s just be aware of the fact that a data management game plan should be part of any analysis project.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Stack\\index{stack}: A Developer's Toolkit</span>"
    ]
  },
  {
    "objectID": "stack-developer-toolkit.html#infrastructure",
    "href": "stack-developer-toolkit.html#infrastructure",
    "title": "2  Stack: A Developer’s Toolkit",
    "section": "2.5 Infrastructure",
    "text": "2.5 Infrastructure\nFor researchers and business analysts who want to program with data, the starting infrastructure is very likely their own desktop or notebook computer. Nowadays, this already means access to remarkable computing power suited for many tasks.\nYet, it is good to be aware of the many reasons that can turn the infrastructure choice from a no-brainer into a question with many options and consequences. Computing power, repeated or scheduled tasks, hard-to-configure or non-compatible runtime environment, online publication or high availability may be some of the needs that make you think about infrastructure beyond your own notebook.\nToday, thanks to software-as-a-service (SaaS) offers and cloud computing, none of the above needs imply running a server, let alone a computing center on your own. Computing has not only become more powerful over the last decade, but also more accessible. Entry hurdles are lower than ever. Many needs are covered by services that do not require serious upfront investment. It has become convenient to try and let the infrastructure grow with the project.\nFrom a researcher’s and analyst’s perspective, one of the most noteworthy infrastructure developments of recentness may be the arrival of containers in data analysis. Containers are isolated, single-purpose environments that run either on a single container host or with an orchestrator in a cluster. Although technically different from virtual machines, we can regard containers as virtual computers running on a computer for now. With containers, data analysts can create isolated, single purpose environments that allow to reproduce analysis even after one’s system was upgraded and with it all the libraries used to run the analysis. Or think of some exotic LaTeX configuration, Python environment or database drivers that you just can’t manage to run on your local machine. Bet there is a container blueprint (aka image) around online for exactly this purpose.\n\nPremature optimization is the root of all evil. – Donald Knuth.\n\nOn the flip side, product offerings and therefore our options have become a fast-growing, evolving digital jungle. So, rather than trying to keep track (and eventually loosing it) of the very latest gadgets, this book intends to show a big picture{big picture} overview and pass on a strategy to evaluate the need to add to your infrastructure. Computing power, availability, specific services and collaboration are the main drivers for researchers to look beyond their own hardware. Plus, public exposure, i.e., running a website as discussed in the publishing section, asks for a web host beyond our local notebooks.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Stack\\index{stack}: A Developer's Toolkit</span>"
    ]
  },
  {
    "objectID": "stack-developer-toolkit.html#automation",
    "href": "stack-developer-toolkit.html#automation",
    "title": "2  Stack: A Developer’s Toolkit",
    "section": "2.6 Automation",
    "text": "2.6 Automation\n“Make repetitive work fun again!” could be the claim of this section. Yet, it’s not just the typical intern’s or student assistant’s job that we would love to automate. Regular ingestion of data from an application programming interface (API) or a web scraping process are one form of reoccurring tasks, often called cron jobs after the Linux cron command, which is used to schedule execution of Linux commands. Regular computation of an index, triggered by incoming data would be a less time-focused, but more event-triggered example of an automated task.\nThe one trending form of automation, though, is continuous integration/continuous development (CI/CD). CI/CD processes are typically closely linked to a git repository and are triggered by a particular action done to the repository. For example, in case some pre-defined part (branch) of a git repository gets updated, an event is triggered and some process starts to run on some machine (usually some container). Builds of software packages are very common use cases of such a CI/CD process. Imagine you are developing an R package and want to run all the tests you’ve written, create the documentation and test whether the package can be installed. Once you’ve made your changes and push to your remote git repository, your push triggers the tests, the check, the documentation rendering and the installation. Potentially a push to the main branch of your repository could even deploy a package that cleared all of the above to your production server.\nRendering documentation, e.g., from Markdown to HTML into a website or presentation or a book like this one is a very similar example of a CI/CD process. Major git providers like Gitlab (GitLab CI/CD) or GiTHub (GitHub Actions) offer CI/CD tool integration. In addition, standalone services like CircleCI can be used as well as open source, self-hosted software like Woodpecker CI6.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Stack\\index{stack}: A Developer's Toolkit</span>"
    ]
  },
  {
    "objectID": "stack-developer-toolkit.html#communication-tools",
    "href": "stack-developer-toolkit.html#communication-tools",
    "title": "2  Stack: A Developer’s Toolkit",
    "section": "2.7 Communication Tools",
    "text": "2.7 Communication Tools\nIts community is one of the reasons for the rise of open source software over the last decades. Particularly, newcomers would miss a great chance for a kick-start into programming if they did not connect with the community. Admittedly, not all the community’s communication habits are for everyone, yet it is important to make your choices and pick up a few channels.\nChat clients like Slack, Discord or the Internet Relay Chat (IRC) (for the pre-millenials among readers) are the most direct form of asynchronous communication. Though many of the modern alternatives are not open source themselves, they offer free options and remain popular in the community despite self-hosted approaches such as matrix7 along with Element8. Many international groups around data science and statistical computing such as RLadies or the Society of Research Software Engineering have Slack spaces that are busy 24/7 around the world.\nSocial media is less directed and more easily consumed in a passive fashion than a chat space. Over the last decade, X, formerly known as twitter, has been a very active and reliable resource for good reads but has seen parts of the community switch to more independent platforms such as Mastodon9. Linkedin is another suitable option to connect and find input, particularly in parts of the world where other platforms are less popular. Due to its activity, social media is also a great way to stay up-to-date with the latest developments.\nMailing lists are another, more old-fashioned form to discuss things. They do not require a special client and just an e-mail address to subscribe to their regular updates. If you intend to avoid social media, as well as signing up at knowledge sharing platforms such as stackoverflow.com10 or reddit11, mailing lists are a good way to get help from experienced developers.\nIssue trackers are one form of communication that is often underestimated by newcomers. Remotely hosted git repositories, e.g., repositories hosted at GitHub or GitLab, typically come with an integrated issue trackers to report bugs. The community discusses a lot more than problems on issue trackers: feature requests are a common use case, and even the future direction of an open source project may be affected substantially by discussions in its issue tracker.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Stack\\index{stack}: A Developer's Toolkit</span>"
    ]
  },
  {
    "objectID": "stack-developer-toolkit.html#publishing-and-reporting",
    "href": "stack-developer-toolkit.html#publishing-and-reporting",
    "title": "2  Stack: A Developer’s Toolkit",
    "section": "2.8 Publishing and Reporting",
    "text": "2.8 Publishing and Reporting\nData analysis hopefully yields results that the analyst intends to report internally within their organization or share with the broader public. This has led to a plethora of options on the reporting end. Actually, data visualization and reproducible, automated reporting are two of the main drivers researchers and analysts turn to programming for their data analysis.\nIn general, we can distinguish between two forms of output: pdf-like, print-oriented output and HTML-centered web content. Recent tool chains have enabled analysts without strong backgrounds in web frontend development (HTML/CSS/JavaScript) to create nifty reports and impressive, interactive visualizations. Frameworks like R Shiny or Python Dash even allow creating complex interactive websites.\nNotebooks that came out of the Python world established themselves in many other languages implementing the idea of combining text with code chunks that get executed when the text is rendered to HTML or PDF. This allows researchers to describe, annotate and discuss results while sharing the code that produced the results described. In academia, progressive scholars and journals embrace this form of creating manuscripts as reproducible research that improves trust in the presented findings.\nBesides academic journals that started to require researchers to hand in their results in reproducible fashion, reporting based on so-called static website generators12 has taken data blogs and reporting outlets including presentations by storm. Platforms like GitHub render Markdown files to HTML automatically, displaying formatted plain text as a decent website. Services such as Netlify allow using a broad variety of build tools to render input that contains text and code.\nCentered around web browsers to display the output, HTML reporting allows creating simple reports, blogs, entire books (like this one) or presentation slides for talks. But thanks to document converters like Pandoc and a typesetting juggernaut called LaTeX, rendering sophisticated .pdf is possible, too. Some environments even allow rendering to proprietary word processor formats.\n\n\n\n\nBezanson, Jeff, Alan Edelman, Stefan Karpinski, and Viral B Shah. 2017. “Julia: A Fresh Approach to Numerical Computing.” SIAM Review 59 (1): 65–98. https://doi.org/10.1137/141000671.\n\n\nSink, Eric. 2011. Version Control by Example. 1st ed. PYOW Sports Marketing.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Stack\\index{stack}: A Developer's Toolkit</span>"
    ]
  },
  {
    "objectID": "stack-developer-toolkit.html#footnotes",
    "href": "stack-developer-toolkit.html#footnotes",
    "title": "2  Stack: A Developer’s Toolkit",
    "section": "",
    "text": "https://x.com/danariely/status/287952257926971392↩︎\nConcurrent Versions System: https://cvs.nongnu.org/↩︎\nApache Subversion↩︎\nhttps://git-scm.com/book/en/v2/Getting-Started-About-Version-Control↩︎\nSome dialects use different separators like ‘;’ or tabs, partly because of regional differences like the use of commas as decimal delimiters.↩︎\nhttps://woodpecker-ci.org/↩︎\nhttps://matrix.org/↩︎\nhttps://element.io/↩︎\nhttps://mastodon.social/↩︎\nhttps://stackoverflow.com↩︎\nhttps://reddit.com↩︎\nAs opposed to content management systems (CMS) that keep content in a database and put content and layout template together when users visit a website, static website generators render a website once triggered by an event. If users want to update a static website, they simply rerun the render process and push the HTML output of said process online.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Stack\\index{stack}: A Developer's Toolkit</span>"
    ]
  },
  {
    "objectID": "programming.html",
    "href": "programming.html",
    "title": "3  Programming 101",
    "section": "",
    "text": "3.1 The Choice That Doesn’t Matter\nThe very first (and intimidating) choice a novice hacker faces is which is the programming language to learn. Unfortunately, the medium popularly summed up as the internet offers a lot of really, fantastic advice on the matter. The problem is, however, that this advice does not necessarily agree which language is the best for research. In the realm of data science – get accustomed to that label if you are a scientist who works with data – the debate basically comes down to two languages: The R Language for Statistical Computing and Python.\nAt least to me, there is only one valid advice: It simply does not matter. If you stick around in data science long enough, you will eventually get in touch with both languages and, in turn, learn both. There is a huge overlap of what you can do with either of those languages. R came out of the rather specific domain of statistics more than 25 years ago and made its way to a more general programming language thanks to over 20K extension packages (and counting). Built by a mathematician, Python continues to be as general purpose as it has ever been. But it got more scientific, thanks to extension packages of its own such as pandas1, SciPy2 or NumPy3. As a result, there is a huge overlap of what both languages can do, and both will extend your horizon in unprecedented fashion if you did not use a full-fledged programming language for your analysis beforehand.\nBut why is there such a heartfelt debate online if it doesn’t matter? Let’s pick up a random argument from this debate: R is easier to set up and Python is better for machine learning. If you worked with Java or another environment that’s rather tricky to get going, you are hardened and might not cherish easy onboarding. If you got frustrated before you really started, you might feel otherwise. You may just have been unlucky making guesses about a not so well-documented paragraph, trying to reproduce a nifty machine learning blog post. Or imagine the frustration in case just had installed the wrong version of Python or did not manage to make sense of virtualenv right from the beginning.\nThe point is, rest assured, if you just start doing analytics using a programming language, both languages are guaranteed to carry you a long way. There is no way to tell for sure which one will be the more dominant language 10 years from now, or whether both will be around holding their ground the way they do now. But once you reached a decent software carpentry level in either language, it will help you a lot learning the other. If your peers work with R, start with R; if your close community works with Python, start with Python. If you are in for the longer run, either language will help you understand the concepts and ideas of programming with data. Trust me, there will be a natural opportunity to get to know the other.\nIf you associate programming more often than not with hours of fiddling, tweaking and fighting to galvanize approaches found online, this chapter is for you. Don’t expect lots of syntax. If you came for examples of useful little programs from data visualization to parallel computing, check out the Case Studies.\nThe following sections share a blueprint to go from explorative script to production-ready package. Organize your code and accompany the evolution of your project: start out with experiments, define your interface, narrow down to a proof of concept and scale up. Hopefully, the tips, tricks and the guidance in this chapter will help you to experience the rewarding feeling of a software project coming together like a plan originated by Hannibal Smith4.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Programming 101</span>"
    ]
  },
  {
    "objectID": "programming.html#the-choice-that-doesnt-matter",
    "href": "programming.html#the-choice-that-doesnt-matter",
    "title": "3  Programming 101",
    "section": "",
    "text": "R: “Dplyr smokes pandas.” Python: “But Keras is better for ML!” Language wars can be entertaining, sometimes spectacular, but are essentially just useless. (Source: own illustration).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Programming 101</span>"
    ]
  },
  {
    "objectID": "programming.html#plan-your-program",
    "href": "programming.html#plan-your-program",
    "title": "3  Programming 101",
    "section": "3.2 Plan Your Program",
    "text": "3.2 Plan Your Program\nHow much planning ahead is optimal for your project ultimately depends on your experience, number of collaborators and the size of your project. But still, a rough standard checklist helps any project.\n\n3.2.1 Think Library!\nThe approach that I find practical for applied, empirical research projects involving code is: think library. Think package. Think reusable code. Don’t think you can’t do it. Let me demystify packages for you: Packages are nothing but source code organized in folders following some convention. Thanks to modern IDEs, it has never been easier to stay inline with conventions. Editors like RStudio ship with built-in support to create package skeletons with a few clicks. Thousands of open source extension packages allow you to learn from their structure. Tutorials like Packaging Python Projects5 or Hadley Wickham’s book *R Packages (H. Wickham 2015) explain how to create packages good enough to make the official PyPi or CRAN package repositories.\nIn other words, it is unlikely that someone with moderate experience comes with the best folder structure ever invented. Sure, every project is different and not every aspect (folder) is needed in every project. Nevertheless, there are well-established blueprints, guides and conventions that suit almost any project. Unlike Office type of projects which center around one single file, understand that a research project will live in a folder with many subfolders and files. Not in one single file.\nTrust me on this one: The package approach will pay off early. Long before you ever thought about publishing your package. Write your own function definition, rather than just calling functions line-by-line. Write code as if you need to make sure it runs on another computer. Write code as if you need to maintain it.\nGo from scripts like this\n\n# This is just data for the sake of \n# reproducible example\nset.seed(123)\nd1 &lt;- rnorm(1000)\nd2 &lt;- rnorm(1000)\n\n# let's create some custom descriptive\n# stats summary for the data generated above\nd1_mean &lt;- mean(d1)\nd1_sd &lt;- sd(d1)\nd1_q &lt;- quantile(d1)\ndesc_stats_d1 &lt;- list(d1_mean = d1_mean,\n                      d1_sd = d1_sd,\n                      d1_q = d1_q)\n\nd2_mean &lt;- mean(d2)\nd2_sd &lt;- sd(d2)\nd2_q &lt;- quantile(d2)\ndesc_stats_d2 &lt;- list(d2_mean = d2_mean,\n                      d2_sd = d2_sd,\n                      d2_q = d2_q)\n\nTo function definitions and calls like that\n\n# Imagine you had thousand of datasets.\n# Imagine you wanted to add some other stats\n# Imagine all the error prone c&p with \n# the above solution. \n# Think of how much easier this is to document. \n# This is automation\\index{automation}. Not cooking. \ncreate_basic_desc &lt;- function(distr){\n  out &lt;- list(\n    mean = mean(distr),\n    sd = sd(distr),\n    quantiles = quantile(distr)\n  )\n  out\n}\n\n\ncreate_basic_desc(d1)\n\n$mean\n[1] 0.01612787\n\n$sd\n[1] 0.991695\n\n$quantiles\n          0%          25%          50%          75%         100% \n-2.809774679 -0.628324243  0.009209639  0.664601867  3.241039935 \n\ncreate_basic_desc(d2)\n\n$mean\n[1] 0.04246525\n\n$sd\n[1] 1.009674\n\n$quantiles\n         0%         25%         50%         75%        100% \n-3.04786089 -0.65322296  0.05485238  0.75345037  3.39037082 \n\n\nStart to document functions and their parameters using Roxygen (Hadley Wickham et al. 2022) syntax, and you’re already very close to creating your first package.\n\n\n\n\n\n\nTip\n\n\n\nHit Cmd+Alt+Shift+R6 while inside a function definition with your cursor. When working with R Studio, it will create a nifty Roxygen skeleton with all your function’s parameters.\n\n\n\n#' Create Basic Descriptive Statistics\n#'\n#' Creates means, standard deviations and default\n#' quantiles from an numeric input vector. \n#' \n#' @param distr numeric vector drawn from an\n#' arbitraty distribution. \n#' @export \ncreate_basic_desc &lt;- function(distr){\n  out &lt;- list(\n    mean = mean(distr),\n    sd = sd(distr),\n    quantiles = quantile(distr)\n  )\n  out\n}\n\nWriting reusable code will improve your ability to remember syntax and apply concepts to other problems. The more you do it, the easier and more natural it becomes. Just like a toddler figuring out how to speak in a natural language. At first, progress seems small, but once kids understand the bits and pieces of a language, they start building at a remarkable speed, learn and never forget again.\n\n\n3.2.2 Documentation\nFirst things first. Write the first bit of documentation before your first line of code. Documentation written with hindsight will always be written with an all-knowing, smartest-person-in-the-room mindset and the motivation of someone who already gave their best programming. Understand, I am not talking about the fine-tuning here, but about a written outline. Describe how parts of the code are going to do stuff. Also, examples can’t hurt to illustrate what you meant. Research projects often take breaks, and getting back to work after months should be as easy as possible.\nPseudocode is a good way of writing up such an outline documentation. Take a simple application programming interface (API) wrapper, for example. Assume there is an API that returns numeric IDs of hit entries when queried for keywords. These IDs can be passed on to yet another endpoint, to obtain a profile. A rough game plan for an API Wrapper could look like this:\n\n# function: \n# keyword_search(\n#  keyword,\n#  url = \"https://some.default.url.com\"\n#)\n# returns numeric ids according to some\n# api documentation\n\n# function: query_profile(vec_in_ids)\n# a json object that should be immediately\n# turned into list by the function, \n# returns list of properties\nDocumentation should use your ecosystem’s favorite documentation framework. Yet, your comments within the code are the raw, initial form of documentation. Comments help to understand key parts of a program as well as caveats. Comments help tremendously during development time, when debugging or coming back to a project. Let alone when joining a project started by others.\nWhile pseudocode where comments mimmick code itself is the exception to that rule, good comments should always follow the not-what-but-why principle. Usually, most high-level programming languages are fairly easy to read and remind of rudimentary English. Therefore, a what comment like this is considered rather useless:\n\n# compute the cumulative sum of a vector\ncumsum(c(T,F,F,F,F,T,F,F,T,F,F,F,T))\n\nWhereas this why comment may actually be helpful:\n\n# use the fact that TRUE is actually stored as 1 \n# to create a sequence until the next true\n# this is useful for splitting the data later on.\ncumsum(c(T,F,F,F,F,T,F,F,T,F,F,F,T))\n\nComment on why you do things, especially with which plan for future use in mind. Doing so will certainly foster exchange with others who enter or re-visit the code at a later stage (including yourself).\n\n\n3.2.3 Design Your Interface\nIn many languages, it is fairly common to define the data type of both: the input and the output7. Though doing so is not necessary in R, it is good practice to define the types of all parameters and results in your comments/documentation.\nOnce you know a bit more about your direction of travel, it’s time to think about how to modularize your program. How do different parts of the program play together? How do users interact with your program? Will your code just act as a storage pit of tools, a loose collection of commands for ad hoc use? Are others using the program, too? Will there be machine-to-machine interaction? Do you need a graphical user interface (GUI) like a Shiny app?\nThese questions will determine whether you use a strictly functional approach8, a rudimentary form of object orientation like S39 (Hadley Wickham 2019), a stricter implementation like [R6 (Chang 2021) or something completely exotic. There are plenty of great resources out there, so I will not elaborate on this for the moment. The main message of this section is: Think about the main use case. Is it interactive? Is it a program that runs in batch, typically? Do your users code? Would they prefer a GUI?\n\n\n3.2.4 Dependencies\nOne important word of advice for novice package developers is to think about your dependencies. Do not take dependencies lightly. Of course, it is intriguing to stand on the shoulders of giants. Isn’t R great because of its over 20K extension packages? Isn’t exactly this was made R such as popular language?\nYes, extension packages are cool. Yes, the ease with which CRAN packages are distributed is cool. But, just because packages are easy to install and free of license costs, it does not mean leaning on plenty of packages comes at no costs: One needs to stay informed about updates, issues, breaking changes or undesired interdependencies between packages.\nThe problem is mitigated a bit when (a) package is required in an interactive script and (b) one is working with a very popular package. Well-managed packages with a lot of reverse dependencies tend to deprecate old functionality more smoothly, as authors are aware of the issues breaking changes cause to a package’s ecosystem.\nIn R, the tidyverse bundle of packages seems ubiquitous and easy to use, but it leads to quite a few dependencies. The data.table ecosystem might be less popular but provides its functionality with a single R package dependency (the {methods} package).\nOften it does not take much to get rid of dependency:\n\nlibrary(stringr)\ncow &lt;- \"A cow sounds off: mooooo\"\nstr_extract(cow,\"moo+\")\n\n[1] \"mooooo\"\n\n\nSure, the above code is more intuitive, but shrewd use of good ol’ gsub and back-referencing allows you to do the very same thing in base R.\n\ngsub(\"(.+)(mooo+)\",\"\\\\2\",cow)\n\n[1] \"mooooo\"\n\n\nAgain, {stringr} (Hadley Wickham 2022) is certainly a well-crafted package, and it is definitely not the worst of all packages. But when you just loaded a package because it adds convenience to one single line or worse just because you found your solution online, think again before adding more dependencies to a production environment.\n\n\n3.2.5 Folder Structure\nIn R, packages may have the following folders. Note that this does not mean a package has to contain all of these folders. FWIW, an R package, needs to have NAMESPACE and DESCRIPTION files, but that is not the point here. Also, there are more comprehensive, better books on the matter than this little section. The point of this section though is to discuss the role of folders and how they help you structure your work, even if you don’t want to create an R package in the first place.\nThis chapter describes the role of different folders in a package and what these folders are good for. More likely than not, this will cover a lot of the aspects of your project, too.\n\nR\ndata\ndocs\nvignettes\nsrc\ninst\nman\n\nThe below description explains the role of all of these folders.\nR\nThe R folder stores function definitions as opposed to function calls. Typically every function goes into a separate file. Sometimes, it makes sense to group multiple functions into a single file when functions are closely related. Another reason for putting more than one function into a single file is when you have a collection of relatively simple, short helper functions. The R folder must not contain calls10.\n\nmy_func_def &lt;- function(param1, param2){\n  # here goes the function body, i.e.,\n  # what the function does\n  a &lt;- (param1 + param2) * param3\n  # Note that in R, return statements are\n  # not necessary and even\n  # relatively uncommon, R will return\n  # the last unassigned statement\n  return(a)\n}\n\nman\nThis folder contains the context manual of your package. What you’ll find here is the so-called function reference, basically a function and dataset-specific documentation. It’s what you see when you run ?function_name. The content of the man/ folder is usually created automatically from the Roxygen style documentation (note the #’ styled comments) during a `devtools::document() run. Back in the days when people wore pijamas and lived life slow, the man folder was filled up manually with some LaTeX reminiscent .rd files, but ever since R Studio took over in 2012, most developers use Roxygen and render the function reference part of the documentation from their comments.\n\n#' Sum of Parameters Multiplied by First Input\n#'\n#' This functions only exists as a show case. \n#' It's useless but nevertheless exported\n#' to the NAMESPACE of this\n#' package so users can see it and\n#'  call the function by it's name.\n#'\n#' @param param1 numeric input \n#' @param param2 numeric input \n#' @export\nmy_func_def &lt;- function(param1, param2){\n  # here goes the function body, i.e.,\n  # what the function does\n  a &lt;- (param1 + param2) * param1\n  # Note that in R, return statements are\n  # not necessary and even\n  # relatively uncommon, R will return\n  # the last unassigned statement\n  return(a)\n}\n\ndocs\nThis folder is typically not filled with content manually. When pushed to GitHub a docs folder can easily be published as website using Github Pages11. With GitHub Pages you can host a decently styled modern website for free. Software projects often use GitHub Pages to market a product or project or simply for documentation purposes. All you need to do is check a couple of options inside the Github Web GUI and make sure the docs/ folder contains .md or .html files as well as stylesheets (.css). The latter may sound a bit like Latin to people without a basic web development background, but there is plenty of help. The R ecosystem offers different flavors of the same idea: use a bit of markdown + R to generate website code. There is the {blogdown} R package (Xie, Hill, and Thomas 2017) for your personal website or blog. There is {pkgdown} (Hadley Wickham, Hesselberth, and Salmon 2022) for your packages documentation. And there is even bookdown to write an online book like this. Write the Markdown file, render it as HTML into the docs folder and push the docs folder to GitHub. Done. Your website will be online at username.github.io/reponame. Here is a an example of a {pkgdown} website:\nhttps://mbannert.github.io/timeseriesdb/\ndata\nIf you have file-based data like .csv, .RData, .json or even .xlsx, put them in here. Keeping data in a separate folder inside the project directory helps to keep reference to the data relative. There is nothing more novice than read.csv(\"C:\\mbannert\\My Documents\\some_data.csv\"). Even if you like this book, I doubt you have a folder named ‘mbannert’ on your computer. Ah, and in case you wondered, extensive use of setwd() is even worse. Keep your reference to data (and functions alike) relative. If you are sourcing data from a remote NAS drive as it is common at many universities, you can simply mount this drive to your folder (LTMGTFY: How to mount a network drive Windows/OSX).\n\n\n\n\n\n\nNote\n\n\n\nTo actually bundle data into packages, I recommend to use a data-raw folder for data formats other than native R formats. From the raw folder, go on to process these raw files into R representations that are then stored in data. The {usethis} R package (Hadley Wickham et al. 2023) is a modern boilerplate approach and suggest a smooth packaging workflow.\n\n\nvignettes\nAdmittedly not the most intuitive names for a folder that is supposed to contain articles. Vignettes are part of the documentation of a good package. It’s kind of a description as if you were to write a paper about your package, including some examples of how to use it. For modern packages, vignettes are often part of their package down based online documentation. Feel free, to name this folder differently, though sticking to the convention will make it easier to turn your project into a project at a later stage. This folder typically contains Markdown or RMarkdown files.\nsrc\nThe source folder is just here for the sake of completeness and is not needed in projects that only involve R source code. It’s reserved for those parts of a package that need compilation, e.g., C++ or Fortran source code.\ninst\nWhen you install an R package using install.packages() it will be installed in some deep dungeon on your computer where R lives within your OS. The inst/ folder allows you to ship non-R files with your installation. The files of the inst folder will just be copied into the package root folder inside your installation of that package.\ninst is also a great place to store experimental function calls or playground files once the package ambitions become more concrete, and those type of files do not live conveniently in the project root anymore. Also, I sometimes put Shiny apps for local use into the inst/ folder if I want to make them part of a package.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Programming 101</span>"
    ]
  },
  {
    "objectID": "programming.html#naming-conventions-snake-camel-or-kebab-case",
    "href": "programming.html#naming-conventions-snake-camel-or-kebab-case",
    "title": "3  Programming 101",
    "section": "3.3 Naming Conventions: Snake, Camel or Kebab Case",
    "text": "3.3 Naming Conventions: Snake, Camel or Kebab Case\nLet me drop a quick and general note on naming. As in how to name files, folders and functions. It may look like a mere detail, but concise formatting and styling of your code will be appreciated by your peers and by those you ask for help. Plus, following an established convention will not make you look like a complete novice.\n\nDo not use spaces in folder or file names! Never. If you need lengthy descriptions, use underscores ’_‘, dashes’-’ or camelCase.\nAvoid umlauts and special characters. Encoding and internationalization is worth a book of its own. It’s not like modern programming environments can’t handle it, but encoding will introduce further complications. These are exactly the type of complications that may lead to an unplanned, frustrating waste of hours. You may be lucky enough to find a quick fix, but you may as well not. Avoid encoding issues if you do not plan to build a deeper understanding of encoding on the fly. This is especially true for cross-platform collaborations (Windows vs. Unix/OSX).\neither go for camelCase, snake_case or kebab-case. Otherwise, prefer lower-case characters. Also make sure to not switch styles within a project. There a plenty of style guides around, go with whatever your lab or community goes.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Programming 101</span>"
    ]
  },
  {
    "objectID": "programming.html#testing",
    "href": "programming.html#testing",
    "title": "3  Programming 101",
    "section": "3.4 Testing",
    "text": "3.4 Testing\nOne of the things that help scientists and business analysts reach the next level in programming is to develop an understanding of testing the way software engineers use the term. The colloquial understanding of testing basically comes down to doing a couple of dry runs before using code in production. Looking at tests as a systematic and standardized procedure manifested in code substantially improves the quality and reliability of one’s code.\nWhen writing software for statistical analysis, testing mostly refers to unit tests. Unit tests are expectations expressed in code often using a testing framework to help define expectations. In R the {testthat} (Hadley Wickham 2011) or {tinytest} R packages (van der Loo 2020) are examples of such frameworks.\n\n# the function\ncow &lt;- function(){\n  \n  sound_of_a_cow &lt;- \"moo\"\n  sound_of_a_cow\n  \n}\n\n# A test that uses the \n# testthat package to formulate \n# and check expectations\nlibrary(testthat)\ntest_that(\"The cow still mooos...\", {\n  expect_gte(nchar(cow()),3)\n  expect_true(grepl(\"^m\",cow()))\n})\n\nTest passed 🥳\n\n\nThe above dummy function simply returns a character. The accompanying test checks whether the “moo” of the cow is loud enough (=has at least 3 characters) and whether it starts with “m” (so it’s not a “woof”). Note how tests typically come in bunches to thoroughly test functionality.\nSo why don’t we write code correctly in the first place instead of writing code to check everything that could eventually go wrong? When developing new features, we might be confident that newly introduced features do not break existing functionality. At least until we test it :) . Experience proves that seemingly unrelated edits do cause side effects. That is why well-maintained packages have so-called unit tests that are run when the package is rebuilt. If one of the checks fails, the developer can take a look before the a change that broke the code is actually deployed. To foster the development of these type of tests, there are unit testing frameworks for many languages and frameworks.\nHadley’s book R packages (H. Wickham 2015) has a more thorough introduction to testing in R with the {testthat} (Hadley Wickham 2011) package. Though the book is focused on R, its introduction to formal testing is very illustrative for anyone interested to add testing to their development routine.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Programming 101</span>"
    ]
  },
  {
    "objectID": "programming.html#debugging",
    "href": "programming.html#debugging",
    "title": "3  Programming 101",
    "section": "3.5 Debugging",
    "text": "3.5 Debugging\nIn programming, there is no way around debugging. From copy&paste artists to the grandmasters of hacking: writing code implies the need to debug. One of the main differences between amateur and professional is the degree to which the hunt for errors, a.k.a. bugs, is done systematically. This section gives a few tips to help organize a debugging strategy and assess how much honing of one’s debugging approach is reasonable for your individual needs.\n\n3.5.1 Read Code from the Inside Out\nMany scripting languages allow some form of nesting code. In order to understand what’s going on, reading and running code from innermost element first helps. Even if you are not an R user, applying the inside-out idea helps to understand what’s going on. Consider the following piece of R code:\n\nidentical(sum(cumsum(1:10)),sum(cumsum(rev(1:10))))\n\n[1] FALSE\n\n\nThe re-formatted version below helps to identify the innermost part of the code immediately:\n\nidentical(\n  sum(\n      cumsum(1:10)\n    ),\n  sum(\n    cumsum(\n      rev(1:10)\n      )\n    )\n  )\n\nTo understand the above demo code, let’s take a closer look at the innermost element(s). Also consider looking at the single function’s documentation, e.g., ?rev:\n\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nrev(1:10)\n\n [1] 10  9  8  7  6  5  4  3  2  1\n\n\nThe calls to cumsum() and sum() are the next layers. Finally, identical() is the outermost function.\n\n\n3.5.2 Debugger, Breakpoints, Traceback\nTypically modern interpreters and/or source code editors (see chapter 4 on IDEs) provide support to make debugging more systematic. In R, you can use debug(function_name) to activate debug mode for one particular function. On the next call of function_name(), the interpreter will go through the function, executing its source line-by-line. One of the insightful things about it is that standard exploration functions like ls() list all objects within the private environment of that function (by default ls() would just list all objects within the global environment). Inspection of objects as they are seen by the functions helps to find out whether parameters are passed on correctly (or at all). Often, an error message from execution of a function motivates a debugging session. In that case, try to identify the line that causes the error and just do all the object inspection right before the line that causes the crash.\nAnother, similar approach is to use breakpoints which are a feature of your editor. You can activate a break point to set execution of a function to debug mode by clicking next to a line in the source of the function that should trigger the switch to debug mode. Breakpoints may be the more convenient version of using the debug tool ‘per pedes’ as described above because of its ability to follow function dispatch across multiple (wrapper) functions. Consider the following demo function:\n\ndebug_me &lt;- function(a,b){\n  out &lt;- list()\n  out$add &lt;- a + b\n  out$prod &lt;- a * b\n  out$dev &lt;- a / b\n  out$sub &lt;- a - b\n  out\n}\n\ndebug_me(1,2)\n\n$add\n[1] 3\n\n$prod\n[1] 2\n\n$dev\n[1] 0.5\n\n$sub\n[1] -1\n\n\nNow let’s give this function a bad argument. (Unlike Python, R’s addition operation will not simply concatenate strings when facing string input.)\n\ndebug_me(\"1\",\"2\")\n# if evaluated this would return\nError in a + b : non-numeric argument\nto binary operator\n\nMotivated by this error message, let’s switch into debug mode\n\ndebug(debug_me)\n\nThe below screenshot shows line-by-line execution of our demo function. The highlighted line marks the line which will be executed on the next press of the return key. R’s debug mode can be stopped by either executing the erroneous line or by executing a capital Q command in the R console window.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Programming 101</span>"
    ]
  },
  {
    "objectID": "programming.html#a-word-on-peer-programming",
    "href": "programming.html#a-word-on-peer-programming",
    "title": "3  Programming 101",
    "section": "3.6 A Word on Peer Programming",
    "text": "3.6 A Word on Peer Programming\nPeer programming, also called pair programming, just means that two developers sit in front of the same screen to collaborate on a piece of code. So why is there such a buzz about it? Why is there even a term for it? And why is there a section in an applied book on it?\nThat is because novice programmers (and their scientific supervisors) often doubt the efficiency of two paid persons working at the same workstation. But programming is not about digging a hole with two shovels. Particularly not when it comes to building the software basis or frame of a project.\nWorking together using one single keyboard and screen or the virtual equivalent thereof can be highly efficient. The virtual equivalent, i.e., in fact, using two computers but sharing the screen while in call, helps tremendously with (a) your concept, (b) your documentation. Plus, it is a code review at the same time. But most importantly, both developers learn from each other. Having to explain and being challenged, deepens the understanding of experienced developers and ruthlessly identifies holes in one’s knowledge. One important advice when peer programming is to switch the driver’s seat from time to time. Make sure the lesser programmer holds the keys occasionally and maneuvers through articles, repositories, code and data. Doing so prevents the co-pilot from taking a back seat and letting the veteran do the entertainment. Visual Studio Code Live Share12 is a great feature for next-level virtual peer programming, as it allows for two drivers using two cursors.\nOf course, there are downsides of the pair programming approach, too. Also, timing within the lifecycle of a project is an important factor and not every project is the same fit for this agile method. But given there are so many approaches, I will leave the back and forth to others. The goal of this section is to point the reader to a practical approach that tends to work well in programming with data setups in social sciences. Googlers Jeff Dean and Sanjay Ghemawat had their fair share of success, too, according to The New Yorker’s The Friendship That Made Google Huge13.\n\n\n\n\nChang, Winston. 2021. R6: Encapsulated Classes with Reference Semantics. https://CRAN.R-project.org/package=R6.\n\n\nvan der Loo, MPJ. 2020. “A Method for Deriving Information from Running r Code.” The R Journal, Accepted for publication. https://arxiv.org/abs/2002.07472.\n\n\nWickham, H. 2015. R Packages: Organize, Test, Document, and Share Your Code. O’Reilly Media. https://r-pkgs.org/.\n\n\nWickham, Hadley. 2011. “Testthat: Get Started with Testing.” The R Journal 3: 5–10. https://journal.r-project.org/archive/2011-1/RJournal_2011-1_Wickham.pdf.\n\n\n———. 2019. Advanced r. 2nd ed. Chapman & Hall/CRC the r Series. Taylor & Francis.\n\n\n———. 2022. Stringr: Simple, Consistent Wrappers for Common String Operations. https://CRAN.R-project.org/package=stringr.\n\n\nWickham, Hadley, Jennifer Bryan, Malcolm Barrett, and Andy Teucher. 2023. Usethis: Automate Package and Project Setup. https://CRAN.R-project.org/package=usethis.\n\n\nWickham, Hadley, Peter Danenberg, Gábor Csárdi, and Manuel Eugster. 2022. Roxygen2: In-Line Documentation for r. https://CRAN.R-project.org/package=roxygen2.\n\n\nWickham, Hadley, Jay Hesselberth, and Maëlle Salmon. 2022. Pkgdown: Make Static HTML Documentation for a Package. https://CRAN.R-project.org/package=pkgdown.\n\n\nXie, Yihui, Alison Presmanes Hill, and Amber Thomas. 2017. Blogdown: Creating Websites with R Markdown. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/blogdown/.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Programming 101</span>"
    ]
  },
  {
    "objectID": "programming.html#footnotes",
    "href": "programming.html#footnotes",
    "title": "3  Programming 101",
    "section": "",
    "text": "https://pandas.pydata.org/↩︎\nhttps://www.scipy.org/↩︎\nhttps://numpy.org/↩︎\nGoogle me!↩︎\nhttps://packaging.python.org/tutorials/packaging-projects/↩︎\nOn Windows/Linux use Ctrl instead of Cmd.↩︎\nSee statically typed language vs. dynamically typed language.↩︎\nhttp://adv-r.had.co.nz/Functional-programming.html↩︎\nhttp://adv-r.had.co.nz/OO-essentials.html↩︎\nEssentially, examples are calls, too. Note, I do recommend adding examples. Hadley Wickham’s guide to documenting functions within packages (H. Wickham 2015) shows how to add examples correctly.↩︎\nhttps://pages.github.com/↩︎\nhttps://visualstudio.microsoft.com/services/live-share/↩︎\nhttps://www.newyorker.com/magazine/2018/12/10/the-friendship-that-made-google-huge↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Programming 101</span>"
    ]
  },
  {
    "objectID": "interaction.html",
    "href": "interaction.html",
    "title": "4  Interaction Environment",
    "section": "",
    "text": "4.1 Integrated Development Environments\nWhile some prefer lightning-fast editors such as Sublime Text that are easy to customize but rather basic out-of-the-box, Integrated Development Environments (IDEs) are the right choice for most people. In other words, it’s certainly possible to move a five-person family into a new home by public transport, but it is not convenient. The same holds for (plain) text editors in programming. You can use them, but most people would prefer an IDE just like they prefer to use a truck over public transport when they move. IDEs are tailored to the needs and idiosyncrasies of a language, some working with plugins and covering multiple languages. Others have a specific focus on a single language or a group of languages.\nThe below sections will focus on data science’ most popular editors, namely Visual Studio Code and RStudio. Hence, I would like to mention at least some IDE juggernauts here, for the reader looking for alternatives: Eclipse (mostly Java but tons of plugins for other languages), or JetBrains’ IntelliJ (Java) and PyCharm (Python).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interaction Environment</span>"
    ]
  },
  {
    "objectID": "interaction.html#integrated-development-environments",
    "href": "interaction.html#integrated-development-environments",
    "title": "4  Interaction Environment",
    "section": "",
    "text": "4.1.1 RStudio\nPosit’s RStudio has become the default environment for most R people and those who mostly use R but C or Python and Markdown, in addition. The open source version ships for free as RStudio Desktop and RStudio Server Open Source. In addition, the creators of RStudio offer commercially supported versions of both the desktop and server version (Posit Workbench). If you want your environment to essentially look like the environment of your peers, RStudio is a good choice. To have the same visual image in mind can be very helpful in workshops, coaching or teaching.\nRStudio has four control panes which the user can fill with a different functionality like script editor, R console, terminal, file explorer, environment explorer, test suite, build suite and many others. I advise newcomers to change the default to have the script editor and the console next to each other (instead of below each other). That is because (at least in the Western world) we read from left to right and send source code from left to right to execute it in the console. Combine this practice with the run selection shortcut (cmd+enter or ctrl+enter on a PC) and you have gained substantial efficiency compared to leaving your keyboard, reaching for your mouse and finding the right button. In addition, this workflow should allow you to see larger chunks1 of your code as well as your output.\n\nExplore Extensions\n\nexplore add-ins\nexplore the API\n\n\n\nFavorite Shortcuts\n\nuse cmd+enter (ctrl+enter on PCs) to send selected code from the script window (on the left) to the console (on the right)\ncmd+shift+option+R, while the cursor is within a function’s body (create a Roxygen documentation skeleton)\nuse ctrl 1,2 to switch between console and script pane\n\n\n\nPitfalls\n\nyou may stumble over RStudio’s defaults, such as storing your global environment on exit and thus resurrecting long forgotten objects impacting your next experiment through, e.g., lexical scoping.\nRStudio’s git integration abstracts git essentials away, so it hampers understanding of what’s going on.\n\nFor R, the Open Source Edition of RStudio Desktop is the right choice for most people. (If you are working in a team, R Studio’s server version is great. It allows having a centrally managed server which clients can use through their web browser without even installing R and RStudio locally.) R Studio has solid support for a few other languages often used together with R, plus it’s customizable. The French premier thinkR Colin_Fay gave a nice tutorial on Hacking RStudio at the useR! 2019 conference.\nBack in fall 2020, long before RStudio turned into Posit, the company already indicated that data science was not about R vs. Python to them (Remember the first section of Chapter 3 of this book: The Choice That Doesn’t Matter of this book?)\n\n\n\nCalled RStudio back then, the company called Posit today already indicated years ago, it was not solely about R. (Source: rstudio.com website in 2021)\n\n\n\n\n\n4.1.2 Visual Studio Code\nOutside the R universe (and to some degree even inside it), Visual Studio Code became the go-to editor in data science. Microsoft’s Visual Studio Code started out as a modular, general purpose IDE not focused on a single language. Meanwhile, there is not only great Python support, but also auto-completion, code highlighting for R or Julia and many other languages. VS Code Live Share is just one rather random example of its remarkably well-implemented features. Live share allows developers to edit a document simultaneously using multiple cursors similarly to Google Docs, but with all the IDE magic in a Desktop client.\n\n\n4.1.3 Editors on Steroids\nAnother approach is to go for a highly customizable editor such as Sublime2 or Atom3. The idea is to send source code from the editor to interchangeable read-eval-print-loop (REPL)s) which can be adapted according to the language that needs to be interpreted. That way a good linter/code highlighter for your favorite language is all you need to have a lightweight environment to run things. An example of such a customization approach is Christoph Sax’s small project Sublime Studio4.\nFor readers with Unix experience, vim5 may just be the most ubiquitous editor of them all, to everyone else it may just live in deep nerd territory. The fact that a quick online search for “How to quit vim” came back with more than 56.4 million (!) results shows that both perspectives have a point. With vim, users can switch between a move around and insert mode. The former allows the users to use single letters as shortcuts to navigate a text file instead of typing the actual letter. This enables users to do things like move-three-words-forward or delete the next three lines and many other more complex things. Given some practice and regular use, it is easy to imagine that vim wizards can navigate their source code incredibly quickly.\nBack when IDEs were less comfortable and often clunky due to their heavy lifting, a broader group of people had their incentives to invest into vim. Now that IDEs became so much better and many of them even offering vim modes or plugins, the point of contact with vim for a data scientist is mostly Unix server administration or work inside containers. GNU Emacs6 is another noteworthy editor because, even though much more exotic, it is popular among long-tenured R folks thanks to the Emacs Speaks Statistics extension.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interaction Environment</span>"
    ]
  },
  {
    "objectID": "interaction.html#notebooks",
    "href": "interaction.html#notebooks",
    "title": "4  Interaction Environment",
    "section": "4.2 Notebooks",
    "text": "4.2 Notebooks\nNotebooks are yet another popular choice for a home court among data scientists and analysts. Though most often associated with the Python world, notebooks are language-agnostic and also common for the R and Julia languages. The basic idea of notebooks is to run web server in the background to present a browser-based frontend to the developer. The difference between the Markdown rendering approach described in Chapter 10 about publishing is that the browser is not just used to display the result, but it is also the environment to interactively add commands. The resulting workflow, e.g., a data analysis in the making feels like a social media timeline: execute a command, receive a result posted on a web page printed below the command, again posting a prompt to expect the next command. This way we get an endless scroll of commands, analysis and descriptive text in between.\nConsider the following example, drawing a basketball court using the matplotlib Python package. This example uses screenshots of a notebook to illustrate the difference between a notebook and concepts like RMarkdown. Note the prompts in between the results!\n\nThe above figure depicts a Markdown element in editing mode, showing the Markdown syntax (double ## for a section header of type 2). The second element is a chunk of Python code to import two well-known Python libraries.\n\nAfter the function definition, we see another Markdown section header element that says Call the Function – this time already rendered to HTML. Finally, we call the Python function defined above in another code chunk element7.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interaction Environment</span>"
    ]
  },
  {
    "objectID": "interaction.html#consoleterminal",
    "href": "interaction.html#consoleterminal",
    "title": "4  Interaction Environment",
    "section": "4.3 Console/Terminal",
    "text": "4.3 Console/Terminal\nIn addition to the editor with which you will spend most of your time, it is also worthwhile to put some effort into configuring keyboard-driven interaction with your operating systems. And again, you do not need to be a terminal virtuoso to easily outperform mouse pushers, a terminal carpentry level is enough. Terminals come in all shades of gray, integrated into IDEs, built into our operating system or as pimped third-party applications. A program called a shell runs inside the terminal application to run commands and display output. bash is probably the most known shell program, but there are tons of different flavors. FWIW, I love fish8 shell (combined with iterm2) for its smooth auto-completion, highlighting and its extras.\nIn the Windows world, the use of terminal applications has been much less common than for OSX/Linux – at least for non-systemadmins. Git Bash, which ships with git installations on Windows, mitigates this shortcoming, as it provides a basic Unix style console for Windows. For a full-fledged Unix terminal experience, I suggest using a full terminal emulator like CYGWIN. More recent, native approaches like powershell brought the power of keyboard interaction at the OS level to a broader Windows audience – albeit with different Windows specific syntax. The ideas and examples in this book as shown in the below table are limited to Unix flavored shells.\n\nBasic Unix Terminal Commands\n\n\n\n\n\n\nCommand\nWhat it does\n\n\n\n\nls\nlist files (and directories in a directory)\n\n\ncd\nchange directory\n\n\nmv\nmove file (also works to rename files)\n\n\ncp\ncopy files\n\n\nmkdir\nmake directory\n\n\nrmdir\nremove directory\n\n\nrm -rf\n(!) delete everything recursively. DANGER: shell does not ask for confirmation and just wipes out everything.\n\n\n\n\n4.3.1 Remote Connections SSH, SCP\nOne of the most important use cases of the console for data analysts is the ability to log into other machines, namely servers that most often run on Linux. Typically, we use the SSH protocol to connect to a (remote) machine that allows to connect through port 22.\n\n\n\n\n\n\nNote\n\n\n\nNote that sometimes firewalls limit access to ports other than those needed for surfing the web (8080 for http:// and 443 for https://) so you can only access port 22 inside an organization’s VPN network.\n\n\nTo connect to a server using a username and password, simply use your console’s SSH client like this:\nssh mbannert@someserver.org\nYou will often encounter another login procedure, though. SSH key pair authentication is more secure and therefore preferred by many organizations. You need to make sure the public part of your key pair is located on the remote server and hand the ssh command the private file:\nssh -i ~/.ssh/id_rsa mbannert@someserver.org\nFor more details on SSH key pair authentication, check this example from the case studies in Chapter 10 Section 11.1.\nWhile SSH is designed to log in to a remote server and from then on, issue commands like the server was a local Linux machine, scp copies files from one machine to another.\nscp -i ~/.ssh/id_rsa ~/Desktop/a_file_on_my_desktop\n mbannert@someserver.org:/some/remote/location/\nThe above command copies a file dwelling on the user’s desktop into a /some/remote/location on the server. Just like SSH, secure copy (scp) can use SSH key pair authentication, too.\n\n\n4.3.2 Git Through the Console\nAnother common use case of the terminal is managing git . Admittedly, there is git integration for many IDEs that allows you to point and click your way to commits, pushes and pulls as well as dedicated git clients like GitHub Desktop or Source Tree. But there is nothing like the console in terms of speed and understanding what you really do. Chapter 5 sketches an idea of how to operate git through the console from the very basics to a feature branch-based workflow.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interaction Environment</span>"
    ]
  },
  {
    "objectID": "interaction.html#footnotes",
    "href": "interaction.html#footnotes",
    "title": "4  Interaction Environment",
    "section": "",
    "text": "Many coding conventions recommend having no more than 80 characters in one line of code. Sticking to this convention should prevent cutting off your code horizontally.↩︎\nhttps://www.sublimetext.com/↩︎\nhttps://atom.io/↩︎\nhttps://github.com/christophsax/SublimeStudio↩︎\nhttps://www.vim.org/↩︎\nhttps://www.gnu.org/software/emacs/↩︎\nThough notebooks were originally designed to run on a (local) Python based webserver and to be used in a web browser, there is a neat, Electron-based standalone app called JupyterLab. I have used this app for the illustration in this book because of its slim, no-nonsense interface that unlike browsers comes without distractions from plugins.↩︎\nhttps://fishshell.com/↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interaction Environment</span>"
    ]
  },
  {
    "objectID": "version-control.html",
    "href": "version-control.html",
    "title": "5  Git Version Control",
    "section": "",
    "text": "5.1 What Is Git Version Control?\nGit is a decentralized version control system. It manages different versions of your source code (and other text files) in a simple but efficient manner that has become the industry standard: The git program itself is a small console program that creates and manages a hidden folder inside the folder you put under (you know those folders with a leading dot in their folder name, like .myfolder). This folder keeps track of all differences between the current version and other versions before the current one.\nThe key to appreciating the value of git is to appreciate the value of semantic versions. Git is not Dropbox nor Google Drive. It does not sync automagically (even if some Git GUI Tools suggest so). GUI tools GitHub Desktop1, Atlassian’s Source Tree2 and Tortoise3 are some of the most popular choices if you are not a console person. Though GUI tools may be convenient, we will use the git console throughout this book to improve our understanding. As opposed to the sync approaches mentioned above, a system allows summarizing a contribution across files and folders based on what this contribution is about. Assume you got a cool pointer from an econometrics professor at a conference, and you incorporated her advice in your work. That advice is likely to affect different parts of your work: your text and your code. As opposed to syncing each of these files based on the time you saved them, creates a version when you decide to bundle things together and to commit the change. That version could be identified easily by its commit message “incorporated advice from Anna (discussion at XYZ Conference 2020)”.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Git Version Control</span>"
    ]
  },
  {
    "objectID": "version-control.html#what-is-git-version-control",
    "href": "version-control.html#what-is-git-version-control",
    "title": "5  Git Version Control",
    "section": "",
    "text": "Meaningful commit messages help to make sense of a project’s history. Screenshot of a commit history on GitHub. (Source: own GitHub repository.)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Git Version Control</span>"
    ]
  },
  {
    "objectID": "version-control.html#why-use-version-control-in-research",
    "href": "version-control.html#why-use-version-control-in-research",
    "title": "5  Git Version Control",
    "section": "5.2 Why Use Version Control in Research?",
    "text": "5.2 Why Use Version Control in Research?\nA based workflow is a path to your goals that rather consists of semantically relevant steps instead of semantically meaningless chunks based on the time you saved them.\nIn other, more blatant, applied words: naming files like final_version_your_name.R or final_final_correction_collaboratorX_20200114.R is like naming your WiFi dont_park_the_car_in_the_frontyard or be_quiet_at_night to communicate with your neighbors. Information is supposed to be sent in a message, not a file name. With , it is immediately clear what the most current version is, no matter the file name. There is no room for interpretation. There is no need to start guessing about the delta between the current version and another version.\nAlso, you can easily try out different scenarios on different branches and merge them back together if you need to. is a well-established industry standard in software development. And it is relatively easy to adopt. With datasets growing in complexity, it is only natural to improve management of the code that processes these data.\nAcademia has probably been the only place that would allow you to dive into hacking at somewhat complex problems for several years without ever taking notice of . As a social scientist who rather collaborates in small groups and writes moderate amount of code, have you ever thought about how to collaborate with more 100 than persons in a big software project? Or to manage 10,000 lines of code and beyond? is an important reason these things work. And it’s been around for decades. But enough about the rant.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Git Version Control</span>"
    ]
  },
  {
    "objectID": "version-control.html#how-does-git-work",
    "href": "version-control.html#how-does-git-work",
    "title": "5  Git Version Control",
    "section": "5.3 How Does Git Work?",
    "text": "5.3 How Does Git Work?\nThis introduction tries to narrow things down to the commands that you’ll need if you want to use git in similar fashion to what you learn from this book. If you are looking for more comprehensive, general guides, three major git platforms, namely, Atlassian’s Bitbucket, GitHub and GitLab offer comprehensive introductions as well as advanced articles or videos to learn git online.\nThe first important implication of decentralized is that all versions are stored on the local machines of every collaborator, not just on a remote server (this is also a nice, natural backup of your work). So let’s consider a single local machine first.\n\n\n\nSchematic illustration of an example git workflow including remote repositories. (Source: own illustation.)\n\n\nLocally, a git repository consists of a checkout which is also called current working copy. This is the status of the file that your file explorer or your editor will see when you use them to open a file. To check out a different version, one needs to call a commit by its unique commit hash and check out that particular version.\nIf you want to add new files to or bundle changes to some existing files into a new commit, add these files to the staging area, so they get committed next time a commit process is triggered. Finally, committing all these staged changes under another commit ID, a new version is created.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Git Version Control</span>"
    ]
  },
  {
    "objectID": "version-control.html#moving-around",
    "href": "version-control.html#moving-around",
    "title": "5  Git Version Control",
    "section": "5.4 Moving Around",
    "text": "5.4 Moving Around\nSo let’s actually do it. Here’s a three-stage walk-through of git commands that should have you covered in most use cases a researcher will face. Note that git has some pretty good error messages that guess what could have gone wrong. Make sure to read them carefully. Even if you can’t make sense of them, your online search will be a lot more efficient when you include these messages.\nStage 1: Working Locally\nTable 5.1 summarizes essential git commands to move around your local repository.\n\n\n\n\nTable 5.1: Basic Commands for Working with Git\n\n\n\n\n\n\n\n\n\n\nCommand\nEffect\n\n\n\n\ngit init\nputs current directory and all its subdirs under version control.\n\n\ngit status\nshows status\n\n\ngit add filename.py\nadds file to tracked files\n\n\ngit commit -m “meaningful msg”\ncreates a new version/commit out of all staged files\n\n\ngit log\nshows log of all commit messages on a branch\n\n\ngit checkout some-commit-id\ngoes to commit, but in detached HEAD state\n\n\ngit checkout main-branch-name\nleaves temporary state, goes back to last commit\n\n\n\n\n\n\n\n\nStage 2: Working with a Remote Repository\nThough git can be tremendously useful even without collaborators, the real fun starts when working together. The first step en route to getting others involved is to add a remote repository. Table 5.2 shows essential commands for working with a remote repository.\n\n\n\n\nTable 5.2: Commands for Working with a Remote Repository\n\n\n\n\n\n\n\n\n\n\nCommand\nEffect\n\n\n\n\ngit clone\ncreates a new repo based on a remote one\n\n\ngit pull\ngets all changes from a linked remote repo\n\n\ngit push\ndeploys all commit changes to the remote repo\n\n\ngit fetch\nfetches branches that were created on remote\n\n\ngit remote -v\nshows remote repo URL\n\n\ngit remote set-url origin https://some-url.com\nsets URL to remote repo\n\n\n\n\n\n\n\n\nStage 3: Branches\nBranches are derivatives from the main branch that allow to work on different features at the same time without stepping on someone else’s feet. Through branches, repositories can actively maintain different states. Table 5.3 shows commands to navigate these states.\n\n\n\n\nTable 5.3: Commands for for Handling Multiple Branches\n\n\n\n\n\n\n\n\n\n\nCommand\nEffect\n\n\n\n\ngit checkout -b branchname\ncreates new branch named branchname\n\n\ngit branch\nshows locally available branches\n\n\ngit checkout branchname\nswitches to branch named branchname\n\n\ngit switch branchname\nswitches to branch named branchname\n\n\ngit merge branchname\nmerges branch named branchname into current branch\n\n\n\n\n\n\n\n\nFixing Merge Conflicts\nIn most cases, git is quite clever and can figure out which is the desired state of a file when putting two versions of it together. When git’s recursive strategy is possible, git will merge versions automatically. When the same lines were affected in different versions, git cannot tell which line should be kept. Sometimes, you would even want to keep both changes.\nBut even in such scenario, fixing the conflict is easy. Git will tell you that your last command caused a merge conflict and which files are conflicted. Open these files and take a look at all parts of the files in question. Figure 5.1 shows a situation in which trying merge a file that had changes across different branches caused a conflict.\n\n\n\n\n\n\nFigure 5.1: Ouch! We created a conflict by editing the same line in the same file on different branches. (Source: screenshot of own GitHub repository.)\n\n\n\nLuckily, git marks the exact spot where the conflict happens. Good text editors/IDEs ship with cool colors to highlight all our options. Some of the fancier editors even have git conflict resolve plugins that let you walk through all conflict points.\n\n\n\nGo for the current status or take what’s coming in from the a2 branch? (Source: screenshot of Sublime text editor.)\n\n\n\n\n\nIn VS Code, you can even select the option by clicking. (Source: screenshot VS Code IDE.)\n\n\nAt the end of the day, all do the same, i.e., remove the unwanted part, including all the marker gibberish. After you have done so, save, commit and push (if you are working with a remote repo). Don’t forget to make sure you kinked out all conflicts.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Git Version Control</span>"
    ]
  },
  {
    "objectID": "version-control.html#collaboration-workflow",
    "href": "version-control.html#collaboration-workflow",
    "title": "5  Git Version Control",
    "section": "5.5 Collaboration Workflow",
    "text": "5.5 Collaboration Workflow\nThe broad acceptance of git as a framework for collaboration has certainly played an important role in git’s establishment as an industry standard.\n\n5.5.1 Feature Branches\nThis section discusses real-world collaboration workflows of modern open source software developers. Hence, the prerequisites to benefitting the most from this section are bit different. Make sure you are past the mere ability to describe and explain git basics, make sure you can create and handle your own repositories.\nIf you had only a handful of close collaborators so far, you may be fine with staying on the main branch and trying not to step on each other’s feet. This is reasonable because, git aside, it is rarely efficient to work asynchronously on exact the same lines of code anyway. Nevertheless, there is a reason why feature-branch-based workflows became very popular among developers: Imagine yourself collaborating in asynchronous fashion, maybe with someone in another time zone. Or with a colleague, who works on your project, but in a totally different month during the year. Or, most obviously, with someone you have never met. Forks and feature-branch-based workflows are the way a lot of modern open source projects tackle the above situations.\nForks are just a way to contribute via feature branches, even in case you do not have write access to a repository. But let’s just have a look at the basic feature branch case, in which you are part of the team first with full access to the repository. Assume there is already some work done, some version of the project is already up on a some remote GitHub account. You join as a collaborator and are allowed to push changes now. It’s certainly not a good idea to simply add things without review to a project’s production. Like if you got access to modify the institute’s website, and you made your first changes and all your changes go straight to production. Like this:\n\nBet everybody on the team took notice of the new team member by then. In a feature branch workflow, you would start from the latest production version. Remember, git is decentralized, and you have all versions of your team’s project on your local machine. Create a new branch named indicative of the feature you are looking to work on.\ngit checkout -b colorways\nYou are automatically being switched to the freshly created branch. Do your thing now. It could be just a single commit, or several commits by different persons. Once you are done, i.e., committed all changes, add your branch to the remote repository by pushing.\ngit push -u origin colorways\nThis will add your branch called colorways to the remote repository. If you are on any major git platform with your project, it will come with a decent web GUI. Such a GUI is the most straightforward way to do the next step: get your Pull Request (PR) out.\n\n\n\nGitHub Pull Request dialog: Select the Pull Request; choose which branch you merge into which target branch. (Source: own GitHub repository.)\n\n\nAs you can see, git will check whether it is possible to merge automatically without interaction. Even if that is not possible, you can still issue the pull request. When you create the request, you can also assign reviewers, but you could also do so at a later stage.\nEven after a PR was issued, you can continue to add commits to the branch about to be merged. As long as you do not merge the branch through the PR, commits are added to the branch. In other words, your existing PR gets updated. This is a very natural way to account for reviewer comments.\n\n\n\n\n\n\nNote\n\n\n\nUse commit messages like ‘added join to SQLquery, closes #3’. The keyword ‘closes’ or ‘fixes’, will automatically close issues referred to when merged into the main branch.\n\n\nOnce the merge is done, all your changes are in the main branch, and you and everyone else can pull the main branch that now contains your new feature. Yay!\n\n\n5.5.2 Pull Requests from Forks\nNow, let’s assume you are using an open source software created by someone else. At some point, you miss a feature that you feel is not too hard to implement. After googling and reading up a bit, you realize others would like to have these features, too, but the original authors did not find the time to implement it yet. Hence, you get to work. Luckily, the project is open source and up on GitHub, so you can simply get your version of it, i.e., fork the project to your own GitHub account (just click the fork button and follow the instructions) .\nNow that you have your own version of the software with all the access rights to modify it, you can implement your feature and push it to your own remote git repository. Because you forked the repository, your remote git platform will still remember where you got it from and allows you to issue a pull request to the original author. The original authors can now review the pull request, see the changes, and decide whether they are fine with the feature and its implementation.\nThere may very well be some back and forth in the message board before the pull requests gets merged. But usually these discussions are very context-aware and sooner or later, you will get your first pull request approved and merged. In that case, congratulations – you have turned yourself into a team-oriented open source collaborator!\n\n\n5.5.3 Rebase vs. Merge\nPowerful systems often provide more than one way to achieve your goals. In the case of git, putting together two branches of work – a very vital task, is exactly such a case: We can either merge or rebase branches.\nWhile merge keeps the history intact, rebase is a history-altering command. Though most people are happy with the more straightforward merge approach, a bit of context is certainly helpful.\nImagine the merge approach as a branch that goes away from the trunk at some point and then grows alongside the trunk in parallel. In case both histories become out of sync because someone else adds to the main branch while you keep adding to the feature branch, you can either merge or rebase to put the two together.\nSitting on a checkout of the feature branch, a merge of the main branch would simply create an additional merge commit sequentially after the last commit of the feature branch. This merge commit contains the changes of both branches, no matter if they were automatically merged using the standard recursive strategy or through resolving a merge conflict.\nAs opposed to that, rebase would move all changes to main to before the feature branch started, then sequentially add all commits of the feature branch. That way your history remains linear, looks cleaner and does not contain artificial merge commits.\nSo, when should we merge, and when should we rebase? There is no clear rule to that other than to not use rebase on exposed branches such as main because you would have a different main branch than other developers. Rebase can ruin your collaborative workflow, yet it helps to clean up. In my opinion, merging feature branches is just fine for most people and teams. So unless you have too many people working on too many different features at once and are in danger of not being able to move through your history, simply go with the merge approach. The following Atlassian tutorial4 offers more insights and illustrations to deepen your understanding of the matter.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Git Version Control</span>"
    ]
  },
  {
    "objectID": "version-control.html#footnotes",
    "href": "version-control.html#footnotes",
    "title": "5  Git Version Control",
    "section": "",
    "text": "https://desktop.github.com/↩︎\nhttps://www.sourcetreeapp.com/↩︎\nhttps://tortoisegit.org/↩︎\nhttps://www.atlassian.com/git/tutorials/merging-vs-rebasing↩︎",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Git Version Control</span>"
    ]
  },
  {
    "objectID": "data-management.html",
    "href": "data-management.html",
    "title": "6  Data Management",
    "section": "",
    "text": "6.1 Forms of Data\nIn research and analytics, data appear in a plethora of different forms. Yet, most researchers and business analysts are mainly trained to handle different flavors of two-dimensional data, as in one-observation-one-row. Ad hoc studies conducted once result in cross-sectional data: one line per observation; columns represent variables. Sensor data, server logs or forecasts of official statistics are examples of single variable data observed over time. These single variable, longitudinal data are also known as time series. Multivariate time series, i.e., multivariable, longitudinal data are often referred to as panel data. In our heads, all of these forms of data are typically represented as rectangular, two-dimensional one-line-per-observation, spreadsheet-like tables. Here are a few easy-to-reproduce examples using popular R demo datasets.\nh &lt;- head(mtcars)\nh\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\ndim(h)\n\n[1]  6 11\nThe above output shows an excerpt of the mtcars cross-sectional dataset with 6 lines and 11 variables. Airpassenger is a time series dataset represented in an R ts object which is essentially a vector with time-based index attribute.\nAirPassengers\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1949 112 118 132 129 121 135 148 148 136 119 104 118\n1950 115 126 141 135 125 149 170 170 158 133 114 140\n1951 145 150 178 163 172 178 199 199 184 162 146 166\n1952 171 180 193 181 183 218 230 242 209 191 172 194\n1953 196 196 236 235 229 243 264 272 237 211 180 201\n1954 204 188 235 227 234 264 302 293 259 229 203 229\n1955 242 233 267 269 270 315 364 347 312 274 237 278\n1956 284 277 317 313 318 374 413 405 355 306 271 306\n1957 315 301 356 348 355 422 465 467 404 347 305 336\n1958 340 318 362 348 363 435 491 505 404 359 310 337\n1959 360 342 406 396 420 472 548 559 463 407 362 405\n1960 417 391 419 461 472 535 622 606 508 461 390 432\nLet’s create a multivariate time series (panel) dataset, i.e., multiple variables observed over time:\nd &lt;- data.frame(Var1 = rnorm(10, 0),\n           Var2 = rnorm(10, 10),\n           Var3 = rnorm(10, 30))\nmulti_ts &lt;- ts(d, start = c(2000,1), frequency = 4)\nmulti_ts\n\n               Var1      Var2     Var3\n2000 Q1 -0.46187584  9.301828 30.28219\n2000 Q2  0.09597353  9.478683 30.52734\n2000 Q3  1.44672559  9.808107 29.62971\n2000 Q4  1.54083734  9.599374 28.20309\n2001 Q1 -0.09513387 12.320572 30.10012\n2001 Q2 -2.35218830 10.826742 29.19028\n2001 Q3  0.42442924  8.111077 30.35953\n2001 Q4 -1.11984748 11.900924 28.49679\n2002 Q1 -0.06350694 11.889724 29.63926\n2002 Q2  0.10103366  8.858031 30.50806\nlibrary(tsbox)\nts_dt(multi_ts)[1:15,]\n\n        id       time       value\n    &lt;char&gt;     &lt;Date&gt;       &lt;num&gt;\n 1:   Var1 2000-01-01 -0.46187584\n 2:   Var1 2000-04-01  0.09597353\n 3:   Var1 2000-07-01  1.44672559\n 4:   Var1 2000-10-01  1.54083734\n 5:   Var1 2001-01-01 -0.09513387\n 6:   Var1 2001-04-01 -2.35218830\n 7:   Var1 2001-07-01  0.42442924\n 8:   Var1 2001-10-01 -1.11984748\n 9:   Var1 2002-01-01 -0.06350694\n10:   Var1 2002-04-01  0.10103366\n11:   Var2 2000-01-01  9.30182797\n12:   Var2 2000-04-01  9.47868310\n13:   Var2 2000-07-01  9.80810744\n14:   Var2 2000-10-01  9.59937373\n15:   Var2 2001-01-01 12.32057235\nThe ability to transform data from one format into the other and to manipulate both formats is an essential skill for any data scientist or data engineer. It is important to point out that the ability to do the above transformations effortlessly is an absolute go-to skill for people who want to use programming to run analysis. (Different analyses or visualizations may require one form or the other and ask for quick transformation).\nHence, popular data science programming languages offer great toolsets to get the job done. Mastering these toolboxes is not the focus of this book. R for Data Science and the Carpentries are good starting points if you feel the need to catch up or solidify your know-how.\nYet, not all information suits a two-dimensional form. Handling nested or unstructured information is one of the fields where the strength of a programming approach to data analysis and visualization comes into play. Maps are a common form of information that is often represented in nested fashion. For an example of nested data1, let’s take a look at the map file and code example case study in Section 11.6. In memory, i.e., in our R session, the data is represented in a list that contains multiple list elements and may contain more lists nested inside.\nlibrary(jsonlite)\n\njson_ch &lt;- jsonlite::read_json(\n  \"https://raw.githubusercontent.com/....\"\n)\nls.str(json_ch)\nWarning: Paket 'jsonlite' wurde unter R Version 4.3.3 erstellt\n\n\ncrs : List of 2\n $ type      : chr \"name\"\n $ properties:List of 1\nfeatures : List of 7\n $ :List of 3\n $ :List of 3\n $ :List of 3\n $ :List of 3\n $ :List of 3\n $ :List of 3\n $ :List of 3\ntype :  chr \"FeatureCollection\"\nAnother example of nested but structured data is HTML or XML trees obtained from scraping websites. Typically, web scraping approaches like rvest (Wickham 2022) or BeautifulSoup (Zheng, He, and Peng 2015) parse the hierarchical Document Object Model (DOM) and turn it into an in-memory representation of a website’s DOM. For a DOM parsing example, see case study Section 11.4.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Management</span>"
    ]
  },
  {
    "objectID": "data-management.html#forms-of-data",
    "href": "data-management.html#forms-of-data",
    "title": "6  Data Management",
    "section": "",
    "text": "A Note on Long Format vs. Wide Format The above multivariable time series is shown in what the data science community calls wide format. In this most intuitive format, every column represents one variable, time is on the Y-axis. The counterpart is the so-called long format shown below. The long format is a machine-friendly, flexible way to represent multi-variable data without altering the number of columns with more variables.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Management</span>"
    ]
  },
  {
    "objectID": "data-management.html#representing-data-in-files",
    "href": "data-management.html#representing-data-in-files",
    "title": "6  Data Management",
    "section": "6.2 Representing Data in Files",
    "text": "6.2 Representing Data in Files\nTo create the above examples of different forms of data, it was mostly sufficient to represent data in memory, in this case within an R session. As an interpreted language, an R interpreter has to run at all times when using R. The very same is true for Python. Users of these languages can work interactively, very much like with a pocket calculator on heavy steroids. All functions, all data, are in loaded into a machine’s RAM (memory) represented as objects of various classes. This is convenient, but has an obvious limitation: once the sessions ends, the information is gone. Hence, we need to have a way to store at least the results of our computation in persistent fashion.\nJust like in office or image editing software, the intuitive way to store data persistently from a programming language is to store data into files. The choice of the file format is much less straightforward in our case, though. The different forms of data discussed above, potential collaborators and interfaces are factors among others that weigh into our choice of a file format.\n\n6.2.1 Spreadsheets\nBased on our two-dimensional focused intuition and training, spreadsheets are the on-disk analog of data.frames, data.tables and tibbles. Formats like .csv or .xlsx are the most common way to represent two-dimensional data on disk.\nOn the programming side, the ubiquity of spreadsheets leads to a wide variety of libraries to parse and write different spreadsheet formats.\n\n\nimport csv\nimport pandas as pd\n\nd = {'column1': [1,2], 'column2': [3,4]}\ndf = pd.DataFrame(data=d)\ndf.to_csv(\"an_example.csv\", sep=\";\",encoding='utf-8')\n\n\nComma-separated values (.csv)2 are a good and simple option. Their text-based nature makes .csv files language agnostic and human-readable through a text editor.\n;column1;column2\n0;1;3\n1;2;4\nThough Excel spreadsheets are a convenient interface to office environments that offer extras such organization into workbooks, the simpler .csv format has advantages in machine-to-machine communication and as an interface between different programming languages and tools. For example, web visualization libraries such as highcharts or echarts are most commonly written in JavaScript and can conveniently consume data from .csv files. The above example .csv file was written in Python and is now easily read by R.\n\n\nlibrary(readr)\ncsv &lt;- readr::read_csv2(\"an_example.csv\")\ncsv\n\n# A tibble: 2 × 3\n   ...1 column1 column2\n  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1     0       1       3\n2     1       2       4\n\n\n\n\n\n6.2.2 File Formats for Nested Information\nFor many data engineers and developers. JavaScript Object Notation (JSON)3 has become the go-to file format for nested data. Just like with .csv basically every programming language used in data science and analytics has libraries to serialize and deserialize JSON (read and write). Though harder to read for humans than .csv, prettified JSON with a decent highlighting color scheme is easy to read and gives the human reader a good understanding of the hierarchy at hand. The added complexity comes mostly from the nested nature of the data, not so much from the file format.\n\n\nlibrary(jsonlite)\n\nli &lt;- list(\n  element_1 = head(mtcars, 2),\n  element_2 = head(iris, 2)\n)\n\ntoJSON(li, pretty = TRUE)\n\n{\n  \"element_1\": [\n    {\n      \"mpg\": 21,\n      \"cyl\": 6,\n      \"disp\": 160,\n      \"hp\": 110,\n      \"drat\": 3.9,\n      \"wt\": 2.62,\n      \"qsec\": 16.46,\n      \"vs\": 0,\n      \"am\": 1,\n      \"gear\": 4,\n      \"carb\": 4,\n      \"_row\": \"Mazda RX4\"\n    },\n    {\n      \"mpg\": 21,\n      \"cyl\": 6,\n      \"disp\": 160,\n      \"hp\": 110,\n      \"drat\": 3.9,\n      \"wt\": 2.875,\n      \"qsec\": 17.02,\n      \"vs\": 0,\n      \"am\": 1,\n      \"gear\": 4,\n      \"carb\": 4,\n      \"_row\": \"Mazda RX4 Wag\"\n    }\n  ],\n  \"element_2\": [\n    {\n      \"Sepal.Length\": 5.1,\n      \"Sepal.Width\": 3.5,\n      \"Petal.Length\": 1.4,\n      \"Petal.Width\": 0.2,\n      \"Species\": \"setosa\"\n    },\n    {\n      \"Sepal.Length\": 4.9,\n      \"Sepal.Width\": 3,\n      \"Petal.Length\": 1.4,\n      \"Petal.Width\": 0.2,\n      \"Species\": \"setosa\"\n    }\n  ]\n} \n\n\n\nThe above example shows the first two lines of two different, unrelated rectangular datasets. Thanks to the hierarchical nature of JSON, both datasets can be stored in the same file albeit totally different columns. Again, just like .csv, JSON works well as an interface, but it is more flexible than the former.\nBesides JSON, XML is the most common format to represent nested data in files. Though there are a lot of overlapping use cases, there is a bit of a different groove around both of these file formats. JSON is perceived as more lightweight and close to “the web” while XML is the traditional, very explicit no-nonsense format. XML has a Document Type Definition (DTD) that defines the structure of the document and which elements and attributes are legal. Higher level formats use this more formal approach as XML-based definition. SDMX4, a world-wide effort to provide a format for exchange statistical data and metadata, is an example of such a higher level format build on XML.\nThe above example shows an excerpt of the main economic forward-looking indicator (FLI) for Switzerland, the KOF Economic Barometer, represented in an SDMX XML file. Besides the value and the date index, several attributes provide the consumer with an elaborate data description. In addition, other nodes and their children provide information like Contact or ID in the very same file. Note that modern browsers often provide code folding for nodes and highlighting to improve readability.\n\n\n6.2.3 A Word on Binaries\nUnlike all file formats discussed above, binaries cannot be read by humans using a simple text editor. In other words, you will need the software that wrote the binary to read it again. If that software was expensive and/or exotic, your work is much less accessible, more difficult to share and harder to reproduce. Though this disadvantage of binaries is mitigated when you use freely available open source software, storing data in binaries can still be a hurdle.\nBut, of course, binaries do have advantages, too: binaries can compress their content and save space. Binaries can take on all sorts of in-memory objects including functions, not just datasets. In other words, binaries can bundle stuff. Consider the following load/save operation in R:\n\n\nbogus &lt;- function(a,b){\n  a + b\n} \n\ndata(Airpassengers)\ndata(mtcars)\n\ns &lt;- summary(mtcars)\n\nsave(\"bogus\", \"Airpassengers\",\"s\",\n     file=\"bundle.RData\")\n\n\nIn memory, bogus is a function, Airpassengers is an R time series object and s is a list based summary object. All of these objects can be stored in a single binary RData file using save(). A fresh R session can now load() everything stored in that file.\n\n\nload(\"bundle.RData\")\n\n\n\n\n\n\n\n\nNote\n\n\n\nNotice that unlike reading a .csv or .json file, the call does not make any assignments into a target object. This is because all objects are loaded into an R environment (.globalEnv by default) with their original names.\n\n\n\n\n6.2.4 Interoperable File Formats\nInteroperable file formats cover some middle ground between the options described above. The cross-language in-memory development platform Apache Arrow5 is a well-established project that also implements file formats that work across many popular (data science) environments. Though the major contribution of the Apache Arrow project is to allow sharing in-memory data store across environments, I will just show it as an example for interoperable file formats here. Nevertheless, if you’re interested in a modern, yet established cross-environment data science project, digging deeper into Apache Arrow is certainly a fun experience.\nFrom the Apache Arrow documentation:\n\n\nlibrary(dplyr)\nlibrary(arrow)\ndata(\"starwars\")\nfile_path_sw &lt;- \"starwars.parquet\"\nwrite_parquet(starwars, file_path_sw)\n\n\nThe above R code writes the starwars demo dataset from the dplyr R package to a temporary .parquet file. The {arrow} R package (Richardson et al. 2022) comes with the necessary toolset to write the open source columnar6 .parquet format. Though they are not text files, .parquet files can be read and written from different environments and consume the file written with R. The below code uses the arrow library for Python to read the file we have just written with R.\n\n\nimport pyarrow.parquet as pa\nsw = pa.read_table(\"starwars.parquet\")\nprint(*sw.column_names, sep=\"\\n\")\n\nname\nheight\nmass\nhair_color\nskin_color\neye_color\nbirth_year\nsex\ngender\nhomeworld\nspecies\nfilms\nvehicles\nstarships\n\n\n\nHere’s Julia reading our Parquet file:\n\n\nusing Parquet\nsw = Parquet.File(\"starwars.parquet\")\n\nParquet file: starwars.parquet\n    version: 2\n    nrows: 87\n    created by: parquet-cpp-arrow version 14.0.0\n    cached: 0 column chunks\n\n\n\nWhen I composed this example, reading and writing Parquet files in different environments, I ran into several compatibility issues. This shows that the level of interoperability is not the same as the interoperability of text files.\n\n6.2.4.1 A Note on Overhead\nThe parquet format is designed to read and write files swiftly and to consume less disk space than text files. Both features can become particularly relevant in the cloud. Note though that Parquet comes with some overhead, which may eat up gains if datasets are small. Consider our starwars dataset. At 87 rows and 14 columns, the dataset is rather small.\n\n\nlibrary(readr)\nwrite_csv(starwars, file = \"starwars.csv\")\ndim(starwars)\n\n[1] 87 14\n\nround(file.size(\"starwars.parquet\") / \n      file.size(\"starwars.csv\"),\n      digits = 2)\n\n[1] 1.47\n\n\n\nHence, the overhead of a schema implementation and other meta information outweighs Parquet’s compression for such a small dataset, leading to a Parquet file that is almost 1.5 times larger than the corresponding csv file. Yet, Parquet already turns the tables for the diamonds demo dataset from the ggplot2 R package, which is by no means a large dataset.\n\n\nlibrary(ggplot2)\ndata(diamonds)\nwrite_csv(diamonds, file = \"diamonds.csv\")\nwrite_parquet(diamonds, \"diamonds.parquet\" )\nround(file.size(\"diamonds.parquet\") /\n      file.size(\"diamonds.csv\"),\n      digits = 2)\n\n[1] 0.21\n\n\n\nThe Parquet file for the diamonds dataset has roughly one fifth of the size of the corresponding text file. This is a great example of why there is not one single, perfect, one-size-fits all form of data that emerged from decades of information engineering. So when you choose how you are going to represent data in our project, think about your goals, your most common use or query and a smooth data transformation strategy for when the use cases or goals change.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Management</span>"
    ]
  },
  {
    "objectID": "data-management.html#databases",
    "href": "data-management.html#databases",
    "title": "6  Data Management",
    "section": "6.3 Databases",
    "text": "6.3 Databases\nGiven the options that file-based approaches provide, what is (a) the difference and (b) the added value of going for a database to manage data? The front-and-center difference is the client interface, but there are many more differences and benefits.\ndatabase users use a client program and a query language to send queries written to a database. The client sends these queries to the database host and either performs an operation on the database quietly or returns a result. The most common example of such a query language is the Structured Query Language (SQL). Using such a query language leads to a standard way of interaction with the data, no matter how the dataset looks like in terms of dimensions, size etc.SQLdatabases have been around much longer than data science itself, and continue to be inevitable as application backends and data archives for many use cases.\n\n\nSELECT * FROM myschema.demotable\n\n\nThe above query would return all rows and all columns from a table called demotable in a schema called myschema. Such a query can easier be sent from a standalone database client, a database specific IDE with a built in client such as DataGrip7 or a programming language. Given the ubiquity of databases most basically any programming language has native interfaces to the most common database. And if that is not the case, there is the database management system agnostic ODBC standard that is supported by all majorSQLdatabases. The below code shows how to connect from R to PostgreSQL, send queries from within R and receive results as R objects.\n\n\nlibrary(RPostgres)\ncon &lt;- dbConnect(\n  host = \"localhost\",\n  user = \"bugsbunny\",\n  # only works within RStudio\n  passwd = .rs.AskForPassword(\"Enter Pw\"), \n  dbname = \"some_db_name\"\n)\n\n# the result is an R data.frame\nres &lt;- dbSendQuery(con,\n \"SELECT * FROM myschema.demotable\")\n\n# and we can do R things with it\n# such as show the first 6 lines.\nhead(res)\ndbDisconnect(con)\n\n\nObviously, the above example barely shows the tip of the iceberg, as it is just meant to illustrate the way we interact with databases as opposed to a file system. To dig a little deeper into databases, I recommend getting a solid understanding of the basic CREATE, SELECT, INSERT, UPDATE, DELETE, TRUNCATE, DROP processes as well as basic JOINs and WHERE clauses. Also, it is helpful to understand the concept of normalization up to the third normal form.\n\n\n\nThe iconic Postgres elephant logo.\n\n\n\n6.3.1 Relational database Management Systems (RDBMS)\nWhen you need to pick a concrete database technology for your project, the first major choice is whether to go for a relational system or not. Unless you have a very compelling reason not to, you are almost always better off with a relational database: Relational databases are well established and accessible from any programming language used in programming with data that one could think of. In addition, modern RDBMS implementations offer many non-relational features such as JSON field types and operations.\nI would classify the most popular relational database implementations as follows. First, there is SQLite. As the name suggestions, SQLite is a light-weight, stripped down, easy-to-use and install implementation.\n\nSQLite is a C-language library that implements a small, fast, self-contained, high-reliability, full-featured,SQLdatabase engine. SQLite is the most used database engine in the world. SQLite is built into all mobile phones and most computers and comes bundled inside countless other applications that people use every day. – SQLite.org\n\nSQLite data lives in a single file that the user queries through the SQLite engine. Here is an example using that engine from R.\n\n\nlibrary(RSQLite)\ndb_path &lt;- \"rse.sqlite3\"\ncon &lt;- dbConnect(RSQLite::SQLite(), db_path)\ndbWriteTable(con, dbQuoteIdentifier(con,\"mtcars\"),\n             mtcars, overwrite = T)\ndbWriteTable(con, dbQuoteIdentifier(con,\"flowers\"),\n             iris, overwrite = T)\n\n\nThe above code initiates a SQLite database and continues to write the built-in R demo datasets into separate tables in that newly created database. Now we can useSQLto query the data. Return the first three rows of flowers:\n\n\ndbGetQuery(con, \"SELECT * FROM flowers LIMIT 3\")\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n\n\n\nReturn cars that are more fuel efficient than 30 miles per gallon:\n\n\ndbGetQuery(con, \"SELECT * FROM mtcars WHERE mpg &gt; 30\")\n\n   mpg cyl disp  hp drat    wt  qsec vs am gear carb\n1 32.4   4 78.7  66 4.08 2.200 19.47  1  1    4    1\n2 30.4   4 75.7  52 4.93 1.615 18.52  1  1    4    2\n3 33.9   4 71.1  65 4.22 1.835 19.90  1  1    4    1\n4 30.4   4 95.1 113 3.77 1.513 16.90  1  1    5    2\n\n\n\nMySQL8 can do a little more and is also immensely popular, particularly as a database backend for web content management systems and other web-based applications. The so-called LAMP stack (Linux, Apache, MySQL and PHP) contributed to its rise decades ago when it fueled many smaller and medium-level web projects around the world. In its early days, MySQL used to be an independent open source project, but it was later on acquired by database Juggernaut Oracle as a light version to go with its flagship product.\nWhile certainly doing its job in millions of installations, MySQL is not in at the same level as [Microsoft SQL Server9 (MSSQL), PostgreSQL10 and Oracle database11, and I suggest one of these three enterprise-level databases as a data store for research projects that go beyond hosting a blog. Especially when it comes to long-term conservation of data and enforcing consistency, MSSQL, PostgreSQL and Oracle are hard to beat. Among the three, personally I would always lean toward the license cost free open source PostgreSQL, but fitting into existing ecosystems is a good reason to go with either MSSQL or Oracle if you can afford the licenses. For many use cases, there is hardly any difference for the analyst or scientific end user. PostgreSQL may have the coolest spatial support, MSSQL T-SQL dialect, may have some extra convenient queries if your developers mastered the dialect and Oracle may have the edge in performance and Java interaction here and there, but none of these systems is a bad choice.\nAnother database management that gets a lot of attention recently (and rightfully so) is DuckDB12. Because it is mentioned so positively and often, it is important to understand what it is and when to use it. DuckDB is not yet another competitor that tries to gain some ground from the big three of MSSQL, PostgreSQL and Oracle. DuckDB does offer an SQL interface, but it is very different in its aims from the traditionalSQLdatabases. DuckDB is serverless and allows accessing Parquet files via a very fast SQL interface. This makes DuckDB a great tool for interactive analysis and transfer of large result sets, but it is not so suitable for enterprise data warehousing.\n\n\n6.3.2 A Word on Non-Relational databases\nAmong other things, relational databases are ACID (Atomicity, Consistency, Isolation, and Durability) compliant and ask for very little in return to provide us with a framework to keep our data quality high for decades. So unless, you have a very specific use case that translates to a compelling reason to use a non-relational database stick to SQL. Document-oriented storage or very unstructured information could be such a reason to use non-relational databases, yet their JSON support allows to also handle JSON in database cells. About a decade ago, mongoDB13 gained traction, partly piggybacking the success of JavaScript and server-side JavaScript in particular. In web development, the MEAN (mongoDB, expressjs, angular and node) stack become popular, and, with the bundle, the idea of non-relational databases as fast track to a backend spread.\nColumnar stores, which are also considered non-relational, are conceptionally similar to relational databases, though denormalized and designed to structure sparse data. database systems like Apache Cassandra14 are designed to scale horizontally and be highly available, managing massive amounts of data. Cloud applications that distribute data across multiple nodes for high availability benefit from such an approach. Other options include Redis15 or Couchbase16. If you are not happy with the “beyond-the-scope-of-this-book” argument, blogging experts like Lukas Eder17 maybe biased but much better educated (and fun) to educate you here.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Management</span>"
    ]
  },
  {
    "objectID": "data-management.html#non-technical-aspects-of-managing-data",
    "href": "data-management.html#non-technical-aspects-of-managing-data",
    "title": "6  Data Management",
    "section": "6.4 Non-Technical Aspects of Managing Data",
    "text": "6.4 Non-Technical Aspects of Managing Data\nThe fact that we can do more data work single-handedly than ever before does not only equate to more options. It also means we need to be aware of new issues and responsibilities. Those responsibilities range from leading by example when it comes to etiquette and ethical aspects, to sticking to privacy rules and complying with security standards. In addition, to normative restrictions that come with handling data, the options and choices of data dissemination are a realm of their own. Just like software publications, you should not just “drop” data without a license and instructions on acceptable use of the data.\n\n6.4.1 Etiquette\nJust because content is publicly available on a website does not automatically mean that bulk downloads, aggregation and republishing are ok. For example, the ability to scrape a website daily and doing so with good intent for science does not mean a website’s Acceptable Use Policy (AUP) allows to systematically archive its content.\n\n\n\n\n\n\nBe responsible when scraping data from websites by following polite principles: introduce yourself, ask for permission, take slowly and never ask twice. – CRAN description of the {polite} R package (Perepolkin 2023).\n\n\n\nIn other words, the new type of researcher discussed in this book needs to be aware of potential legal and social consequences. The {polite} R package quoted above is an example of an alternative approach that favors etiquette over hiding IP addresses to avoid access denial.\n\n\n6.4.2 Security\n\nI ain’t got nothing to hide. I don’t care about people reading my stuff. – an argument I have heard about a zillion times.\n\nPeople who argue like that do not only endanger their environment, they also contribute to a less secure internet at large, as they leave their device open to contribute to malicious activity. It does not have to be you who has been trusted to work with data that qualify as sensitive. Someone on your team or within your network might have been, and that person may trust you more than a stranger and may be less vigilant when harmful actions are executed in your name. This is why you are obliged to care about security. As in, do not store your credentials in your scripts. As in, passwords are not part of your analysis. You may accidentally push your password to GitHub, where it is not only publicly available but also hard to truly delete for beginners. Make sure to choose secure passwords. Use a password manager, so the passwords do not need to be your cat’s name for you to remember. Also, with a password manager you can afford not to have the same passwords for multiple applications. Use a password manager so you can afford to change your passwords. Use key files where you can. The case study chapter gives you a hands-on recipe to use RSA key pairs to connect to remote servers, e.g., your GitHub account instead of a username/password combination. This is also a good way to connect to a remote server via SSH.\nIn addition to the above Brainy Smurf advice, let me mention security as a reason to consider using a database to manage and archive your data for the long haul. Enterprise-level databases allow for granular access management and help to stay ahead of users and their rights regarding your database’s entries.\n\n\n6.4.3 Privacy\nPrivacy in data science is a complex issue and could legitimately fill a book on its own. Though I cannot comprehensively cover privacy in a section, it is important to me to raise awareness and hopefully create an entry point to the matter. When working with data, in its essence, respecting privacy is about avoiding exposure of individual units without their explicit prior consent. It is important to understand that exposure does not stop at names. A single extraordinary feature or an exotic combination of features can identify an individual within a dataset, or at least expose a group. This is why merging multiple datasets may also cause privacy concerns when datasets were not created to be merged in the first place, and/or individuals were not aware that merging was possible. So, what can we as researchers learn from here, except from concerns and further complication of our work? First, licenses and usage policies are a service to users of the data. Second, awareness of what is sensitive data is a valuable skill to have on a team. That being said, management of in-depth knowledge is rather easy to organize in a centralized fashion. Most universities and larger corporations will have an officer to run these things by.\n\n\n6.4.4 Data Publications\nYet, there is more to managing your data’s exposure than just making sure everything is encrypted and locked up. Publication of data makes your results reproducible and improves trust in said results. As a consequence, there is a notable crescendo in the claim for reproducible research. While reproducible research is great, I would like to raise awareness that essentially all solutions created and advertised to improve reproducibility implicitly assume the researcher deals with datasets obtained through a study. In other words, it is implied that your work is not about monitoring an ongoing process.\n\nData Archives\nResearch repositories like Zenodo18 that allow to archive data follow a snapshot thinking: A catalog entry refers to a particular version of an research paper, report, software or dataset. Whenever there is an update, a new version is added to a catalog entry. Adding datasets or software publications to catalogs like Zenodo does not only improve reproducibility, it also helps data workers get credit for their contribution. Fortunately, feeding research repositories is a task that is easy to automate thanks to great integration, APIs and community work.\n\n\nOpen Data\nOpen Data archives are a special form of repositories. The term “open data” refers to publicly available data that are available free of license costs and in machine-readable fashion.\n\nOpen data and content can be freely used, modified, and shared by anyone for any purpose – opendefinition.org\n\nBecause of approaches such as the Swiss government’s “open by default,” open government data (OGD) has become a common form of open data. The basic idea that data generated from publicly funded processes should be publicly available whenever no privacy rights are violated has been the motor for many OGD projects out of public administration. From small local governments to international organizations like the World Bank or OECD, open data have become a valuable and growing source for researchers. Open data initiatives of your country, as well as major international organizations, will help you to create interoperable datasets. In addition, open data organizations provide you with a publication channel for your data or with a data catalog that stores the data description. Initiatives like SDMX (Statistical Data and Meta eXchange)19 aim to improve exchange of data and data descriptions. Their XML-based format has become an international standard which led to the implementation of SDMX read and write routines in statistical software. Whether you think about the conditions of your own data publications or about a source for your own research project, make sure to consider open data.\n\nopenwashdata is an active global community that applies FAIR principles to data generated in the greater water, sanitation, and hygiene (WASH) sector – openwashdata.org\n\nThe openwashdata project I have been contributing to might be an hands-on inspiration to get a more concrete understanding of what open data is actually about. Among other things, the openwashdata project collects datasets from publications and republishes them in machine readable format alongside their meta information. The data manipulation to reach this end is documented in reproducible fashion. The final results, R data packages, are published in freely available git repositories in the project’s GitHub organization.\n\n\n\n\nPerepolkin, Dmytro. 2023. Polite: Be Nice on the Web. https://CRAN.R-project.org/package=polite.\n\n\nRichardson, Neal, Ian Cook, Nic Crane, Dewey Dunnington, Romain François, Jonathan Keane, Dragoș Moldovan-Grünfeld, Jeroen Ooms, and Apache Arrow. 2022. Arrow: Integration to ’Apache’ ’Arrow’. https://CRAN.R-project.org/package=arrow.\n\n\nWickham, Hadley. 2022. Rvest: Easily Harvest (Scrape) Web Pages. https://CRAN.R-project.org/package=rvest.\n\n\nZheng, Chunmei, Guomei He, and Zuojie Peng. 2015. “A Study of Web Information Extraction Technology Based on Beautiful Soup.” J. Comput. 10: 381–87.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Management</span>"
    ]
  },
  {
    "objectID": "data-management.html#footnotes",
    "href": "data-management.html#footnotes",
    "title": "6  Data Management",
    "section": "",
    "text": "The original GeoJSON file from the example can be found at https://raw.githubusercontent.com/mbannert/maps/master/ch_bfs_regions.geojson.↩︎\nNote that commas are not always necessarily the separator in .csv files. Because of the use of commas as decimal delimiters in some regions, columns are also often separated by semicolons to avoid conflicts.↩︎\nhttps://json.org↩︎\nSDMX (https://sdmx.org) stands for Statistical Data and Metadata eXchange is an international initiative that aims at standardizing and modernizing (“industrializing”) the mechanisms and processes for the exchange of statistical data and metadata among international organizations and their member countries. SDMX is sponsored by seven international organizations including the Bank for International Settlements (BIS), the European Central Bank (ECB), Eurostat (Statistical Office of the European Union), the International Monetary Fund (IMF), the Organisation for Economic Co-operation and Development (OECD), the United Nations Statistical Division (UNSD), and the World Bank.↩︎\nhttps://arrow.apache.org/↩︎\nsee also↩︎\nhttps://www.jetbrains.com/datagrip/↩︎\nhttps://www.mysql.com/↩︎\nhttps://www.microsoft.com/en-us/sql-server/sql-server-2019↩︎\nhttps://www.postgresql.org/↩︎\nhttps://www.oracle.com/database/technologies/↩︎\nhttps://duckdb.org/↩︎\nhttps://www.mongodb.com/↩︎\nhttps://cassandra.apache.org/↩︎\nhttps://redis.io/↩︎\nhttps://www.couchbase.com/↩︎\nhttps://blog.jooq.org/tag/nosql/↩︎\nhttps://zenodo.org↩︎\nhttps://sdmx.org/↩︎",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Management</span>"
    ]
  },
  {
    "objectID": "infrastructure.html",
    "href": "infrastructure.html",
    "title": "7  Infrastructure",
    "section": "",
    "text": "7.1 Why Go Beyond a Local Notebook?\nAdmittedly, unless you just always had a knack for Arduinos, Raspberry Pis or the latest beta version of the software you use, infrastructure may be the one area you perceive as distracting, none-of-your-business overhead. So, why leave the peaceful, well-known shire of our local environment for the uncharted, rocky territory of unwelcoming technical documentations and time-consuming rabbit holes?\nPerformance, particularly in terms of throughput, is one good reason to look beyond desktop computers. Data protection regulations that prohibit data downloads may simply force us to not work locally. Or we just do not want a crashing office application to bring down a computation that ran for hours or even days. Or we need a computer that is online 24/7 to publish a report, website or data visualization.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Infrastructure</span>"
    ]
  },
  {
    "objectID": "infrastructure.html#why-go-beyond-a-local-notebook",
    "href": "infrastructure.html#why-go-beyond-a-local-notebook",
    "title": "7  Infrastructure",
    "section": "",
    "text": "Though servers can provide access to massive computing resources, do not think of them as a data vault that looks like a fridge. (Source: own illustration.)\n\n\n\n\n\n\nThink of a server as a “program that listens”. (Source: own illustration.)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Infrastructure</span>"
    ]
  },
  {
    "objectID": "infrastructure.html#hosting-options",
    "href": "infrastructure.html#hosting-options",
    "title": "7  Infrastructure",
    "section": "7.2 Hosting Options",
    "text": "7.2 Hosting Options\nSo, where should we go with our project when it outgrows the local environment of our notebooks? This has actually become a tough question because of all the reasonable options out there. Technical advances, almost unparalleled scalability and large profits for the biggest players made modern infrastructure providers offer an incredible variety of products to choose from. Obviously, a description of product offerings in a vastly evolving field is not well suited for discussions in a book. Hence, Research Software Engineering intends to give an overview to classify the general options and common business models.\n\n7.2.1 Software-as-a-Service\nThe simplest solution and fastest time-to-market is almost always to go for a Software-as-a-Service (SaaS) product – particularly if you are not experienced in hosting and just want to get started without thinking about which Linux to use and how to maintain your server. SaaS products abstract all of that away from the user and focus on doing one single thing well. The shinyapps.io1 platform is a great example of such a service: users can sign up and deploy their web applications within minutes. The shinyapps.io platform is a particularly interesting example of a SaaS product because R developers who come from field-specific backgrounds other than programming are often not familiar with web development and hosting websites. Some of these developers, for whom R might be their first programming language, are suddenly empowered to develop and run online content thanks to the R Shiny web application framework that uses R to generate HTML, CSS and JavaScript based applications. Still, those applications need to be hosted somewhere. This is precisely what shinyapps.io does. The service solely hosts web applications that were created with the Shiny web application framework. This ease of use is also the biggest limitation of SaaS products. A website generated with another tool cannot be deployed easily. In addition, the mere bang-for-buck price is rather high compared to self-hosting as users pay for a highly, standardized, managed hosting product. Nevertheless, because of the low volume of most projects, SaaS is feasible for many projects, especially at a proof of concept stage.\n\n\n\n\n\n\nIn case you are interested in getting started with Shiny, take a look at the Shiny case study in this book. The study explains basic concepts of Shiny to the user and walks readers through the creation and deployment of a simple app.\n\n\n\nSaaS, of course, is neither an R nor a data science idea. Modern providers offer databases, storage calendars, face recognition and location services, among other things.\n\n\n7.2.2 Self-Hosted\nThe alternative approach to buying multiple managed services, is to host your applications by yourself. Since – this applies to most users at least – you do not take your own hardware, connect to your home Wi-Fi and aim to start your own hosting provider, we need to look at different degrees of self-hosting. Larger organizations, e.g., universities, often like to host applications on their own hardware, within their own network to have full control of their data. Yet, self-hosting exposes you to issues, such as attacks, that you would not need to worry about as much in a software as a service setting (as long as you trust the service).\nSelf-hosting allows you to host all the applications you want on a dedicated machine. Self-hosters can configure their server depending on their access rights. Offerings range from root access that allows users to do anything to different shades of managed hosting with more moderated access. Backed by virtual infrastructure, modern cloud providers offer a very dynamic form of self-hosting: their clients can use a web GUIs and/or APIs to add, remove, reboot and shut down nodes. Users can spin up anything from pre-configured nodes optimized for different use cases to containerized environments and entire Kubernetes (K8s) clusters in the cloud. Flexible pricing models allow paying based on usage in a very dynamic fashion.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Infrastructure</span>"
    ]
  },
  {
    "objectID": "infrastructure.html#building-blocks",
    "href": "infrastructure.html#building-blocks",
    "title": "7  Infrastructure",
    "section": "7.3 Building Blocks",
    "text": "7.3 Building Blocks\nExactly because of this dynamic described above and the ubiquity of the cloud, it is good to know about the building blocks of modern IT infrastructure.\n\n7.3.1 Virtual Machines\nVirtual machines (VMs) remain the go-to building blocks for many set-ups. Hence, university IT, private sector IT, independent hosting providers and online giants all offer VMs. Virtual machines allow running a virtual computer that has its own operating system on some host machine. Running applications on such a virtual computer feels like running an application on a standalone computer dedicated to run this application.\n\n\n\n\n\n\nOracle’s Virtual Box is a great tool to use and try virtual machines locally. Virtual Box allows to run a Virtual Windows or Linux inside macOS and vice versa. Running a virtual box locally may not be the most performant solution, but it allows to have several test environments without altering one’s main environment.\n\n\n\n\n\n7.3.2 Containers and Images\nAt the first glance, containers look very much like Virtual Machines to the practitioner. The difference is that every Virtual Machine has its own operating system, while containers use the host OS to run a container engine on top of the OS. By doing so, containers can be very lightweight and may take only a few seconds to spin up, while spinning up Virtual Machines can take up to a few minutes – just like booting physical computers. Hence, Docker containers are often used as single-purpose environments: Fire up a container, run a task in that environment, store the results outside the container and shut down the container again.\nDocker (Why Docker?2) is the most popular containerized solution and quickly became synonymous to container environments configured in a file. So-called Docker images are built layer-by-layer based on other less specific Docker images. A DOCKERFILE is the recipe for a new image. Images are blueprints for containers, an image’s running instance. A Docker runtime environment can build images from DOCKERFILEs and distribute these images to an image registry. The platform Docker Hub3 hosts a plethora of pre-built Docker images from ready-to-go databases to Python ML environments or minimal Linux containers to run a simple shell script in a lab-type environment.\nContainers run in a Docker runtime environment and can either be used interactively or in batches which execute a single task in an environment specifically built for this task. One of the reasons why Docker is attractive to researchers is its open character: DOCKERFILEs are a good way to share a configuration in a simple, reproducible file, making it easy to reproduce setups. Less experienced researchers can benefit from Docker Hub which shares images for a plethora of purposes, from mixed data science setups to database configuration. Side effect free working environments for all sorts of tasks can especially be appealing in exotic and/or dependency heavy cases.\nBesides simplification of system administration, Docker is known for its ability to work in the cloud. All major cloud hosters offer Docker environments and the ability to deploy Docker containers that were previously developed and tested locally. You can also use Docker to tackle throughput problems using container orchestration tools like Docker Swarm4 or K8s (say: Kubernetes)5 to run hundreds of containers (depending on your virtual resources).\n\n\n7.3.3 Kubernetes\nThough hosting Kubernetes (K8s) is clearly beyond the scope of basic level DevOps, the ubiquity of the term and technology as well as the touching points and similarities with the previously introduced concept of containers justify a brief positioning of Kubernetes. We cannot really see Kubernetes as a building block like the technologies introduced above. K8s is a complete cluster with plenty of features to manage the system and its applications. Kubernetes is designed to run on multiple virtual nodes and distribute processes running in so-called pods across its nodes.\nBecause a plain vanilla Kubernetes cluster is not easy to set up and manage, the tech sector’s big three and some of their smaller alternatives offer their own flavors of cloud-based Kubernetes. The basic idea of these offerings is to standardize and to take some of the administrative burden from their clients through pre-configuration and automation support. Red Hat’s Openshift is a different approach that targets enterprises who want to set up a cluster on their own infrastructure (on premise).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Infrastructure</span>"
    ]
  },
  {
    "objectID": "infrastructure.html#applied-containerization-basics",
    "href": "infrastructure.html#applied-containerization-basics",
    "title": "7  Infrastructure",
    "section": "7.4 Applied Containerization Basics",
    "text": "7.4 Applied Containerization Basics\nWhile the above Building Blocks section contextualizes the container approaches, this section gives a simple 101 into the basics of containerization, enabling the readers to take their first steps in the container world. One of the beautiful things about containers is that, due to their isolated nature, one can go a long way trying out things as containers get destroyed and recreated all the time. Also, because containers run in a standardized runtime environment, locally developed images easily transfer to large remote machines and clusters.\n\n7.4.1 DOCKERFILEs\nDOCKERFILEs are text file recipes to create images, i.e., blueprints for containers. One great thing about container images is that they are layered. That is, one can stack images and benefit from previously created images. The below example DOCKERFILE uses a standard, publicly available image from dockerhub.com and adds some custom packages.\nFROM rocker/shiny:latest\nRUN apt-get update\nRUN apt-get install -qq -y libpq-dev\nRUN install2.r ggplot2 shiny shinydashboard  \\\n               shinydashboardPlus  \\\n               dplyr RPostgres  \nIn this case, we make use of a pre-built image from the rocker project. The rocker project designs useful images around the R language ecosystem, builds them on a regular basis and makes them available via Docker Hub. Here, our image allows running the open source version of shiny server in a Docker container. We add a Postgres driver at the operating system level before we install several R packages from CRAN.\n\n\n7.4.2 Building and Running Containers\nThere are plenty of ways to run and build containers. Online tools either offered as a service or self-hosted can build images server-side. Yet, the easiest way to get started with containers is to run and build them locally with Docker Desktop.\nEven though Docker may not even be the best way to build containers to some, Docker is by far the most known way and therefore comes with the largest ecosystem and most community material. Docker Desktop is an easy-to-use application available on Windows and OSX. With Docker Desktop, one can execute Docker commands and build images, either in a GUI or using its CLI. Table 7.1 shows a few of the most basic Docker commands.\n\n\n\n\nTable 7.1: Basic Docker Commands\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\ndocker run\nstarts application\n\n\ndocker ps\nlists containers\n\n\ndocker images\nlists docker images\n\n\ndocker pull\npulls images from registry, e.g., dockerhub.com\n\n\ndocker run\nruns container based on image\n\n\ndocker kill\nkills container\n\n\ndocker build &lt;dir-with-docker-file&gt;\nbuilds image based on DOCKERFILE\n\n\ndocker exec &lt;command&gt; &lt;container&gt;\nexecutes command inside container\n\n\n\n\n\n\n\n\n\n\n\nTo learn what a container looks like, e.g., to find out how the container was affected by changes to the DOCKERFILE, it can be very illustrative to walk around inside. To do so with a container created from the above DOCKERFILE, start the container and execute a bash with the interactive flag -it.\ndocker run -d rocker/shiny\ndocker exec -it rocker/shiny /bin/bash\n\n# alternative you could start R right away\n# note that you need to know the location \n# of the executable\ndocker exec -it rocker/shiny /usr/local/bin/R\n\n\n7.4.3 Docker Compose – Manage Multiple Containers\nDocker is a great way to give a new tool a spin without affecting one’s proven environment. So, even if you are a container beginner, the time when you would like to spin up multiple containers at once will come quickly. While you can start as many containers as your local resources allow for, running containers at once does not necessarily mean those containers are aware of each other, let alone they could talk to each other.\nModern applications often follow modular architecture patterns, i.e., they have a front end, some middle layer such as a REST API and a database backend. A web application may have a statically generated HTML front and simply expose HTML/CSS/JavaScript files and query a REST API. The REST API may use the express.io framework and is served using a node server which talks to a Postgres database backend. Each of these three parts could live in its own container. This is where docker could help to create a development environment locally that essentially mimics the production setup and therefore facilitates deployment to production.\nDocker Compose allows defining how multiple containers play together. Consider the following example file that creates two containers: a Shiny web server and a database which can be queried by the Shiny server.\nservices:\n   postgres:\n      # a name, e.g.,  db_container is \n      # instrumental to be\n      # called as host from the shiny app\n      container_name: db_container\n      build: ./postgres\n      restart: always\n      environment:\n         - POSTGRES_USER=postgres\n         - POSTGRES_PASSWORD=postgres\n      # This port mapping is only necessary \n      # to connect from the host, \n      # not to let containers talk to each other. \n      ports:\n         - \"5432:5432\"\n      volumes:\n         - \"./pgdata:/var/lib/postgresql/data\"\n   shiny: \n      container_name: shiny\n      depends_on: \n         - postgres\n      build: ./shiny\n      volumes:\n         - \"./shiny-logs:/var/log/shiny-server\"\n         - \"./shiny-home:/srv/shiny-server\"\n      ports:\n         - \"3838:3838\"\nNote how images are built from local directories postgres and shiny that contain DOCKERFILEs. It is also possible to pull images directly from a registry. To run such a system of multiple containers, simply use\ndocker compose up --force-recreate\n\n\n\n\n\n\nNote that docker-compose does not replace an orchestrator and cannot provide cluster functionality like Docker Swarm or even Kubernetes.\n\n\n\n\n\n7.4.4 A Little Docker Debugging Tip\nSometimes containers keep crashing right after they start. This makes debugging a bit harder because we cannot simply use the -it flag to get inside and stroll around to find the issue. In such a case, even if you briefly log in, your container will shut down before you can even reach the location in question inside your container. Of course, there are log files\ndocker logs &lt;container-name&gt;\nMaybe though these logs are not verbose enough, or some permission issue may not be fully covered. Hence, adding `command: “sleep infinity” to your compose file prevents the service/container in question from running into the problem and crashing immediately.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Infrastructure</span>"
    ]
  },
  {
    "objectID": "infrastructure.html#footnotes",
    "href": "infrastructure.html#footnotes",
    "title": "7  Infrastructure",
    "section": "",
    "text": "https://shinyapps.io↩︎\nhttps://www.docker.com/why-docker↩︎\nhttps://dockerhub.com↩︎\nhttps://docs.docker.com/engine/swarm/swarm-tutorial/↩︎\nhttps://kubernetes.io/↩︎",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Infrastructure</span>"
    ]
  },
  {
    "objectID": "automation.html",
    "href": "automation.html",
    "title": "8  Automation",
    "section": "",
    "text": "8.1 Continuous Integration/Continuous Deployment\nBecause of its origin in build, test and check automation, Continuous Integration/Continuous Deployment (CI/CD)1 may not be the first thing that comes into mind when one approaches programming through the analytics route. Yet, thorough testing and automated builds have not only become well-established parts of the data science workflow, CI/CD tools can also help to automate tasks beyond testing and packaging your next release.\nModern software providers are an easy way to add the tool chain that is often fuzzily called CI/CD. While CI stands for continuous integration and simply refers to a workflow in which the team tries to release new features to production as continuously as possible, CD stands for either continuous delivery or continuous deployment.\nThanks to infrastructure as code and containerization, automation of development and deployment workflows become much easier also because local development can run in an environment very close to the production setup. Git hosting powerhouses GitHub and GitLab run their flavors of CI/CD tools, making the approach well documented and readily available by default to a wide array of users: GitHub Actions2 and GitLab CI3. In addition, services like CircleCI4 offer this toolchain independently of hosting git repositories.\nUsers of the above platforms can upload a simple text file that follows a name convention and structure to trigger a step-based tool-chain based on an event. An example of an event may be a push to a repository’s main branch. A common example would be to run tests and/or build a package and upon success deploy the newly created package to some server – all triggered by a simple push to master. One particularly cool thing is, that there are multiple services which allow running the testing on their servers using container technologies. This leads to a great variety of setups for testing. That way, software can easily be tested on different operating systems/environments.\nHere is a simple sketch of a .gitlab-ci.yml configuration that builds and tests on pushes to all branches and deploys a package after a push to the main branch and successful build and test steps:\nFor more in depth examples of the above, Jim Hester’s talk on GitHub Actions for R5 is an excellent starting point. CI/CD tool chains are useful for a plethora of actions beyond mere building, testing and deployment of software. The publication chapter covers another common use case: rendering and deployment of static websites. That is, websites that are updated by re-rendering their content at build time, creating static files (artifacts) that are uploaded to web host. Outlets like the GitHub Actions Marketplace, the r-lib collections for R specific actions and the plethora of readily available actions are a great showcase of the broad use of CI/CD applications.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Automation</span>"
    ]
  },
  {
    "objectID": "automation.html#continuous-integrationcontinuous-deployment",
    "href": "automation.html#continuous-integrationcontinuous-deployment",
    "title": "8  Automation",
    "section": "",
    "text": "stages:\n- buildncheck\n- deploy_pack\n\ntest:\nimage:\nname: some.docker.registry.com/some-image:0.2.0\nentrypoint:\n- \"\"\nstage: buildncheck\nartifacts:\nuntracked: true\nscript:\n# we don't need it and it causes a hidden file NOTE\n- rm .gitlab-ci.yml \n- install2.r --repos custom.mini.cran.ch .\n- R CMD build . --no-build-vignettes --no-manual\n- R CMD check --no-manual *.tar.gz\n\ndeploy_pack:\nonly: \n- main\nstage: deploy_pack\nimage:\nname: byrnedo/alpine-curl\nentrypoint: [\"\"]\ndependencies:\n- 'test'\nscript:\n- do some more steps to login and deploy to server ...",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Automation</span>"
    ]
  },
  {
    "objectID": "automation.html#cron-jobs",
    "href": "automation.html#cron-jobs",
    "title": "8  Automation",
    "section": "8.2 Cron Jobs",
    "text": "8.2 Cron Jobs\nCron syntax is very common in the Unix world and is also useful in the context of the CI/CD tools explained above. Instead of on dispatch or event based triggers, CI/CD processes can also be triggered based on time.\nNamed after the Unix job scheduler cron, a cron job is a task that runs periodically at fixed times. Pre-installed in most Linux server setups, data analysts often use cronjobs to regularly run batch jobs on remote servers. Cron jobs use a funny syntax to determine when jobs run.\n#min hour day month weekday\n15 5,11 * * 1 Rscript run_me.R\nThe first position denotes the minute mark at which the job runs – in our case 15 minutes after the new hour started. The second mark denotes hours during the day – in our case the 5th and 11th hour. The asterisk * is a wildcard expression for running the job on every day of the month and in every month throughout the year. The last position denotes the weekday, in this example we run our job solely on Mondays.\nMore advanced expressions can also handle running a job at much shorter intervals, e.g., every 5 minutes.\n*/5 * * * * Rscript check_status.R\nTo learn and play with more expressions, check crontab guru6. If you have more sophisticated use cases, like overseeing a larger number of jobs or execution on different nodes, consider using Apache Airflow7 as a workflow scheduler.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Automation</span>"
    ]
  },
  {
    "objectID": "automation.html#workflow-scheduling-apache-airflow-dags",
    "href": "automation.html#workflow-scheduling-apache-airflow-dags",
    "title": "8  Automation",
    "section": "8.3 Workflow Scheduling: Apache Airflow DAGs",
    "text": "8.3 Workflow Scheduling: Apache Airflow DAGs\nThough much less common than the above tools, Apache Airflow has earned a mention because of its ability to help researchers keep an overview of regularly running processes. Examples of such processes could be daily or monthly data sourcing or timely publication of a regularly published indicator. I often referred to Airflow as “cronjobs8 on steroids.” Airflow ships with a dashboard to keep track of many timed processes, plus a ton of other logs and reporting features, which are worth a lot when maintaining reoccurring processes. Airflow has its own Airflow Summit conference, a solid community and Docker compose setup to get you started quickly. The setup consists of a container image for the web frontend and another image for the PostgreSQL backend. The fact that there is also a Managed Workflow for Apache Airflow offering in the Amazon cloud at the time of writing shows the broad acceptance of the tool. Airflow also runs on Kubernetes in case you are interested in hosting Airflow in a more robust production setup.\nSo, what does Airflow look like in practice? Airflow uses the Python language to define directed acyclic graphs (DAGs). Essentially, a DAG pictures a process that has – unlike a cycle – a clear start and ending. DAGs can be one-dimensional and fully sequential, but they can also run tasks in parallel.\n\nfrom datetime import datetime, timedelta\nfrom airflow import DAG\n# Operators; we need this to operate!\nimport BashOperator\nwith DAG(\n    default_args={\n        \"depends_on_past\": False,\n        \"email\": [\"airflow-admin@your-site.com\"],\n        \"email_on_failure\": False,\n        \"email_on_retry\": False,\n        \"retries\": 1,\n        \"retry_delay\": timedelta(minutes=5)\n    },\n    description=\"Bash Operator\"\n    schedule='5 11 * * *',\n    start_date=datetime(2023, 1, 1),\n    catchup=False,\n    tags=[\"example\"],\n) as dag:\n    # t1, t2 and t3 are examples of tasks\n    # created by instantiating operators\n    t1 = BashOperator(\n        task_id=\"print_date\",\n        bash_command=\"date &gt; date.txt\",\n    )\n\n    t2 = BashOperator(\n        task_id=\"sleep\",\n        depends_on_past=False,\n        bash_command=\"sleep 5\",\n        retries=3,\n    )\n    \n    t3 = BashOperator(\n        task_id=\"2 dates\",\n        depends_on_past=False,\n        bash_command=\"cat date.txt & date\",\n    )\n\n    t1 &gt;&gt; t2 &gt;&gt; t3\nThe above code shows an example of a simple DAG that combines three strictly sequential tasks. Task t1 depends on tasks t2 and t3. In this simple example, all tasks use the Airflow BashOperator to execute Bash commands, but Airflow provides plenty of other operators from its PythonOperator, to Docker or Kubernetes operators.\nSuch operators also allow executing tasks on remote machine or clusters, not solely the machine or container that serves Airflow itself.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Automation</span>"
    ]
  },
  {
    "objectID": "automation.html#make-like-workflows",
    "href": "automation.html#make-like-workflows",
    "title": "8  Automation",
    "section": "8.4 Make-Like Workflows",
    "text": "8.4 Make-Like Workflows\nMakefiles and the make software that made them popular are the classic of build automation. Best known for its support in the compilation of C programs, Makefiles became popular across programming languages thanks to their straightforward concept and readable approach. A Makefile contains a target, an optional prerequisite, and a recipe to get there.\ntarget: prerequisite\n    recipe\nLike this:\nsay_yo: store_yo\n    cat echo.txt\n\nstore_yo: \n    echo \"yo!\" &gt; echo.txt\nThis will simply print\necho \"yo!\" &gt; echo.txt\ncat echo.txt\nyo!\nin your terminal. Very much like Apache Airflow and its DAGs which were introduced above, Makefiles allow declaring and managing dependencies. The ability of make and make-like approaches such as the {targets} R package (Landau 2021) goes well beyond the simple sequential definitions like the basic example above. Parallel execution of independent tasks, cherry-picking execution of single tasks from a larger workflow, variables and caching are highly useful when runtime increases. Consider, a multistep workflow, parts of which are running for quite a while but hardly change while other swift parts change all the time, and it becomes obvious how a simple CLI-based workflow helps.\nModern implementation like the target R package ship with network visualizations of the execution path and its dependencies. The walkthrough provided in R OpenSci’s book9 on target is a great summary available as text, code example and video.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Automation</span>"
    ]
  },
  {
    "objectID": "automation.html#infrastructure-as-code",
    "href": "automation.html#infrastructure-as-code",
    "title": "8  Automation",
    "section": "8.5 Infrastructure as Code",
    "text": "8.5 Infrastructure as Code\nIn recent years, declarative approaches helped to make task automation more inclusive and appealing to a wider audience. Not only a workflow itself, but also the environment a workflow should live and run in is often defined in declarative fashion. This development does not only make maintenance easier, it also helps to make a setting reproducible and shareable.\nDo not think infrastructure as code is only relevant for system admins or other infrastructure professionals who make use of it every day. The ability to reproduce and tweak infrastructure at essentially no costs enables other forms of automation such as CI/CD. Just like a flat rate with your mobile carrier will lead to more calls.\ninfrastructure as code approaches do not only describe infrastructure in declarative and reproducible fashion as stated above; infrastructure-as-code can also be seen as a way to automate setting up the environment you work with. The infrastructure section explains in greater detail how such definitions look like and containers are building blocks of modern infrastructure defined in code.\nautomation is more complex for cluster setups because among other things, applications need to be robust against pods getting shut down on one node and spawned on another node allowing to host applications in high availability mode. On Kubernetes clusters, Helm10, Kubernetes’ package manager, is part of the solution to tackle this added complexity. Helm is, as the Helm website defines “the best way to find share and use software built for Kubernetes”. Terraform11 can manage Kubernetes clusters on clouds like Amazon’s AWS, Google’s GPC, Microsoft’s Azure and many other cloud systems using declarative configuration files. Again, a console-based client interface is used to apply a declarative configuration plan to a particular cloud.\n\n\n\n\nLandau, William Michael. 2021. “The Targets r Package: A Dynamic Make-Like Function-Oriented Pipeline Toolkit for Reproducibility and High-Performance Computing.” Journal of Open Source Software 6 (57): 2959. https://doi.org/10.21105/joss.02959.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Automation</span>"
    ]
  },
  {
    "objectID": "automation.html#footnotes",
    "href": "automation.html#footnotes",
    "title": "8  Automation",
    "section": "",
    "text": "See also: https://www.atlassian.com/continuous-delivery/principles/continuous-integration-vs-delivery-vs-deployment↩︎\nhttps://docs.github.com/en/actions↩︎\nhttps://docs.gitlab.com/ee/ci/↩︎\nhttps://circleci.com/↩︎\nhttps://www.jimhester.com/talk/2020-rsc-github-actions/↩︎\nhttps://crontab.guru/↩︎\nhttps://airflow.apache.org/↩︎\nhttps://en.wikipedia.org/wiki/Cron↩︎\nhttps://books.ropensci.org/targets/walkthrough.html↩︎\nhttps://helm.sh/↩︎\nhttps://www.terraform.io/↩︎",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Automation</span>"
    ]
  },
  {
    "objectID": "community.html",
    "href": "community.html",
    "title": "9  Community",
    "section": "",
    "text": "9.1 Stay Up-to-Date in a Vastly Evolving Field – Social Media\nI hate to admit it because I am not a big fan of social media, but it has definitely become an important channel for professional use also to me. Social media is a great way to get my regular dose of what’s new in tech. The popularity of platforms varies locally but also evolves pretty dynamically over time. X, back when it was called Twitter, had been the platform of choice for many engineers around data science for almost a decade, before large chunks turned their back on the chirping platform. I am still there4, but I may have to find a new tool to get feedback and bookmark good reads. No matter the platform, make sure not to get caught in politics, memes or cat pictures if you use it professionally. Because once you strictly limit your time and avoid going down too many rabbit holes, you will get a programming tip here, a nice shortcut there and, more importantly, get a sense of trends and dynamics.\nStart with a few accounts to follow, adapt them regularly and use lists in case you need to organize your input a bit more. For X, Tweet Deck is a good standard option for a more advanced use of the platform. If you plan to build some following of your own, learn how to schedule posts and put some thought into the timing of your outlet, regarding time zone and audience.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Community</span>"
    ]
  },
  {
    "objectID": "community.html#knowledge-sharing-platforms",
    "href": "community.html#knowledge-sharing-platforms",
    "title": "9  Community",
    "section": "9.2 Knowledge-Sharing Platforms",
    "text": "9.2 Knowledge-Sharing Platforms\nThe most popular knowledge-sharing platform for programmers, stackoverflow.com, has made itself a name for being the first hit in your favorite search engine to any questions or error messages remotely related to source code. The platform has accumulated millions of questions and answers to programming questions across languages. Through a mix of quality content, a community rating system and a healthy mix of gamification, the platform managed to become a mainstay of the programming community. The platform’s crowd-based rating system does not only assess the quality of questions and answers, a solid stack Overflow score and reach have also become a notch on programmers’ resumés.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Community</span>"
    ]
  },
  {
    "objectID": "community.html#look-out-for-local-community-group",
    "href": "community.html#look-out-for-local-community-group",
    "title": "9  Community",
    "section": "9.3 Look Out for Local Community Group",
    "text": "9.3 Look Out for Local Community Group\nSpeaking of job market opportunities, look out for a local R, Python} or data science user group. Local user groups around programming languages do not only host interesting talks or other formats, they often provide an opportunity to network or even have dedicated time to advertise open positions. Besides the community explorer mentioned in the introduction of this chapter, the meetup5 platform that has become synonymous with special interest groups coming together locally, is a good place to start your research.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Community</span>"
    ]
  },
  {
    "objectID": "community.html#attend-conferences---online-can-be-a-viable-option",
    "href": "community.html#attend-conferences---online-can-be-a-viable-option",
    "title": "9  Community",
    "section": "9.4 Attend Conferences - Online Can Be a Viable Option!",
    "text": "9.4 Attend Conferences - Online Can Be a Viable Option!\nIn similar fashion, conferences are a good channel to stay connected and up-to-date. A conference like PiConf6, Posit Conf or useR!7 is basically a bulk meetup: potentially hundreds of talks, thousands of potential contacts, and quite a bit of time to digest a large conference. In recent years, conferences often provide an option to attend online. While, to some, most of the fun and networking will be missing online, the ability to attend online can also be a great chance for a conference and its attendance. Online conferences have the chance to be a lot more inclusive than their in-person counterparts. Even costs aside, i.e., covered by scholarships, many cannot afford the time to leave their jobs and families to travel internationally for an entire week. Online conferences also give people a chance to attend who may not be involved enough to attend a software development conference for an entire week, but who want to catch a glimpse and evaluate. In any case, the entry hurdle is lower than ever. These paragraphs are meant to convince you to take your time, to prepare, attend and debrief a major open source software conference – online or in-person.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Community</span>"
    ]
  },
  {
    "objectID": "community.html#join-a-chat-space",
    "href": "community.html#join-a-chat-space",
    "title": "9  Community",
    "section": "9.5 Join a Chat Space",
    "text": "9.5 Join a Chat Space\nCommunity chat spaces run by conference organizers, local user groups or societies are another way to stay connected to the community. More dynamic than message boards and forums, chats allow for both asynchronous and live communication. Usually chat spaces are more domain- or even topic-specific than local meetups or social media. Popular software to participate or even run one’s own community is Slack8, Discord9, Mattermost10 or Matrix11. Slack is available as a service run by a company of the same name. Slack users can start new workspaces for free for hundreds of members, but will have to pay when they want to keep their communities messages beyond the last 10,000 messages. Just like Slack, Discord works in our web browsers or as a standalone Desktop or mobile client. Popular among gamers, Discord does not quite have the business attire of Slack, but it certainly has its fans. Mattermost is an open source alternative to the former two. It is also available as a service, but is rather known as a package to self-host one’s independent chat space. Matrix has a different approach, labelling itself an open network for secure, decentralized communication. Reminiscent of the Internet Relay Chat (IRC)12 of the old days, Matrix is server software and network of the same name. Just like IRC, Matrix is not bound to a single client and therefore appears modular to the end users allowing to choose between different clients such as Element13 to connect to established and self-hosted networks alike.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Community</span>"
    ]
  },
  {
    "objectID": "community.html#footnotes",
    "href": "community.html#footnotes",
    "title": "9  Community",
    "section": "",
    "text": "https://society-rse.org/↩︎\nhttps://us-rse.org/↩︎\nhttps://de-rse.org/de/index.html↩︎\nhttps://x.com/whatsgoodio↩︎\nhttps://www.meetup.com/↩︎\nhttps://piconf.net/↩︎\nhttps://x.com/_useRconf↩︎\nhttps://slack.com↩︎\nhttps://discord.com/↩︎\nhttps://mattermost.com/↩︎\nhttps://matrix.org/↩︎\nhttps://en.wikipedia.org/wiki/Internet_Relay_Chat↩︎\nhttps://element.io/↩︎",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Community</span>"
    ]
  },
  {
    "objectID": "publishing.html",
    "href": "publishing.html",
    "title": "10  Publishing and Reporting",
    "section": "",
    "text": "10.1 Getting Started with a Simple Report\nThe simplest form of such a Markdown-based output, is a simple HTML report.\nTo those of us without a web development past or present, HTML may rather sound daunting than simple. But hear me out: HTML is (a) a lot more useful and flexible than newcomers might think and (b) simple indeed: HTML is text, does not need LaTeX to be rendered from Markdown like PDF, and can be displayed by web browsers on a plethora of devices. Plus, based on HTML, you can create (fancy) presentations, websites or simple reports. With the self-contained option, you can even embed included files such as images into one single HTML document using byte encoding. That way, you can easily share a report that can be viewed on any device with a web browser across different operating systems and setups.\nTo see a Markdown process in action, consider the following basic example of Markdown syntax:\nWhen rendered, the above Markdown turns into the below output, stored in HTML and CSS files.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Publishing and Reporting</span>"
    ]
  },
  {
    "objectID": "publishing.html#getting-started-with-a-simple-report",
    "href": "publishing.html#getting-started-with-a-simple-report",
    "title": "10  Publishing and Reporting",
    "section": "",
    "text": "#| output: asis\n\n\n### This is a level three header \n\nThis is plain text *this is italic*\nand **this is bold**\n\n### This is another level three header \n\nmore text\n\n\n10.1.1 This is a level three header\nThis is plain text this is italic and this is bold\n\n\n10.1.2 This is another level three header\nmore text",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Publishing and Reporting</span>"
    ]
  },
  {
    "objectID": "publishing.html#static-website-generators",
    "href": "publishing.html#static-website-generators",
    "title": "10  Publishing and Reporting",
    "section": "10.2 Static Website Generators",
    "text": "10.2 Static Website Generators\nOnce we understood how the output files are created, let’s take a more in-depth look at how a report goes online in fully automated fashion from computation to a website. In a nutshell, static website generators turn Markdown code into a combination of HTML, CSS and JavaScript, so a web browser can display the result. The Go based Hugo2 or the classic Jekyll3 are popular static website generator approaches to run a website or blog.\nApproaches such as the {distill} R package (Dervieux et al. 2022) or Quarto, the new kid on the block, add a data science and analytics flavor to the static website generator approach: Those implementations allow running analytics’ code at render time. Quarto documents, for example, are text files that contain Markdown text and possibly code chunks in the same documents. When rendered, the analytics code runs first, potentially creating output such as tables or figures. If necessary, this output is stored in separate files and smoothly integrated into the main document that is about to be rendered afterward.\nRendered output often consists of multiple files such as HTML documents, CSS style information, JavaScript code or images. Because this may be fine for websites but inconvenient for presentations or the exchange of reports, analytics-minded renderers such as Quarto offer a self-contained option. When enabled, Quarto renders a single self-contained, encoded HTML file that contains everything from JavaScript to images.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Publishing and Reporting</span>"
    ]
  },
  {
    "objectID": "publishing.html#hosting-static-websites",
    "href": "publishing.html#hosting-static-websites",
    "title": "10  Publishing and Reporting",
    "section": "10.3 Hosting Static Websites",
    "text": "10.3 Hosting Static Websites\nBecause the requirements of a static website are minimal, we can essentially use the simplest website host possible to host our static site. Unlike with content management system approaches such as WordPress, static websites do not need server side scripting languages or databases. This simplicity allows hosting static websites on a plethora of cheap hosters, including many free plans.\n\n10.3.1 GitHub Pages\nOne excellent and simple solution to host blogs, personal websites, online documentation or presentations is to use the offerings of major git providers such as GitHub’s GitHub Pages4. Though originally meant to be used with themes provided by GitHub and Markdown rendered by Jekyll on GitHub’s servers, GitHub Pages became the home of an abundance of static sites rendered by a wide array of static website generators.\nAll you need is a repository on GitHub to host the static files. You can activate GitHub Pages for your repository and choose whether you rather want the rendered files to be on a separate branch named gh-pages or in a subfolder of your root directory (mostly docs). Personally, I favor using a separate gh-pages branch because git would track every single change made to the automatically rendered content, leading to a messy git history.\nBy convention, the corresponding static website of a git repository is exposed at &lt;username&gt;.github.io/&lt;repository-name&gt;. For GitHub organizations which are popular to group repositories, the exposed URL would be &lt;orgname&gt;.github.io/&lt;repository-name&gt;.\n\n\n\n\n\n\nNote\n\n\n\nNote that the online version of the book you are currently reading is hosted in very similar fashion: rse-book.github.io\n\n\n\n\n10.3.2 GitHub Actions\nHosting a website on a modern git platform comes with an intriguing option: the use of continuous integration tools, as discussed in Chapter 8 on automation. CI/CD is capable of not only rendering Markdown, but can even run computations and virtually any prerequisite step thanks to the underlying docker technology. CI/CD certainly introduces a setup hurdle, but it allows integrating users who do not have the necessary stack installed to execute the render process locally.\n\n\n10.3.3 Netlify\nThe Netlify5 platform does not host git repositories like GitHub or Bitbucket, but it rather focuses on the build part. Netlify supports the user in setting up the build process and offers a few add-ons such as Netlify forms to allow for website features that can process user interaction such as online forms. Also, unlike git platforms, Netlify integrates domain purchase into its own platform. Note that Netlify and git are complementary, not mutually exclusive approaches. Platforms like Netlify and GitHub use application programming interfaces (APIs) to communicate with each other, allowing to trigger Netlify builds based on changes to a GitHub repositories.\nBasic GitHub Pages setups are good enough for online documentation of most software or other sites that simply display static content. Netlify cannot only improve convenience when using advanced features, but it can also extend possibilities for static websites when needed.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Publishing and Reporting</span>"
    ]
  },
  {
    "objectID": "publishing.html#visualization",
    "href": "publishing.html#visualization",
    "title": "10  Publishing and Reporting",
    "section": "10.4 Visualization",
    "text": "10.4 Visualization\nVisualization is a powerful tool to communicate insights derived from data analysis. Good visual communication can not only summarize large datasets, but it also can make your report – and therefore your insights – accessible to a wider audience. No matter if you are working on an online outlet or a printed publication of your analysis, it is safe to say that either channel benefits from aesthetically pleasant and concise visualization. Proficiency in data visualization is an indispensable part of a researcher’s or analyst’s toolbox.\nAlthough plots may look similar across channels, opportunities and challenges plots optimized for online use and traditional printed figures are substantially different. Obviously, the beholder cannot interactively adjust parameters in printed images, while online visualization can be interactive and needs to adapt to varying screens and devices. Hence, the visualization toolbox offers libraries with very different approaches.\n\n10.4.1 Rendered Graphs\nThe idea of visualization libraries that create rendered graphs is straightforward to understand. Libraries such as R’s ggplot2 (Wickham 2016) or Python’s matplotlib (Hunter 2007) fix the dots per inch (dpi) at render time. When writing to disk, such a graph is typically saved as a .png or .jpg file6. To the end user, handling such graphs is rather intuitive, as it feels similar to handling photos or screenshots: We have to handle single image files that we can easily preview with onboard tools of all major operating systems. But just like with a photo, if the resolution of an existing image is too small for our next use case, scaling the image up will lead to interpolation, i.e., loss in quality. To see the effect, take a closer look at the two .png files below: The second image doubles the width of the first image. Because of interpolation, the text looks blurred, particularly in its curves.\n\n\nA streamlined publication workflow, such as the Quarto-based approach described above, mitigates the problem because this type of workflow automation reduces the effort of resizing, re-rendering and fitting graphs into the current document. Further mitigation comes from extensions packages such as R’s gganimate (Pedersen and Robinson 2022) that allows to animate graphs created with ggplot2. Though you might miss out on bleeding-edge interaction and the maximum flexibility of libraries with an online focus, rendered graphs created with a powerful library such as ggplot or matplotlib are a solid way to go for most people’s use cases. The likes of ggplot2 have home court advantage in all things printed and still look decent in most online outlets.\n\n\n10.4.2 JavaScript Visualization Libraries\nTo those old enough to remember, JavaScript visualization may be reminiscent of a Japanese game convention at night: colorful and blinking. But the visualization knowledge embedded in modern JavaScript libraries and the maximum out-of-the-box opportunities of libraries like Apache echarts (Li et al. 2018), Data Driven Documents (d3js.org) (Bostock, Ogievetsky, and Heer 2011) or Highcharts7 have very little in common with the JavaScript of the web of the late-90s. In 2022, online communication can take your data visualization to another level.\nToday, there are basically two popular JavaScript-based approaches to data visualization: SVG manipulation and HTML5 drawing. No matter whether which one you choose, the opportunities are just off the charts (pun intended): From basic bar charts, scatter plots, candle stick and radar charts to heatmaps, treemaps, 3D and spatial charts, there is hardly anything web based visualization cannot do. And if you really needed special treatment, there is an API to extend your possibilities even. Let us quickly dive into both options.\nWith the advent of widespread Scalable Vector Graphics (SVG) support in mainstream web browsers about a decade ago, the online ecosystem enabled numerous options to add channel-specific value to visualization. Unlike formats with fixed resolutions like .gif, .jpg or .png formats, vector graphics are not only scalable without loss, but they are also defined inside your HTML document. This opens up the possibility to manipulate them easily using JavaScript as if they were objects of the DOM8. Mapping data to SVG aesthetics is the exact idea of the [Data Driven Documents (D3), one of the most popular and powerful visualization libraries. For example, your data may drive the size of dots or any other object, e.g., a car pictogram for when you want to visualize the number of car registrations. The above example shows a strength of the SVG approach: one can use existing SVGs without having to draw them from scratch using JavaScript.\nHTML5, on the other hand, is the latest approach and offers more options to support varying screen sizes and device types. While a look into mobile friendliness is beyond the scope of this book, I would like to show an example of how a screen medium (compared to print) can add value. Consider an interactive time series chart of a long-term economic indicator. Depending on the question at hand, you may be interested in the indicator’s development over decades or rather look at the last couple of years or months. Add a zoom window to not only switch between views, but to also continuously zoom in or out on some crisis or boom or draw an intertemporal comparison between different peaks and lows.\n\nlibrary(echarts4r)\nlibrary(kofdata)\nlibrary(tsbox)\n\ntsl &lt;- get_time_series('ch.kof.barometer')\nt_df &lt;- ts_df(tsl$ch.kof.barometer)\nt_df |&gt;\n   e_charts(time) |&gt;\n   e_line(value, symbol = \"none\") |&gt;\n   e_datazoom(type = \"slider\") |&gt;\n   e_title(\"Demo: Interactivity Adds Value\")\n\n\n\n\nScreenshot of an interactive chart created with echarts. Check the online book for the interactive version: https://rse-book.github.io/publishing.html#visualization\n\n\nBesides more universal libraries like D3 or echarts with all their options and opportunities, there are a few smaller libraries that are much less powerful but a lot simpler to master. Libraries like dygraphs are limited to, e.g., time series but are focused on making that one thing as convenient and inclusive as possible. Depending on your needs, such a smaller library may be an excellent option. Also, when creating a wrapper package, it is obviously easier to implement and maintain only a couple of graphics types as opposed to several hundreds.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Publishing and Reporting</span>"
    ]
  },
  {
    "objectID": "publishing.html#datapub",
    "href": "publishing.html#datapub",
    "title": "10  Publishing and Reporting",
    "section": "10.5 Data Publications",
    "text": "10.5 Data Publications\nAs mentioned in Chapter 6, publishing data on research repositories and catalogs is important not only to make research reproducible, but to help data workers to get credit for their contribution to research. Archives like Zenodo9 provide GitHub integration to automate data releases and versioning of data. In addition, archives can parse the citation file format (.cff)10 and generate unique object identification (DOI) numbers for your dataset as well a meta information for the archive entry. Thanks to rOpenSci, there is also curated R package called {cffr} (Hernangómez 2021) to help generate citation files.\n\n\n\n\nAllaire, J. J., Charles Teague, Carlos Scheidegger, Yihui Xie, and Christophe Dervieux. 2022. “Quarto.” https://doi.org/10.5281/zenodo.5960048.\n\n\nAllaire, JJ, Yihui Xie, Jonathan McPherson, Javier Luraschi, Kevin Ushey, Aron Atkins, Hadley Wickham, Joe Cheng, Winston Chang, and Richard Iannone. 2022. Rmarkdown: Dynamic Documents for r. https://github.com/rstudio/rmarkdown.\n\n\nBeg, Marijan, Juliette Taka, Thomas Kluyver, Olexandr Konovalov, Min Ragan-Kelly, Nicolas M. Thiéry, and Hans Fangohr. 2021. “Using Jupyter for Reproducible Scientific Workflows.” CoRR abs/2102.09562. https://arxiv.org/abs/2102.09562.\n\n\nBostock, Michael, Vadim Ogievetsky, and Jeffrey Heer. 2011. “D³ Data-Driven Documents.” IEEE Transactions on Visualization and Computer Graphics 17 (12): 2301–9. https://doi.org/10.1109/TVCG.2011.185.\n\n\nDervieux, Christophe, JJ Allaire, Rich Iannone, Alison Presmanes Hill, and Yihui Xie. 2022. Distill: ’R Markdown’ Format for Scientific and Technical Writing. https://CRAN.R-project.org/package=distill.\n\n\nHernangómez, Diego. 2021. “Cffr: Generate Citation File Format Metadata for r Packages.” Journal of Open Source Software 6 (67): 3900. https://doi.org/10.21105/joss.03900.\n\n\nHunter, J. D. 2007. “Matplotlib: A 2D Graphics Environment.” Computing In Science & Engineering 9 (3): 90–95.\n\n\nLi, Deqing, Honghui Mei, Yi Shen, Shuang Su, Wenli Zhang, Junting Wang, Ming Zu, and Wei Chen. 2018. “ECharts: A Declarative Framework for Rapid Construction of Web-Based Visualization.” Visual Informatics 2 (2): 136–46. https://doi.org/https://doi.org/10.1016/j.visinf.2018.04.011.\n\n\nPedersen, Thomas Lin, and David Robinson. 2022. Gganimate: A Grammar of Animated Graphics. https://CRAN.R-project.org/package=gganimate.\n\n\nWickham, Hadley. 2016. Ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Publishing and Reporting</span>"
    ]
  },
  {
    "objectID": "publishing.html#footnotes",
    "href": "publishing.html#footnotes",
    "title": "10  Publishing and Reporting",
    "section": "",
    "text": "https://quarto.org↩︎\nhttps://gohugo.io/↩︎\nhttps://jekyllrb.com/↩︎\nhttps://pages.github.com/↩︎\nhttps://netlify.app/↩︎\nWhen file size is not relevant, .png is usually preferred because it allows for transparent backgrounds.↩︎\nhttps://www.highcharts.com/↩︎\nThe Document Object Model (DOM) is the hierarchical definition of an HTML website, which in today’s web is modified by JavaScript.↩︎\nhttps://zenodo.org↩︎\nhttps://citation-file-format.github.io/↩︎",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Publishing and Reporting</span>"
    ]
  },
  {
    "objectID": "case-studies.html",
    "href": "case-studies.html",
    "title": "11  Case Studies",
    "section": "",
    "text": "11.1 SSH Key Pair Authentication\nThis section could be headed “log in like a developer”. SSH Key Pairs (often referred to as “RSA key pairs” because of the popular RSA encryption process) are a convenient, relatively secure way to log into an account. SSH-based connections, including secure copy (SCP), often make use of SSH Key Pairs instead of using a combination of username and password. Also, most git platforms use this form of authentication. The basic idea of key pairs is to have a public key and a private key. While the private key is never shared with anyone, the public key is shared with any server you want to log in to. It’s like getting a custom door for any house that you are allowed to enter: share your public key with the server admin/web portal, and you’ll be allowed in when you show your private key. In case you lose your private key or suspect it has been stolen, simply inform the admin, so they can remove the door (public key). This is where a little detail comes into play: you can password protect the authentication process. Doing so buys you time to remove the key from the server before your password gets brute-forced. The downside of this additional password is its need for interaction. So, when you are setting up a batch that talks to a remote server, that is when you do not want a key without a password.\nStep one en route to logging in like a grown up, is to create an RSA key pair. GitHub has a [1-2-3 type of manual1 to get it done. Nevertheless, I would like to show the TL;DR R Studio (Server) specific way here.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Case Studies</span>"
    ]
  },
  {
    "objectID": "case-studies.html#sec-rsa",
    "href": "case-studies.html#sec-rsa",
    "title": "11  Case Studies",
    "section": "",
    "text": "Login to your Posit Workbench, RStudio Server or start your Desktop RStudio IDE.\nGo to Tools → Global Options → Git/SVN.\nHit Create RSA KEY (When you have some crazy ASCII art reminiscent of a rabbit, it’s just ok.)\nClick ‘View Public Key’.\nCopy this key to your clipboard.\nYou can paste the key you obtained to your GitHub settings or put it into your server’s authorized keys file.\nDoing so allows your SSH agent to clone and interact with remote git repositories via SSH or log in to a server if your user is allowed to use a login shell by the server.\n\n\n\n\nThe R Studio GUI is an easy way to create a key pair. (Source: screenshot RStudio IDE taken in 2021.)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Case Studies</span>"
    ]
  },
  {
    "objectID": "case-studies.html#application-programming-interfaces",
    "href": "case-studies.html#application-programming-interfaces",
    "title": "11  Case Studies",
    "section": "11.2 Application Programming Interfaces",
    "text": "11.2 Application Programming Interfaces\nAn Application Programming Interface (API) is an interface to facilitate machine-to-machine communication. An interface can be anything, any protocol or pre-defined process. But, of course, there are standard and not-so-standard ways to communicate. Plus some matter-of-taste types of decisions. But security and standard compliance are none of the latter. There are standards such as the popular, URL-based REST that make developers’ lives a lot easier – regardless of the language they prefer.\nMany services such as Google Cloud, Amazon Webservices (AWS), your university library, your favorite social media platform or your local metro operator provide an API. Often, either the platform itself or the community provides what’s called an API wrapper: A simple program wraps the process of using the interface through dynamic URLs into a parameterized function. Because the hard work is done server-side by the API backend, building API wrappers is fairly easy, and, if you’re lucky, wrappers for your favorite languages exit already. If that is the case, end users can simply use functions like get_dataset(dataset_id) to download data programmatically.\n\n11.2.1 Example 1: The {kofdata} R Package\nThe KOF Swiss Economic Institute at ETH Zurich provides such a wrapper in the {kofdata} R package (Bannert and Thoeni 2022). The underlying API allows access to the KOF time series archive database and to obtain data and meta information alike. The below code snippet gets data from the API and uses another KOF built library (the {tstools} R package (Bannert, Thoeni, and Bisinger 2023)) to visualize the returned time series.\n\nlibrary(kofdata)\n# just for viz\nlibrary(tstools)\ntsl &lt;- get_time_series(\"ch.kof.barometer\")\ntsplot(tsl)\n\n\n\n\n\n\n\n\n\n\n11.2.2 Build Your Own API Wrapper\nHere’s an example of an elementary API wrapper that makes use of the Metropolitan Museum of Modern Art’s API to obtain identifiers of pictures based on a simple search2.\n\nlibrary(jsonlite)\n# Visit this example query\n# https://collectionapi.metmuseum.org/\n# public/collection/v1/search?q=umbrella\n# returns a json containing quite a few\n# ids of pictures that were tagged 'umbrella'\n\n#' Search MET\n#'\n#' This function searches the MET's archive\n#' for keywords and returns object ids of\n#' search hits. It is a simple wrapper\n#' around the MET's Application Programming\n#' interface (API). The function is designed \n#' to work with other API wrappers\n#' and use object ids as an input.\n#' @param character search term\n#' @return list containing the total \n#' number of objects found and a \n#' vector of object ids.\n#'\n# Note these declarations are not relevant\n# when code is not part of a package, \n# hence you need to call library(jsonlite)\n# in order to make this function work if\n# you are not building a package.\n#' @examples\n#' search_met(\"umbrella\")\n#' @importFrom jsonlite formJSON\n#' @export\nsearch_met &lt;- function(keyword){\n  # note how URLencode improves this function\n  # because spaces are common in searches\n  # but are not allowed in URLs\n  uri &lt;- \"https://collect...\"\n  url &lt;- sprintf(uri,\n  URLencode(keyword))\n  fromJSON(url)\n}\n\nYou can use these IDs with another endpoint to receive the pictures themselves3.\n\ndownload_met_images_by_id &lt;- \n  function(ids,\n           download = \"primaryImage\",\n           path = \"met\") {\n  # Obtain meta description objects from MET API\n  obj_list &lt;- lapply(ids, function(x) {\n    uri &lt;- \"https://collect...\"\n    req &lt;- download.file(sprintf(uri,\n    x),destfile = \"temp.json\")\n    fromJSON(\"temp.json\")\n  })\n  \n  \n  public &lt;- sapply(obj_list, \"[[\", \"isPublicDomain\")\n  \n  \n  # Extract the list elements that contains\n  # img URLs in order to pass it to the download function\n  img_urls &lt;- lapply(obj_list, \"[[\", download)\n  \n  # Note the implicit return, no return statement needed\n  # last un-assigned statement is returned\n  # from the function\n  for (x in unlist(img_urls[public])){\n    download.file(x, destfile = sprintf(\"%s/%s\",\n     path, basename(x)))\n  }\n  \n  \n  message(\n    sprintf(\"\n    The following images ids were not public\n    domain and could not be downloaded:\\n %s\",\n            paste(ids[!public], collapse = \",\")\n  ))\n  \n  message(\n    sprintf(\"\n    The following images ids were public\n    domain and could be downloaded to\\n %s: %s\",\n            path, paste(ids[public], collapse = \",\")\n  ))\n  \n}\n\nFinally, execute the functions: first, search for images with umbrellas, second, download these images by ID. Note that even if I do not display the image itself in the book to err on the side of caution w.r.t. to image property rights, the below code shows availability is checked, and an image is actually downloaded to a previously created folder called met.\n\n# Step 4: Use the Wrapper\numbrella_ids &lt;- search_met(\"umbrella\")\numbrella_ids$total\n\n[1] 746\n\nhead(umbrella_ids$objectIDs)\n\n[1] 491511  19840  19958   9167 122311 121842\n\ndownload_met_images_by_id(umbrella_ids$objectIDs[30:33])\n\n\n    The following images ids were not public\n    domain and could not be downloaded:\n 121919,121922,121809\n\n\n\n    The following images ids were public\n    domain and could be downloaded to\n met: 157159\n\ndir(\"met\")\n\ncharacter(0)\n\n\nWonder what’s really in the image? Umbrella or not :)? Try to reproduce this example or come up with your own wrapper from scratch.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Case Studies</span>"
    ]
  },
  {
    "objectID": "case-studies.html#create-your-own-api",
    "href": "case-studies.html#create-your-own-api",
    "title": "11  Case Studies",
    "section": "11.3 Create Your Own API",
    "text": "11.3 Create Your Own API\nThe ability to expose data is a go-to skill to make research reproducible and credible. Especially when data get complex and require thorough description in order to remain reproducible for others, a programmatic, machine-readable approach is the way to go.\n\n11.3.1 GitHub to Serve Static Files\nExposing your data through an API is not something for which you would necessarily need a software engineer, let alone your own server infrastructure for. Simply hosting a bunch of .csv spreadsheets alongside a good description (in separate files!!) on, e.g., GitHub for free is an easy and highly available solution.\nThe swissdata project4 proposes an implementation of such an approach. The project transforms all sorts of datasets into .csv spreadsheets to contain a cleaned version of the data alongside .json files that contain the data descriptions. The demo5 describes the implementation in a greater detail and hands on fashion.\nIf you add a naming convention for your files to such an implementation, you already have a solid interface to a data science environment. Consider the following simple R wrapper that downloads both data and metadata first and then reads both into R6. (Alternatively, direct streaming would be possible, too.)\n\nlibrary(jsonlite)\ndownload_swissdata &lt;- function(dataset){\n  d_ext &lt;- \".csv\"  \n  m_ext &lt;- \".json\"\n  d &lt;- sprintf(\"%s%s\", dataset, d_ext)\n  m &lt;- sprintf(\"%s%s\", dataset, m_ext)\n  gh_url &lt;- \"https://raw.git...\")\n  download.file(file.path(gh_url, d), d)\n  download.file(file.path(gh_url, m), m)\n}\n\n\ndownload_swissdata(\"ch_adecco_sjmi\")\n\nNow, read the data from disk into your R session\n\nd &lt;- read.csv(\"ch_adecco_sjmi.csv\")\nhead(d)\n\n  idx_type       date value\n1      sch 2003-03-01  31.9\n2     pzua 2003-03-01  12.9\n3      ins 2003-03-01   4.9\n4      unw 2003-03-01  14.2\n5      sch 2004-03-01  36.4\n6     pzua 2004-03-01  12.2\n\n\nAs well as the nested meta information{nested data}. JSON maps 1:1 to R lists. Hence, both the on-disk representation and the in-memory representation are equally well suited for nested data. The below example shows a sub-label element containing description in multiple languages.\n\nm &lt;- fromJSON(\"ch_adecco_sjmi.json\")\nm$labels$idx_type$unw\n\n$en\n[1] \"Company websites\"\n\n$de\n[1] \"Unternehmens-Webseiten\"\n\n$fr\nnamed list()\n\n$it\nnamed list()\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote, standard GitHub repositories are not well suited to host larger files or binaries. Check out their file hosting offering or consider other services focused on files.\n\n\n\n\n11.3.2 Simple Dynamic APIs\n\nEven, going past serving static files, does not require much software development expertise. Thanks to frameworks such as express.js or the {plumbr} package in the R ecosystem, it is easy to create an API that turns an URL into a parameterized server-side action. Before we look at one of those frameworks in greater detail, let’s take a look at the two most common HTTP request methods7 GET and POST.\nAccording to Mozilla’s definition, the GET method “requests a representation of the specified resource. Requests using GET should only retrieve data,” while POST “submits an entity to the specified resource, often causing a change in state or side effects on the server”. Obviously, there are many other methods for the HTTP protocol, but the above two should help to understand the idea behind standard compliant REST web application programming interfaces.\nLet’s assume you have a working nodejs8 JavaScript runtime environment installed on your local development machine, so you can run JavaScript files outside a web browser. Such a setup mimics the situation on a web server with Node.js installed. Also, let’s assume you have npm9 installed as a package manager to facilitate installing node packages from the npm open source package registry.\nFirst, create a folder api, go to the freshly created directory and initiate a node package.\n# run initialization in a dedicated folder\nmkdir api\ncd api\nmkdir public \nnpm init\nJust sleepwalk through the interactive dialog accepting all defaults. This created a package.json file to keep track of dependencies and their package versions used in your project. Once done, add express using the npm package manager.\nnpm install express\nNow, that we installed the JavaScript framework Express.js, we can use the framework to create a minimal web application that serves an API endpoint using the node runtime environment. Consider a minimal hello-world example that does about the same as the static file example of the previous action:\nconst express = require('express')\nconst app = express()\nconst port = 3000\n\napp.get('/', (req, res) =&gt; {\n  res.send('Hello World!')\n})\n\napp.use('/static', express.static('public'))\n\n\napp.listen(port, () =&gt; {\n  console.log(`RSE demo app listening on port ${port}`)\n})\nThe first app.get route simply maps the root, a plain, un-nested starting point so to say, in our case localhost:3000/ to the output of the res.send call. The second app command serves the content of the public folder to visitor’s of localhost:3000/static. So if a public/ folder inside the app folder contained a cover image of my Hacking for Science courses, this would be served at localhost:3000/static/h4sci.jpeg.\n\n\n\nCover illustration of the Hacking for Science Courses. (Source: own illustration.)\n\n\nNow let us make use of server-side features beyond simply serving static files and add the need for an API key to get the picture.\nconst express = require('express')\nconst app = express()\nconst port = 3000\n\napp.get('/', (req, res) =&gt; {\n  res.send('Hello World!')\n})\n\napp.use('/static', (req, res, next) =&gt; {  \n  var key = req.query['api_key'];\n  if (!key) res.send('api key required');\n  if (apiKeys.indexOf(key) === -1)  res.send('eh-eh,\n   wrong key.');\n  req.key = key;\n  next();\n})\n\n\n// NOTE THAT this is only a \n// demo example (!)\n// DO NOT use insecure passwords\n// in production.\n// Make sure to find better ways\n// to secure your keys! \n// also transferring keys in URLs\n// via GET is not optimal, \n// consider using an upfront\n// authentication method\nvar apiKeys = ['abc','123'];\napp.use('/static', express.static('public'))\n\napp.listen(port, () =&gt; {\n  console.log(`RSE demo api listening on port ${port}`)\n})\nWe simply added a mount point using app.use (instead of app.get) which makes sure that everything past /static is affected by the logic added to this mount. So, our Hello World! greeting is out there for anybody to see, while displaying the picture whose URL starts with /static needs an API key. Though this is only one, admittedly made up example of mapping URLs and parameters to functions via HTTP(S), it hints at the possibilities of dynamic APIs from database queries to web forms and many other applications. The above example shows also how frameworks like Express JavaScript10 or plumber (Schloerke and Allen 2022) facilitate the definition of machine-to-machine interfaces even for less experienced developers. The impact of frameworks is not limited to the technical implementation, though. Developers benefit from comprehensive approaches like Swagger and the OpenAPI specification during the conceptual part of creating machine to machine interfaces.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Case Studies</span>"
    ]
  },
  {
    "objectID": "case-studies.html#sec-webscrape",
    "href": "case-studies.html#sec-webscrape",
    "title": "11  Case Studies",
    "section": "11.4 A Minimal Webscraper: Extracting Publication Dates",
    "text": "11.4 A Minimal Webscraper: Extracting Publication Dates\nEven though KOF Swiss Economic Institute offers a REST API to consume publicly available data, publication dates are unfortunately not available through in API just yet. Hence, to automate data consumption based on varying publication dates, we need to extract upcoming publication dates of the Barometer from KOF’s media release calendar. Fortunately, all future releases are presented online in an easy-to-scrape table. So, here’s the plan:\n\nUse Google Chrome’s inspect element developer feature to find the X-Path (location in the Document Object Model) of the table.\nRead the web page into R using rvest.\nCopy the X-Path string to R to turn the table into a data.frame\nUse a regular expression to filter the description for what we need.\n\nFirst, let’s take a look at our starting point, the media releases sub-page, first.\n\n\n\nThe KOF Swiss Economic Institute’s media agenda in 2021. (Source: screenshot Google Chrome browser.)\n\n\nThe website looks fairly simple, and the jackpot is not hard, presented in a table right in front of us. Can’t you smell the data.frame already?\nRight-click the table to see a Chrome context window pop up. Select inspect.\n\n\n\nModern browsers come with developer tools built in to help you expect a website’s source code. (Source: screenshot Google Chrome browser.)\n\n\nHover over the highlighted line in the source code at the bottom. Make sure the selected line marks the table. Right click again, select copy → copy X-Path11.\n\n\n\nBrowser tools help you explore the hierarchical structure of a website in interactive fashion. (Source: screenshot Google Chrome browser.)\n\n\nOn to R!\n\nlibrary(rvest)\n# URL of the media release subsite\nurl &lt;- \"https://kof.ethz.ch...\"\n# Extract the DOM object from the path we've \n# previously detected using \n# Chrome's inspect feature\ntable_list &lt;- url %&gt;%\n  read_html() %&gt;%\n  html_nodes(xpath = '\n  //html/body/div[6]/section/div/section\n  /div[2]/div/div[3]/div/div/div/\n  div/div/div/div/table') %&gt;%\n  # turn the HTML table into an R data.frame\n  html_table()\n# because the above result may potentially contain\n# multiple tables, we just use \n# the first table. We know from visual\n# inspection of the site that this is the\n# right table. \nagenda_table &lt;- table_list[[1]]\n\n# *[@id=\"t-6b2da0b4-cec0-47a4-bf9d-\n#   bbfa5338fec8-tbody-row-2-cell-2\"]\n\n# extract KOF barometer lines\npub_date &lt;- agenda_table[grep(\"barometer\",\n                         agenda_table$X3),]\npub_date\n\nYay! We got everything we wanted. Ready to process.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Case Studies</span>"
    ]
  },
  {
    "objectID": "case-studies.html#automate-script-execution-an-example-with-github-actions",
    "href": "case-studies.html#automate-script-execution-an-example-with-github-actions",
    "title": "11  Case Studies",
    "section": "11.5 Automate Script Execution: An Example with GitHub Actions",
    "text": "11.5 Automate Script Execution: An Example with GitHub Actions\ncontainerization as described in the infrastructure section has standardized virtual hardware into building blocks that are available across different platforms and providers. This development has led to a wide array of offerings that allow us to run our code in a language-agnostic environment. In addition to the major cloud platforms, git providers that offer a CI/CD tool chain are a good option to run our code on a remote container. Though originally designed mostly for build automation, CI/CD such as GitHub actions can be used to automate a variety of tasks by running them on a container. Consider the following minimal R script that we will run on GitHub actions.\n\n#!/usr/bin/env Rscript\n\nlibrary(knitr)\nknit(\"README.Rmd\")\n\nThe script uses {knitr} to render an RMarkdown document to a Markdown document that is automatically rendered to a pretty HTML file when pushed to a repository hosted on GitHub.\n\n\n\n\n\n\nNote\n\n\n\nNotice the common shebang comment that defines the executable for batch execution of a script file – in this case Rscript to run R outside of an interactive R session.\n\n\nEmbedded in a bit of descriptive text before and after the code, the main code chunk of the .Rmd template file is equally straightforward:\n\nlibrary(kofdata)\nlibrary(tsbox)\ntsl &lt;- get_time_series(\"ch.kof.barometer\")\nts_plot(tsl$ch.kof.barometer)\ncat(sprintf(\"last update on %s.\",\n as.character(Sys.Date())))\n\nWe make use of the {kofdata} API wrapper to obtain data from the KOF Data API and use the {tsbox} to visualize the time series data we received. Finally, we print the system date of the runtime environment – in our case the GitHub Actions Docker container. Running the README.Rmd file yields two artifacts: (1) a README.md file and a (2) .png graph located in a figure/ folder. Because the above runs inside a temporary single purpose container, our workflow needs to make sure those artifacts are stored persistently.\nThe below .yaml file defines the environment and workflow for GitHub Actions. When located properly according to GitHub’s convention, i.e., in a hidden github/worflows folder, GitHub identifies the .yaml file as input and allows executing the workflow. (Standalone build tools like Woodpecker or other integrated CI/CD tools like GitLab CI/CD use very similar workflow definitions).\n# Simple data transformation workflow\nname: KOF Economic Barometer\n\n# Controls when the action will run. \n# Triggers the workflow on push or pull request\n# events but only for the main branch\non:\n  workflow_dispatch\n\njobs:\n  kof:\n    runs-on: ubuntu-latest\n    container: rocker/verse\n    steps:\n      - uses: actions/checkout@v3\n      - run: git version\n      - run: rm -rf README.md\n      - run: |\n          Rscript -e \\ \n          && 'install.packages(c(\"kofdata\",\"tsbox\"))'\n          chmod +x demo_gha.R\n          ./demo_gha.R\n      - run: ls -lah\n      - run: |\n          git config --global --add safe.directory \\\n          && /__w/h4sci-gha-demo/h4sci-gha-demo\n          git config --global user.name \\\n          && \"Matt Bannert\"\n          git config --global user.email \\\n          && \"mbannert@users.noreply.github.com\"\n          git add figure README.md\n          git commit -m \\\n          && \"Update README.md via GH Actions\"\n          git push\nFirst, the below file gives our workflow a name to identify the workflow among other workflows defined within the same repository. The on block defines what triggers the workflow. The workflow_dispatch option is a rather uncommon trigger, as it simply means the workflow can be triggered by pressing a button in GitHub’s Web GUI. Cronjob-based triggers or triggers based on git actions such as pushes to a certain branch are more common as we are looking to avoid interaction at runtime. Inside the job definitions itself, we first define the operating system of the host and the Docker image in which our process should run.\nThen, walk through the single steps of our workflow. Notice that actions/checkout@v3 is different from the other steps because it is taken from GitHub’s marketplace for actions. Instead of writing standard operations, namely checking out the git repository we’re working with to the container that runs the action, from scratch, we use the marketplace action for that. Be aware though that there is a trade-off between convenience and transparency. When I was teaching this year and wanted to recycle an example from the previous year, it was not all that convenient. Only one year later, my example that leaned heavily on marketplace actions was not working anymore. What’s worse is that it was also relatively hard to debug because I had to deal with the inner workings of a dependency heavy implementation that I would not have implemented this way. If we look at the above steps after the checkout action, we see a list of simple steps that are easy to read: first, simply print the git version to make sure git is working inside the container, and we know its version in case we need to debug. Second, we remove the README.md file we have just checked out. This file will be replaced anyway, and we want to avoid any rights conflicts overwriting the file. Then we run R in batch to install two more specific packages to the R environment running inside the container. Because we use a pre-built tidyverse images from the rocker project, we do not have to install R itself and many common packages. We continue to use the chmod Linux command to change the access rights of our minimal R script. With the shebang comment inside the file, we can directly execute the .R file with the ./ prefix because it knows which executable to use to run it. Finally, we take a look at the content of the current folder before we commit and push all the files we generated back to our GitHub repository. After the process is done, the container will stop and removed.\n\n\n\n\n\n\nNote\n\n\n\nNote that we can see all the output of the process from the GitHub actions menu on our repository’s website. This is why it’s useful to print outputs of intermediate steps.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Case Studies</span>"
    ]
  },
  {
    "objectID": "case-studies.html#sec-map",
    "href": "case-studies.html#sec-map",
    "title": "11  Case Studies",
    "section": "11.6 Choropleth Map: Link Data to a GeoJSON Map File",
    "text": "11.6 Choropleth Map: Link Data to a GeoJSON Map File\nData visualization is a big reason for researchers and data analysts to look into programming languages. Programming languages do not only provide unparalleled flexibility, but they also make data visualization reproducible and allow placing charts in different contexts, e.g., websites, printed outlets or social media.\nOne of the more popular types of plots that can be created smoothly using a programming language is the so-called choropleth. A choropleth maps values of a variable that is available by region to a given continuous color palette on a map. Let’s break down the ingredients of the below map of Switzerland12.\n\n\n\n\n\n\nFirst, we need a definition of a country’s shape. Those definitions come in various formats, from traditional shape files to web-friendly GeoJSON. Edzer Pebesma’s useR! 2021 keynote13 has a more thorough insight.\nSecond, we need some data.frame that simply connects values to regions. In this case, we use regions defined by the Swiss Federal Statistical Office (FSO). Because our charting library makes use of the GeoJSON convention to call the region label “name” we need to call the column that holds the region names “name” as well. That way, we can safely use defaults when plotting. Ah, and note that the values are absolutely bogus that came to my mind while writing this (so please do not mull over how these values were picked).\n\nd &lt;- data.frame(\n  name = c(\"Zürich\",\n           \"Ticino\",\n           \"Zentralschweiz\",\n           \"Nordwestschweiz\",\n           \"Espace Mittelland\",\n           \"Région lémanique\",\n           \"Ostschweiz\"),\n  values = c(50,10,100,50,23,100,120)\n)\n\nFinally, we are calling our charting function from the echarts4r package. {echarts4r} (Li et al. 2018) is an R wrapper for the feature-rich Apache Echarts JavaScript plotting library. The example uses the base R pipes (available from 4+ on, former versions needed to use pipes via extension packages.). Pipes take the result of one previous line and feed it as input into the next line. So, the data.frame d is linked to a charts instance and the name column is used as the link. Then a map is registered as CH and previously read JSON content is used to describe the shape.\n\nd |&gt;\n  e_charts(name) |&gt;\n  e_map_register(\"CH\", json_ch) |&gt;\n  e_map(serie = values, map = \"CH\") |&gt;\n  e_visual_map(values,\n               inRange = list(color = viridis(3)))\n\nAlso note the use of the viridis functions which returns three values from the famous, colorblind friendly viridis color palette.\n\nviridis(3)\n\n[1] \"#440154FF\" \"#21908CFF\" \"#FDE725FF\"\n\n\nHere’s the full example:\n\nlibrary(echarts4r)\nlibrary(viridisLite)\nlibrary(jsonlite)\n\njson_ch &lt;- jsonlite::read_json(\n  \"https://raw.github...\"\n)\n\n\n\nd &lt;- data.frame(\n  name = c(\"Zürich\",\n           \"Ticino\",\n           \"Zentralschweiz\",\n           \"Nordwestschweiz\",\n           \"Espace Mittelland\",\n           \"Région lémanique\",\n           \"Ostschweiz\"),\n  values = c(50,10,100,50,23,100,120)\n)\n\nd |&gt;\n  e_charts(name) |&gt;\n  e_map_register(\"CH\", json_ch) |&gt;\n  e_map(serie = values, map = \"CH\") |&gt;\n  e_visual_map(values,\n               inRange = list(color = viridis(3)))",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Case Studies</span>"
    ]
  },
  {
    "objectID": "case-studies.html#web-applications-with-r-shiny",
    "href": "case-studies.html#web-applications-with-r-shiny",
    "title": "11  Case Studies",
    "section": "11.7 Web Applications with R Shiny",
    "text": "11.7 Web Applications with R Shiny\nTo start, let me demystify the {shiny} R package (Chang et al. 2022). There are basically two reasons why so many inside data science and analytics have Shiny on their bucket list of things to learn. First, it gives researchers and analysts home court advantage on a web server. Second, it gives our online appearances a kick start in the dressing room.\nDon’t be surprised though if your web development professional friend outside data science and analytics never heard of it. Compared to web frontend framework juggernauts such as react, angular or vue.js the Shiny web application framework for R is rather a niche ecosystem.\nInside the data science and analytics communities, fancy dashboards and the promise of an easy, low hurdle way to create nifty interactive visualizations have made {shiny} app development a sought-after skill. Thanks to pioneers, developers and teachers like Dean Attali, John Coene, David Granjon, Colin Fay and Hadley Wickham, the sky seems the limit for R Shiny applications nowadays.\nThis case study does not intend to rewrite {shiny}’s great documentation or blogs and books around it. I’d rather intend to help you get your first app running asap and explain a few basics along the way.\n\n11.7.1 The Web Frontend\n\n\n\nMaybe I am late, but I look goood. Johnny representing the frontend. (Source: own illustration.)\n\n\nStats and figures put together by academic researchers or business analysts are not used to spending a lot of time in front of the mirror. (Often, for the same reason as their creators: perceived opportunity costs.)\nShiny bundles years worth of limelight experience and online catwalk professionalism into an R package. Doing so allows us to use all this design expertise through an R interface, abstracting away the need to dig deep into web programming and frontend design (you know the HTML/CSS/JavaScript).\nLet’s consider the following web frontend put together with a few lines of R code. Consider the following, simple web fronted that lives in a dedicated user interface R file, called ui.R:\n\nlibrary(shiny)\nlibrary(shinydashboard)\n\ndashboardPage(\n  dashboardHeader(title = \"Empty App\"),\n  dashboardSidebar(),\n  dashboardBody(\n    fluidPage(\n      fluidRow(\n        box(title = \"Configuration\",\n            sliderInput(\"nobs\",\n                        \"Number of Observations\",\n                        min = 100,\n                        max = 10000,\n                        value = 500),\n            sliderInput(\"mean_in\",\"Mean\",\n                        min = 0,\n                        max = 10,\n                        value = 0),\n            sliderInput(\"sd_in\",\"SD\",\n                        min = 1,\n                        max = 5,\n                        value = 1),\n            width = 4),\n        box(title = \"Distribution\",\n            plotOutput(\"dist\"),\n            width = 8)\n      ),\n      fluidRow(\n        box(\"Tabelle\",\n            dataTableOutput(\"tab\"),\n            width=12\n            )\n      )\n    )\n  )\n)\n\nBesides the {shiny} package itself, the ecosystem around Shiny brings popular frontend frameworks from the world outside data science to R. In the above case, a boilerplate library for dashboards is made available through the add-on package {shinydashboard} (Chang and Borges Ribeiro 2021).\nTake a moment to consider what we get readily available at our fingertips: Pleasant user experience (UX) comes from many things. Fonts, readability, the ability to adapt to different screens and devices (responsiveness), a clean, intuitive design and many other aspects. The {shinydashboard} package adds components like fluidPages or fluidRow to implement a responsive (Google me!), grid-based design using R. Note also how similar the hierarchical, nested structure of the above code is to HTML tagging. (Here’s some unrelated minimal HTML)\n&lt;!-- &lt; &gt; denotes an opening,\n &lt;/ &gt; denotes an end tag. --&gt;\n&lt;html&gt;\n  &lt;body&gt;\n  &lt;!-- anything in between tags is affected by\n       the tags formatting.\n       In this case bold --&gt;\n    &lt;b&gt; some bold title &lt;/b&gt;\n    &lt;p&gt;some text&lt;/p&gt;\n  &lt;/body&gt;\n&lt;/html&gt;\n\n{shiny} ships with many widgets14 such as input sliders or table outputs that can simply be placed somewhere on your site. Again, add-on packages provide more widgets and components beyond those that ship with Shiny.\n\n\n11.7.2 Backend\nWhile the frontend is mostly busy looking good, the backend has to do all the hard work, the computing, the querying – whatever is processed in the background based on user input.\n\n\n\nWhile the frontend is busy looking spiffy, the backend does the computation work. (Source: own illustration.)\n\n\nUnder-the-hood-work that is traditionally implemented in languages like Java, Python or PHP15 can now be done in R. This is not only convenient for the R developer who does not need to learn Java, it’s also incredibly comfy if you got data work to do. Or put differently: who would like to implement logistic regression, random forests or principal component analysis in PHP?\nConsider the following minimal backend server.R file which corresponds to the above ui.R frontend file. The anonymous (nameless) function which is passed on to the ShinyServer function takes two named lists, input and output, as arguments. The named elements of the input list correspond to the widgetId parameter of the UI element. In the below example, our well-known base R function rnorm takes nobs from the input as its n sample size argument. Mean and standard deviation are set in the same fashion using the user interface (UI) inputs.\n\nlibrary(shiny)\n\nshinyServer(function(input,output){\n  \n  output$dist &lt;- renderPlot({\n    hist(\n      rnorm(input$nobs,\n            mean = input$mean_in,\n            sd = input$sd_in),\n      main = \"\",\n      xlab = \"\"\n      )\n  })\n  \n  output$tab &lt;- renderDataTable({\n    mtcars\n  })  \n})\n\nThe vector that is returned from rnorm is passed on to the base R hist which returns a histogram plot. This plot is then rendered and stored into an element of the output list. The dist name is arbitrary, but again it matched to the UI. The plotOutput function of ui.R puts the rendered plot onto the canvas, so it’s on display in people’s browsers. renderDataTable does so in analog fashion to render and display the data table.\n\n\n11.7.3 Put Things Together and Run Your App\nThe basic app shown above consists of an ui.R and a server.R file living in the same folder. The most straightforward way to run such an app is to call the runApp() function and provide the location of the folder that contains both of the aforementioned files.\n\nlibrary(shiny)\nrunApp(\"folder/that/holds/ui_n_server\")\n\nThis will use your machine’s built-in web server and run Shiny locally on your notebook or desktop computer. Even if you never put your app on a web server and run a website with it, it is already a legitimate way to distribute your app. If it was part of an R package, everyone who downloads your package could use it locally, maybe as a visual inspection tool or a way to derive inputs interactively and feed them into your R calculation.\n\n\n11.7.4 Serve Your App\nTruth be told, the full hype and excitement of a Shiny app only comes into play when you publish your app and make it available to anyone with a browser, not just the R people. Though hosting is a challenge in itself, let me provide you a quick Shiny specific discussion here. The most popular options to host a Shiny app are\n\nSoftware-as-a-service (SaaS). No maintenance, hassle-free, but least bang for the buck. The fastest way to hit the ground running is R Studio’s service shinyapps.io.\nOn premise, aka in-house. Either download the open source version of Shiny server, the alternative Shiny proxy or the Posit Connect premium solution and install them on your own Virtual Machine.\nUse a Shiny server Docker image and run a container in your preferred environment.\n\n\n\n11.7.5 Shiny Resources\nOne of the cool things of learning shiny is how Shiny, and its ecosystem, allow you to learn quickly. Here are some of my favorite resources to hone your Shiny skills.\n\n\n\nThe {shinydashboard} R package provides customizable skeletons for dashboards. (Source: screenshot of a blank dashboard created with shinydashboard.)\n\n\n\nR Studio Shiny’s Widget Gallery: https://shiny.rstudio.com/gallery/widget-gallery.html\nshinydashboard (Chang and Borges Ribeiro 2021): https://rstudio.github.io/shinydashboard\nMastering Shiny (Wickham 2021): (https://mastering-shiny.org/)\nEngineering Production Grade Shiny Apps (Fay et al. 2021): https://engineering-shiny.org/\nRInterface by David Granjon, John Coene, Victor Perrier and Isabelle Rudolf: https://rinterface.com/",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Case Studies</span>"
    ]
  },
  {
    "objectID": "case-studies.html#project-management-basics",
    "href": "case-studies.html#project-management-basics",
    "title": "11  Case Studies",
    "section": "11.8 Project Management Basics",
    "text": "11.8 Project Management Basics\nThe art of stress-free productivity, as I once called it in 2010 blog post, has put a number of gurus on the map and a whole strand of literature on our bookshelves. So, rather than adding to that, I would like to extract a healthy, best-of-breed type of dose here. The following few paragraphs do not intend to be comprehensive, not even for the scope of software projects, but inspirational.\nIn the software development startup community, the waterfall approach became synonymous to conservative, traditional and ancient: Over specification in advance of the project, premature optimization and a lawsuit over expectations that weren’t met. Though, waterfall projects may be better than their reputation, specifications should not be too detailed and rigid.\nMany software projects are rather organized in agile fashion, with SCRUM and KANBAN being the most popular derivatives. Because empirical academic projects have a lot in common with software projects inasmuch that there is a certain expectation and quality control, but the outcome is not known in advance. Essentially, in agile project management you roughly define an outcome similar to a minimum viable product (MVP). That way, you do not end up with nothing after a year of back and forth. During the implementation you’d meet regularly, let’s say every 10 days, to discuss development since the last meet and the short-term plans for the steps ahead. The team splits work into tasks on the issue tracker and assigns them. Solutions to problems will only be sketched out and discussed bilaterally or in small groups. By defining the work package for only a short timespan, the team stays flexible. In professional setups, agile development is often strictly implemented and makes use of sophisticated systems of roles that developers and project managers can get certified for.\nMajor git platforms ship with a decent, carpentry-level project management GUI. The issue tracker is at the core of this. If you use it the minimal way, it’s simply a colorful to-do list. Yet, with a bit of inspiration and the use of tags, comments and projects, an issue tracker can be a lot more.\n\n\nThe GitHub issue tracker (example from one of the course’s repositories) can be a lot more than a to-do list.\n\nSwim lanes (reminiscent of a bird’s-eye view of an Olympic pool) can be thought of columns that you have to walk through from left to right: To Do, Doing, Under Review, done. (you can also customize the number and label of lanes and event associate actions with them, but let’s stick to those basic lanes in this section.) The idea is to use to keep track of the process and make the process transparent.\n\n\n\nGitHub’s web platform offers swim lanes to keep a better overview of issues being worked on. (Source: own GitHub repository.)\n\n\n\n\n\n\n\n\nTip\n\n\n\nNo lane except “Done” should contain more than 5–6 issues. Doing so prevents clogging the lanes at a particular stage which could potentially lead to negligent behavior, e.g., careless reviews.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Case Studies</span>"
    ]
  },
  {
    "objectID": "case-studies.html#parallel-computation",
    "href": "case-studies.html#parallel-computation",
    "title": "11  Case Studies",
    "section": "11.9 Parallel Computation",
    "text": "11.9 Parallel Computation\nThe idea of having multiple workers at your disposal to fix your math problem even quicker than one smart worker seems equally appealing to third graders and researchers. Very much like in school, the ability to assess whether getting help in the first place is worth the overhead and what type of help to get, is the most important skill. The classical challenge where parallel computation using multiple cores really helps is throughput problems, i.e., problems where your tasks are independent of each other. Yet, it may not be clear up-front if the number and computation time of your single tasks justifies the overhead of letting your program know it should split computations across cores and manage memory accordingly. Also, consider that processes can be turned parallel at different levels: you could simply use your local machine’s cores and a parallelization implementation such as R’s {future} (Bengtsson 2021) or {parallel} packages to parrallelize locally. Or you could go parallel at a machine or container level. While the former is easier as it requires less infrastructure knowledge, it is limited by the resources of your local machine. Of course, if you have access to a HPC cluster, this may not be a disadvantage at all (depending on how your cluster balances load and manages access to resources). In any case, you should make a clear decision at which level you go parallel and avoid nested parallelization.\nLet’s consider the following example of a simple local parallelization, including a performance benchmark. The following R example uses the {microbenchmark} R package (Mersmann 2021) to check the effect of parallel computing on running seasonal adjustment of multiple time series. Before we start with the actual example, let’s create some demo data: We simply create a list with 1000 elements, each of which is the same monthly time series about airline passengers from 1949 to 1960.\n\ndata(\"AirPassengers\")\ntsl &lt;- list()\nfor(i in 1:1000){\n  tsl[[i]] &lt;- AirPassengers\n}\n\nNow, let’s load the {seasonal} R package (Sax and Eddelbuettel 2018) and perform a basic seasonal adjustment of each of these time series. The first statement performs 100 adjustments sequentially; the second statement uses parallel computing to spread computations across the processors of the machine that ran this example.\n\nlibrary(seasonal)\nlibrary(parallel)\nlibrary(microbenchmark)\n\n\nno_parallel &lt;- function(li, s){\n  lapply(li[s],seas)\n}\n\nwith_parallel &lt;- function(li, s){\n  out &lt;- list()\n  cl &lt;- makeCluster(detectCores())\n  # load 'seasonal' for each node\n  clusterEvalQ(cl, library(seasonal))\n  parLapply(cl, li[s], function(e) try(seas(e)))\n  stopCluster(cl)\n}\n\n\nout &lt;- microbenchmark(\n    noparallel10 = no_parallel(tsl, 1:10),\n    withparallel10 = with_parallel(tsl, 1:10),\n    noparallel100 = no_parallel(tsl, 1:100),\n    withparallel100 = with_parallel(tsl, 1:100),\n    times = 5,\n    unit = \"seconds\"\n)\n\nd &lt;- summary(out)\n\nObviously, the absolute computation time depends on the hardware used as well as the operating system, depending on the task at hand.\nAs expected, given the perfect independence of the tasks from each other, performance gains are quite substantial (~5.5 times faster) for the above example, though not perfect (on my 8-core machine). Advanced parallel implementations and deeper dives into the way different processors work may further optimize efficiency here.\n\nlibrary(kableExtra)\nkable(d[,c(1:4)],\"latex\",\n      booktabs = TRUE,\n      escape= FALSE) |&gt;\n      kable_styling(\n        latex_options = c(\"repeat_header\")\n        ) |&gt;\n      column_spec(1:2, width = \"2.5cm\")\nkable(d[,c(1,5,6,7)],\"latex\",\n      booktabs = TRUE,\n      escape= FALSE) |&gt;\n      kable_styling(\n        latex_options = c(\"repeat_header\")\n        ) |&gt;\n      column_spec(1:2, width = \"2.5cm\")\n\n\n\nTable 11.1: Runtimes with and without Parallelization.\n\n\n\n\n\n\n\n\n\n\n\n\nYet, the key takeaway from the above exercise is not the parallel computation itself, but the ability to evaluate the need to go parallel and how. Benchmarking does not only help to get a ballpark estimate of the costs should you go outside for more computational power, it gives you an idea whether the gains from a parallel approach are worth the hassle. In other words, of course a computation running four to five times faster than before sounds spectacular, but what if the total runtime is less than a few minutes either way? Development time, particularly for newcomers, is very likely longer than a few minutes. Plus, going parallel jeopardizes your cross-platform compatibility depending on the parallel implementation you use.\nAnother good use of benchmarking is to run a few smaller case experiments to get an idea of how performance gains evolves when we throw more work at our system. Unlike the below example, performance gains do not necessarily have to be linear. Visualization can give us a better idea.\n\nlibrary(ggplot2)\nlibrary(viridis)\nvizframe &lt;- d[,c(1,5)]\nvizframe$iterations &lt;- as.character(c(10,10,100,100))\nvizframe$parallel &lt;- c(\"no\",\"yes\",\"no\",\"yes\")\nnames(vizframe)[2] &lt;- \"seconds\"\n\ngg &lt;- ggplot(\n  vizframe[,-1],\n aes(fill = parallel, \n     y = seconds,\n     x = iterations))\ngg + \n  geom_bar(position = \"dodge\",\n           stat = \"identity\") + \n  coord_flip() + \n  scale_fill_viridis(discrete = T) +\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 11.1: Comparison of computation time (shorter bar is faster).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Case Studies</span>"
    ]
  },
  {
    "objectID": "case-studies.html#good-practice",
    "href": "case-studies.html#good-practice",
    "title": "11  Case Studies",
    "section": "11.10 Good Practice",
    "text": "11.10 Good Practice\nEven though I cannot really aim for a comprehensive section on the Do’s and Dont’s, I would like to share a few common-sense practices here. If you are interested in a more thorough but rather R specific collection, take a look at the wonderful online book What They Forgot to Tell Us About R: https://rstats.wtf/.\nDo NOT Set Static Working Directories in Your Code\nLike C:\\Users\\MatthiasBannert\\My Documents\\R Course\\. Locations that only exist in your own environment have you set up for trouble before you even started.\nPlease,\n\nresist that visceral urge to define the project root folder somewhere at the beginning of your code because your collaborators usually do not have a directory on their machine that bears your name. That might not even use the same operating system. And before you ask, simply putting the neutrally named folder on a shared NAS storage or service such as Dropbox is not really better.\navoid spaces in folder and file names. Though modern operating systems and most languages can handle spaces, you might need to escape certain characters and make a beginner’s life much harder. The same holds for umlauts and other hieroglyphic characters. Simply use lower-case and either kebab-case or snake_case.\nwork with projects of your favorite IDE. The idea of projects is to make a folder the document root of your project. That way, your code can reference to other files inside this project root in relative fashion, no matter where the project itself is located.\n\nTake Time To Learn How To Play the “Piano”\nThere is no need to become a vim virtuoso (that’s essentially Lang Lang status on a computer keyboard), but make sure to learn some basic shortcuts of your environment that help you avoid touching the mouse or trackpad. Like ctrl+L to clear the screen, cmd+tab to switch between applications, cmd+w to close windows, ctrl+number to switch between tabs and, most importantly, some shortcut to execute the selected line of code (often cmd+enter, ctrl+enter or shift+enter depending on your operating system). Note that this is really not so much about a specific programming language, but more about the environment you work in.\nManage Dependencies at the Project Level\nJavaScript projects manage dependencies in their lock files, Python projects have their requirements.txt files, and R has the {renv} package (Ushey and Wickham 2023). All these scripting languages can keep track of the exact library versions used in a project.\n\npip freeze |\n grep -v \"pkg-resources\" &gt; requirements.txt\n\nThe above pip command extracts dependencies from a Python project’s folder and writes it to a requirements file. Nevertheless, because there is no build process, scripting languages do not enforce keeping track of library versions necessarily. And even though it is common sense in engineering, it took data science, R in particular, quite a while to really establish going the extra mile and keep track of a project’s dependencies. For many, an R project A simply loads a few libraries, does its job, while another R project B loads its libraries and does another job – all on the same local machine. After a few more projects, we decided to upgrade R to the next minor release, e.g., go from 4.1. to 4.2. which cause reinstalling all the extensions packages. It is a very safe bet that at least one of the packages we use gets its own update (or two) between two R minor releases. When all projects share the same library, your project that has just started after the latest package release may benefit from the latest feature of a package, while another project might break because its code is affected by a breaking change. Given that most packages are not large in size, I encourage everyone starting a programming with data project to embrace project-based library version management early on.\n\n\n\n\nBannert, Matthias, and Severin Thoeni. 2022. Kofdata: Get Data from the ’KOF Datenservice’ API. https://CRAN.R-project.org/package=kofdata.\n\n\nBannert, Matthias, Severin Thoeni, and Stéphane Bisinger. 2023. Tstools: A Time Series Toolbox for Official Statistics. https://CRAN.R-project.org/package=tstools.\n\n\nBengtsson, Henrik. 2021. “A Unifying Framework for Parallel and Distributed Processing in r Using Futures.” The R Journal 13 (2): 208–27. https://doi.org/10.32614/RJ-2021-048.\n\n\nChang, Winston, and Barbara Borges Ribeiro. 2021. Shinydashboard: Create Dashboards with ’Shiny’. https://CRAN.R-project.org/package=shinydashboard.\n\n\nChang, Winston, Joe Cheng, JJ Allaire, Carson Sievert, Barret Schloerke, Yihui Xie, Jeff Allen, Jonathan McPherson, Alan Dipert, and Barbara Borges. 2022. Shiny: Web Application Framework for r. https://CRAN.R-project.org/package=shiny.\n\n\nFay, C., S. Rochette, V. Guyader, and C. Girard. 2021. Engineering Production-Grade Shiny Apps. Chapman & Hall/CRC the r Series. CRC Press. https://books.google.ch/books?id=qExDEAAAQBAJ.\n\n\nLi, Deqing, Honghui Mei, Yi Shen, Shuang Su, Wenli Zhang, Junting Wang, Ming Zu, and Wei Chen. 2018. “ECharts: A Declarative Framework for Rapid Construction of Web-Based Visualization.” Visual Informatics 2 (2): 136–46. https://doi.org/https://doi.org/10.1016/j.visinf.2018.04.011.\n\n\nMersmann, Olaf. 2021. Microbenchmark: Accurate Timing Functions. https://CRAN.R-project.org/package=microbenchmark.\n\n\nSax, Christoph, and Dirk Eddelbuettel. 2018. “Seasonal Adjustment by X-13ARIMA-SEATS in R.” Journal of Statistical Software 87 (11): 1–17. https://doi.org/10.18637/jss.v087.i11.\n\n\nSchloerke, Barret, and Jeff Allen. 2022. Plumber: An API Generator for r. https://CRAN.R-project.org/package=plumber.\n\n\nUshey, Kevin, and Hadley Wickham. 2023. Renv: Project Environments. https://CRAN.R-project.org/package=renv.\n\n\nWickham, Hadley. 2021. Mastering Shiny. O’Reilly Media, Incorporated. https://books.google.ch/books?id=ha1CzgEACAAJ.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Case Studies</span>"
    ]
  },
  {
    "objectID": "case-studies.html#footnotes",
    "href": "case-studies.html#footnotes",
    "title": "11  Case Studies",
    "section": "",
    "text": "https://docs.github.com/en/free-pro-team@latest/github/authenticating-to-github/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent↩︎\nFull URL used in the below code example: https://collectionapi.metmuseum.org/public/collection/v1/search?q=%s↩︎\nFull URL used in the below code example: https://collectionapi.metmuseum.org/public/collection/v1/objects/%d↩︎\nhttps://github.com/swissdata↩︎\nhttps://github.com/swissdata/demo↩︎\nFull URL of the below code example: https://raw.githubusercontent.com/swissdata/demo/master/data↩︎\nhttps://developer.mozilla.org/en-US/docs/Web/HTTP/Methods↩︎\nhttps://nodejs.org/en/↩︎\nhttps://npmjs.com↩︎\nhttps://expressjs.com/↩︎\nFull URL used in the below code example: https://kof.ethz.ch/news-und-veranstaltungen/medien/medienagenda.html↩︎\nFull URL used in the below example https://raw.githubusercontent.com/mbannert/maps/master/ch_bfs_regions.geojson↩︎\nhttps://www.youtube.com/watch?v=cK08bxUJn5A↩︎\nonline widget galleries like R Studio’s shiny widget gallery: https://shiny.rstudio.com/gallery/widget-gallery.html that help to “shop” for the right widgets.↩︎\nYes, one could lists JavaScript here, too, but let’s keep things simple and think of JavaScript as traditionally client-side here.↩︎",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Case Studies</span>"
    ]
  },
  {
    "objectID": "appendix.html",
    "href": "appendix.html",
    "title": "Glossary",
    "section": "",
    "text": "Term\nDescription\n\n\n\n\nAPI\nApplication Programming Interface\n\n\nCamelCase\nConvention to spell file, variable or function names reminiscent of a camel, e.g., doSomething().\n\n\nCMS\nContent Management System.\n\n\nConsole\nAlso known as terminal, the console is an interface which takes written user commands. Bash is one of the most popular terminals on OS level, but scripting languages like Python and R have consoles to communicate with their interpreter,too.\n\n\nDeployment\nThe art of delivering a piece of software to production.\n\n\nEndpoint\nPart of an API, a generic URL that follows a logic that can be exploited to automate machine-to-machine data exchange.\n\n\nFork\nA clone of a repository that you (usually) do not own.\n\n\nGUI\nGraphical User Interface.\n\n\nIDE\nIntegrated Development Environment.\n\n\nKebab Case\nSpelling convention less known than snake case and camel case, kebap case looks like this: my-super-folder.\n\n\nLexical Scoping\nLook-up of variables in parent environments when they can't be found in the current environment. Be aware that this is the default behavior of R.\n\n\nMerge Request\nSee Pull Request.\n\n\nOS\nOperating System.\n\n\nOSS\nOpen Source Software.\n\n\nPull Request (PR)\nRequest to join a feature branch into another branch, e.g., main branch. Sometimes it's also called merge request.\n\n\nRegular Expression\nPattern to extract specific parts from a text, find stuff in a text.\n\n\nREPL\nread-eval-print-loop.\n\n\nReproducible Example\nA self-contained code example, including the data it needs to run.\n\n\nReverse Dependency\nInverted dependency, another library or piece of code that depends on the code at hand.\n\n\nSnake_case\nConvention to spell file, variable or function names reminiscant of a snake, e.g., do_something().\n\n\nStack\nselection of software used in a project.\n\n\nSQL\nStructured Query Language.\n\n\nSwimlanes\n(Online) Board of columns (lanes). Lanes progress from from left to right and carry issues.\n\n\nThroughput Problem\nA bottleneck which can be mitigated by parallelization, e.g., multiple containers running in parallel.\n\n\nTransactional database\ndatabase optimized for production systems. Such a database is good at reading and writing individual rows without affecting the other and while taking care of data integrity.\n\n\nVirtual Machine (VM)\nA virtual computer hosted on your computer. Often used to run another OS inside your main OS for testing purposes.",
    "crumbs": [
      "Glossary"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Allaire, J. J., Charles Teague, Carlos Scheidegger, Yihui Xie, and\nChristophe Dervieux. 2022. “Quarto.” https://doi.org/10.5281/zenodo.5960048.\n\n\nAllaire, JJ, Yihui Xie, Jonathan McPherson, Javier Luraschi, Kevin\nUshey, Aron Atkins, Hadley Wickham, Joe Cheng, Winston Chang, and\nRichard Iannone. 2022. Rmarkdown: Dynamic Documents for r. https://github.com/rstudio/rmarkdown.\n\n\nBannert, Matthias, and Severin Thoeni. 2022. Kofdata: Get Data from\nthe ’KOF Datenservice’ API. https://CRAN.R-project.org/package=kofdata.\n\n\nBannert, Matthias, Severin Thoeni, and Stéphane Bisinger. 2023.\nTstools: A Time Series Toolbox for Official Statistics. https://CRAN.R-project.org/package=tstools.\n\n\nBeg, Marijan, Juliette Taka, Thomas Kluyver, Olexandr Konovalov, Min\nRagan-Kelly, Nicolas M. Thiéry, and Hans Fangohr. 2021. “Using\nJupyter for Reproducible Scientific Workflows.” CoRR\nabs/2102.09562. https://arxiv.org/abs/2102.09562.\n\n\nBengtsson, Henrik. 2021. “A Unifying Framework for Parallel and\nDistributed Processing in r Using Futures.” The R\nJournal 13 (2): 208–27. https://doi.org/10.32614/RJ-2021-048.\n\n\nBezanson, Jeff, Alan Edelman, Stefan Karpinski, and Viral B Shah. 2017.\n“Julia: A Fresh Approach to Numerical Computing.” SIAM\nReview 59 (1): 65–98. https://doi.org/10.1137/141000671.\n\n\nBostock, Michael, Vadim Ogievetsky, and Jeffrey Heer. 2011. “D³\nData-Driven Documents.” IEEE Transactions on Visualization\nand Computer Graphics 17 (12): 2301–9. https://doi.org/10.1109/TVCG.2011.185.\n\n\nChang, Winston. 2021. R6: Encapsulated Classes with Reference\nSemantics. https://CRAN.R-project.org/package=R6.\n\n\nChang, Winston, and Barbara Borges Ribeiro. 2021. Shinydashboard:\nCreate Dashboards with ’Shiny’. https://CRAN.R-project.org/package=shinydashboard.\n\n\nChang, Winston, Joe Cheng, JJ Allaire, Carson Sievert, Barret Schloerke,\nYihui Xie, Jeff Allen, Jonathan McPherson, Alan Dipert, and Barbara\nBorges. 2022. Shiny: Web Application Framework for r. https://CRAN.R-project.org/package=shiny.\n\n\nDervieux, Christophe, JJ Allaire, Rich Iannone, Alison Presmanes Hill,\nand Yihui Xie. 2022. Distill: ’R Markdown’ Format for Scientific and\nTechnical Writing. https://CRAN.R-project.org/package=distill.\n\n\nFay, C., S. Rochette, V. Guyader, and C. Girard. 2021. Engineering\nProduction-Grade Shiny Apps. Chapman & Hall/CRC the r Series.\nCRC Press. https://books.google.ch/books?id=qExDEAAAQBAJ.\n\n\nHernangómez, Diego. 2021. “Cffr: Generate Citation File Format\nMetadata for r Packages.” Journal of Open Source\nSoftware 6 (67): 3900. https://doi.org/10.21105/joss.03900.\n\n\nHunter, J. D. 2007. “Matplotlib: A 2D Graphics\nEnvironment.” Computing In Science & Engineering 9\n(3): 90–95.\n\n\nJoo, Rocío et al. 2022. “Ten Simple Rules to Host an Inclusive\nConference.” PLOS Computational Biology 18 (7): 1–13. https://doi.org/10.1371/journal.pcbi.1010164.\n\n\nLandau, William Michael. 2021. “The Targets r Package: A Dynamic\nMake-Like Function-Oriented Pipeline Toolkit for Reproducibility and\nHigh-Performance Computing.” Journal of Open Source\nSoftware 6 (57): 2959. https://doi.org/10.21105/joss.02959.\n\n\nLi, Deqing, Honghui Mei, Yi Shen, Shuang Su, Wenli Zhang, Junting Wang,\nMing Zu, and Wei Chen. 2018. “ECharts: A Declarative Framework for\nRapid Construction of Web-Based Visualization.” Visual\nInformatics 2 (2): 136–46. https://doi.org/https://doi.org/10.1016/j.visinf.2018.04.011.\n\n\nMersmann, Olaf. 2021. Microbenchmark: Accurate Timing\nFunctions. https://CRAN.R-project.org/package=microbenchmark.\n\n\nPedersen, Thomas Lin, and David Robinson. 2022. Gganimate: A Grammar\nof Animated Graphics. https://CRAN.R-project.org/package=gganimate.\n\n\nPerepolkin, Dmytro. 2023. Polite: Be Nice on the Web. https://CRAN.R-project.org/package=polite.\n\n\nRichardson, Neal, Ian Cook, Nic Crane, Dewey Dunnington, Romain\nFrançois, Jonathan Keane, Dragoș Moldovan-Grünfeld, Jeroen Ooms, and\nApache Arrow. 2022. Arrow: Integration to ’Apache’ ’Arrow’. https://CRAN.R-project.org/package=arrow.\n\n\nSax, Christoph, and Dirk Eddelbuettel. 2018. “Seasonal Adjustment\nby X-13ARIMA-SEATS in R.” Journal\nof Statistical Software 87 (11): 1–17. https://doi.org/10.18637/jss.v087.i11.\n\n\nSchloerke, Barret, and Jeff Allen. 2022. Plumber: An API Generator\nfor r. https://CRAN.R-project.org/package=plumber.\n\n\nSink, Eric. 2011. Version Control by Example. 1st ed. PYOW\nSports Marketing.\n\n\nUshey, Kevin, and Hadley Wickham. 2023. Renv: Project\nEnvironments. https://CRAN.R-project.org/package=renv.\n\n\nvan der Loo, MPJ. 2020. “A Method for Deriving Information from\nRunning r Code.” The R Journal, Accepted for\npublication. https://arxiv.org/abs/2002.07472.\n\n\nVilhuber, Lars, Marie Connolly, Miklós Koren, Joan Llull, and Peter\nMorrow. 2022. A template README for social\nscience replication packages. https://doi.org/10.5281/zenodo.7293838.\n\n\nWickham, H. 2015. R Packages: Organize, Test, Document, and Share\nYour Code. O’Reilly Media. https://r-pkgs.org/.\n\n\nWickham, Hadley. 2011. “Testthat: Get Started with\nTesting.” The R Journal 3: 5–10. https://journal.r-project.org/archive/2011-1/RJournal_2011-1_Wickham.pdf.\n\n\n———. 2016. Ggplot2: Elegant Graphics for Data Analysis.\nSpringer-Verlag New York. https://ggplot2.tidyverse.org.\n\n\n———. 2019. Advanced r. 2nd ed. Chapman & Hall/CRC the r\nSeries. Taylor & Francis.\n\n\n———. 2021. Mastering Shiny. O’Reilly Media, Incorporated. https://books.google.ch/books?id=ha1CzgEACAAJ.\n\n\n———. 2022a. Rvest: Easily Harvest (Scrape) Web Pages. https://CRAN.R-project.org/package=rvest.\n\n\n———. 2022b. Stringr: Simple, Consistent Wrappers for Common String\nOperations. https://CRAN.R-project.org/package=stringr.\n\n\nWickham, Hadley, Jennifer Bryan, Malcolm Barrett, and Andy Teucher.\n2023. Usethis: Automate Package and Project Setup. https://CRAN.R-project.org/package=usethis.\n\n\nWickham, Hadley, Peter Danenberg, Gábor Csárdi, and Manuel Eugster.\n2022. Roxygen2: In-Line Documentation for r. https://CRAN.R-project.org/package=roxygen2.\n\n\nWickham, Hadley, and Garrett Grolemund. 2017. R for Data Science:\nImport, Tidy, Transform, Visualize, and Model Data. 1st ed.\nO’Reilly Media, Inc.\n\n\nWickham, Hadley, Jay Hesselberth, and Maëlle Salmon. 2022. Pkgdown:\nMake Static HTML Documentation for a Package. https://CRAN.R-project.org/package=pkgdown.\n\n\nXie, Yihui, Alison Presmanes Hill, and Amber Thomas. 2017. Blogdown:\nCreating Websites with R Markdown. Boca Raton,\nFlorida: Chapman; Hall/CRC. https://bookdown.org/yihui/blogdown/.\n\n\nZheng, Chunmei, Guomei He, and Zuojie Peng. 2015. “A Study of Web\nInformation Extraction Technology Based on Beautiful Soup.”\nJ. Comput. 10: 381–87.",
    "crumbs": [
      "References"
    ]
  }
]